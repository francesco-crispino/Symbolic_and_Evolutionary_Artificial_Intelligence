{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106183a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05471163",
   "metadata": {},
   "source": [
    "This section handles the loading and initial preparation of the MNIST dataset. MNIST contains 28x28 pixel grayscale images of handwritten digits (0-9).\n",
    "\n",
    "**Key Operations:**\n",
    "\n",
    "1.  **Data Loading (`load_mnist_images`, `load_mnist_labels`):**\n",
    "    *   These functions read the MNIST dataset from its specific binary file format.\n",
    "    *   Image data is reshaped to `(num_images, rows, cols)`.\n",
    "\n",
    "2.  **One-Hot Encoding Labels:**\n",
    "    *   For multi-class classification with a softmax output and categorical cross-entropy loss, integer labels (e.g., digit `5`) are converted into a one-hot vector format (e.g., `[0,0,0,0,0,1,0,0,0,0]` for 10 classes).\n",
    "    *   This represents the true label as a probability distribution where the correct class has a probability of 1.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"images/mnist_digits.png\", style=\"border-radius:20px;\", width=\"50%\">\n",
    "    <figcaption>Samples from the MNIST dataset</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9edcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "#-------------- Data Extraction ---------------------------\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097cf66",
   "metadata": {},
   "source": [
    "## PyTorch CNN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8be5b",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is defined using PyTorch's `nn.Module` to serve as a reference and source of pre-trained weights.\n",
    "\n",
    "**Architecture (defined as `SimpleCNN` class):**\n",
    "\n",
    "1.  **Conv1 + ReLU1:** `nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 1, 28, 28)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1 = \\lfloor \\frac{(28 - 2 + 0)}{2} \\rfloor + 1 = 14$\n",
    "    *   Output: `(B, 32, 14, 14)`\n",
    "\n",
    "2.  **Conv2 + ReLU2:** `nn.Conv2d(32, 64, 2, 2, 1)`\n",
    "    *   Input: `(B, 32, 14, 14)`\n",
    "    *   Padded input dimension: $14 + 2*1 = 16$\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(16 - 2 + 0)}{2} \\rfloor + 1 = 8$\n",
    "    *   Output: `(B, 64, 8, 8)`\n",
    "\n",
    "3.  **Conv3 + ReLU3:** `nn.Conv2d(64, 128, 2, 2, 0)`\n",
    "    *   Input: `(B, 64, 8, 8)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(8 - 2 + 0)}{2} \\rfloor + 1 = 4$\n",
    "    *   Output: `(B, 128, 4, 4)`\n",
    "\n",
    "4.  **Flatten:** `nn.Flatten()`\n",
    "    *   Input: `(B, 128, 4, 4)`\n",
    "    *   Output: `(B, 128 * 4 * 4)` which is `(B, 2048)`\n",
    "\n",
    "5.  **FC1 + ReLU4:** `nn.Linear(in_features=2048, out_features=250)`\n",
    "    *   Input: `(B, 2048)`\n",
    "    *   Operation: $Y = XW^T + b$\n",
    "    *   Output: `(B, 250)`\n",
    "\n",
    "6.  **FC2:** `nn.Linear(in_features=250, out_features=10)` (Output layer)\n",
    "    *   Input: `(B, 250)`\n",
    "    *   Output: `(B, 10)` (logits for 10 classes)\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"images/cnn.png\", style=\"border-radius:20px;\", height=300>\n",
    "    <figcaption>CNN Architecture (B: Batch size)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dabdea",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff84faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# No bias version of the model\n",
    "class SimpleCNN_no_bias(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN_no_bias, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1, bias=False)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0, bias=False)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# # 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8a955",
   "metadata": {},
   "source": [
    "### Extracting Pre-trained Weights from PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee8995",
   "metadata": {},
   "source": [
    "This section loads weights from a pre-trained PyTorch model (`simple_cnn_mnist.pth`) and converts them into NumPy arrays. These NumPy weights will be used for our custom CNN implementations to ensure consistency for inference comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e62c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 2, 2), NumPy Shape=(64, 32, 2, 2)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 2, 2), NumPy Shape=(128, 64, 2, 2)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # A good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_1 = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pytorch_weights_of_kernels_in_layer_1\n",
    "\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pytorch_weights_of_kernels_in_layer_1.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_2 = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pytorch_weights_of_kernels_in_layer_2\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pytorch_weights_of_kernels_in_layer_2.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_3 = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pytorch_weights_of_kernels_in_layer_3\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pytorch_weights_of_kernels_in_layer_3.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pytorch_fc1_layer_weights = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pytorch_fc1_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pytorch_fc1_layer_biases = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pytorch_fc1_layer_biases.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pytorch_fc1_layer_weights.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pytorch_fc1_layer_biases.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pytorch_fc2_layer_weights = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pytorch_fc2_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pytorch_fc2_layer_biases = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pytorch_fc2_layer_biases.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pytorch_fc2_layer_weights.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pytorch_fc2_layer_biases.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06822b",
   "metadata": {},
   "source": [
    "## CNN - NumPy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c17c8",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ca069",
   "metadata": {},
   "source": [
    "Zero-padding adds a border of zeros around an input image or feature map before convolution. For example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\quad \\xrightarrow{\\textcolor{lightgreen}{\\textnormal{zero padding}}} \\quad\n",
    "\\begin{bmatrix}\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 1 & 2 & 3 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 4 & 5 & 6 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 7 & 8 & 9 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It's important for:\n",
    "\n",
    "1.  **Controlling Output Spatial Dimensions:** Padding can be used to maintain or control the reduction in height/width of feature maps. The output dimension (e.g., height $O_H$) is given by:\n",
    "    $$ O_H = \\left\\lfloor \\frac{I_H - K_H + 2P_H}{S_H} \\right\\rfloor + 1 $$\n",
    "    where $I_H$ is input height, $K_H$ kernel height, $P_H$ padding on one side of height, and $S_H$ stride.\n",
    "2.  **Improving Feature Extraction at Borders:** Allows the kernel to process edge pixels more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70273b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "image_3_by_3 = np.arange(1,37).reshape(2,2,3,3)\n",
    "padded_image_3_by_3 = np.pad(image_3_by_3,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(image_3_by_3)\n",
    "print(padded_image_3_by_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00035f0",
   "metadata": {},
   "source": [
    "### Matrix Dilatation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6aa4a",
   "metadata": {},
   "source": [
    "The `dilateOne` function \"dilates\" an input matrix by inserting a single row and column of zeros between existing rows and columns along its last two spatial dimensions. This means it inserts $S-1$ zeros when the forward convolution stride $S$ was 2.\n",
    "\n",
    "**Relevance in Backpropagation for $\\frac{\\partial L}{\\partial X}$:**\n",
    "\n",
    "This dilation operation is a critical step when computing the gradient of the loss with respect to the input of a convolutional layer ($\\frac{\\partial L}{\\partial X}$), especially if the forward pass utilized a stride $S > 1$. Here is why:\n",
    "* When a forward convolution uses a stride $S > 1$, it effectively downsamples the input, resulting in an output feature map $Z$ with smaller spatial dimensions.\n",
    "* To calculate $\\frac{\\partial L}{\\partial X}$, we need to use the gradient flowing back from the subsequent layer, $\\frac{\\partial L}{\\partial Z}$ (where $Z$ is the output of the strided convolution). **Since the original input $X$ has larger spatial dimensions than $Z$, the gradient $\\frac{\\partial L}{\\partial Z}$ must be \"upsampled\" or \"spread out\" before it can be convolved with the kernel weights to produce a gradient of the correct shape for $X$.**\n",
    "\n",
    "**Dilation Step:** This upsampling is achieved by inserting $S-1$ rows and columns of zeros between the elements of $\\frac{\\partial L}{\\partial Z}$. The `dilateOne` function in this notebook performs this specific operation for a stride $S=2$ (inserting one row/column of zeros).\n",
    "\n",
    "After $\\frac{\\partial L}{\\partial Z}$ is dilated to form $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}$, it is then typically padded (with $K-1$ zeros where $K$ is the kernel dimension, adjusted for any original padding) and subsequently convolved with the 180-degree rotated (or flipped) kernel ($W_{rot180}$). This entire sequence of operations (padding the dilated output gradient and convolving it with the flipped kernel) is what yields $\\frac{\\partial L}{\\partial X}$ and is often referred to as a \"full convolution\" in this context (see \"A guide to convolution arithmetic for deep learning\" by Dumoulin and Visin, or the provided articles by Mayank Kaushik).\n",
    "\n",
    "The image below illustrates the concept of dilating an output gradient tensor. This dilation is a preparatory step before the gradient is used in further convolution operations during backpropagation for layers that had striding in their forward pass.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*luRORFyTmj9mJ7rVhzlbZA.png\" height=250, style=\"border-radius:20px;\">\n",
    "</figure>\n",
    "\n",
    "Illustrative example of dilating an output gradient tensor (like $\\frac{\\partial L}{\\partial Z}$) by inserting $S-1$ (stride minus one) zeros. For `dilateOne`, $S=2$, so one zero is inserted between elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bd18f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Shape: (1, 1, 2, 2)\n",
      "Dilated Image Shape: (1, 1, 4, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dilate(tensor, stride):\n",
    "    if stride == 1:\n",
    "        return tensor\n",
    "\n",
    "    batch_size, num_channels, height, width = tensor.shape\n",
    "    \n",
    "    dilated_height = height + (height - 1) * (stride - 1)\n",
    "    dilated_width = width + (width - 1) * (stride - 1)\n",
    "    \n",
    "    dilated_tensor = np.zeros((batch_size, num_channels, dilated_height, dilated_width)).astype(tensor.dtype)\n",
    "    dilated_tensor[:, :, ::stride, ::stride] = tensor\n",
    "    return dilated_tensor\n",
    "\n",
    "# Example usage of dilate function\n",
    "example_image = np.arange(5, 9).reshape(1, 1, 2, 2)\n",
    "dilated_image = dilate(example_image, 3)\n",
    "print(\"Original Image Shape:\", example_image.shape)\n",
    "print(\"Dilated Image Shape:\", dilated_image.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 2), gridspec_kw={'width_ratios': [0.3, 1]})\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(example_image[0, 0], cmap='gray', vmin=0, vmax=8)\n",
    "plt.xticks([0, 1], [1, 2])\n",
    "plt.yticks([0, 1], [1, 2])\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(dilated_image[0, 0], cmap='gray', vmin=0, vmax=8)\n",
    "plt.xticks([0, 1, 2, 3], [1, 2, 3, 4])\n",
    "plt.yticks([0, 1, 2, 3], [1, 2, 3, 4])\n",
    "plt.title('Dilated Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa841de1",
   "metadata": {},
   "source": [
    "### Benchmark network (debug and testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c783ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class TesterCNN(nn.Module):\n",
    "    def __init__(self, kernels: torch.Tensor, biases: torch.Tensor = None, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernels.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(biases is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernels)\n",
    "            if biases is not None:\n",
    "                self.conv.bias.copy_(biases)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if biases is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817601c0",
   "metadata": {},
   "source": [
    "## Nested-Loops Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4351c2f",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26d187",
   "metadata": {},
   "source": [
    "`nested_loop_convolution` implements a 2D convolution followed by ReLU activation using explicit nested loops. This is computationally highly inefficient.\n",
    "\n",
    "**Inputs:**\n",
    "    *   `batch_of_images`: size `(N, C_in, H_in, W_in)`.\n",
    "    *   `kernels`: size `(C_out, C_in, K_H, K_W)`.\n",
    "    *   `biases`: Per-filter biases `(C_out,)`.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Padding & Output Size:** Input `batch_of_images` is padded to better extract information from tha borders of the images. Output dimensions $(O_H, O_W)$ are calculated using the formula in the Padding section.\n",
    "2.  **Convolution:** The convolution is computed explicitly, iterating over each element of the patches from the input tensor and multiplying them by the \"sliding\" kernels. For each output element $(n, f, y_{out}, x_{out})$:\n",
    "    $$ \\text{Output}(n, f, y_{out}, x_{out}) = \\left( \\sum_{c=0}^{C_{in}-1} \\sum_{k_y=0}^{K_H-1} \\sum_{k_x=0}^{K_W-1} \\text{Img}_{pad}(n, c, y_{out}S + k_y, x_{out}S + k_x) \\cdot \\text{Ker}(f, c, k_y, k_x) \\right) + \\text{Bias}(f) $$\n",
    "3.  **ReLU Activation:** If `applyReLU=True`: $\\text{ActivatedOutput} = \\max(0, \\text{Output})$. A binary `mask` (1 where Output > 0, else 0) is also returned for backpropagation.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/refs/heads/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Convolution of a 7x7 image (blue) with a 3x3 kernel (dark blue), resulting in a 5x5 output (green). <br><i>(Source: Dumoulin & Visin, A guide to convolution arithmetic for deep learning)</i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a51071b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------example_image--------\n",
      "[[[[1. 2. 3.]\n",
      "   [4. 5. 6.]\n",
      "   [7. 8. 9.]]]]\n",
      "-------example_kernel-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Nested-Loop approach-------\n",
      "[[[[  5.  19.]\n",
      "   [ 37.  78.]]\n",
      "\n",
      "  [[ 10.  40.]\n",
      "   [ 82. 191.]]]]\n",
      "-------PyTorch model-------\n",
      "tensor([[[[  5.,  19.],\n",
      "          [ 37.,  78.]],\n",
      "\n",
      "         [[ 10.,  40.],\n",
      "          [ 82., 191.]]]])\n"
     ]
    }
   ],
   "source": [
    "def nested_loop_convolution(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        output_channels, input_channels, kernel_width, kernel_height = kernels.shape\n",
    "        kernel_channels = output_channels\n",
    "    else: # Backward case\n",
    "        input_channels, output_channels, kernel_width, kernel_height = kernels.shape\n",
    "        kernel_channels = input_channels\n",
    "\n",
    "    # biases has shape (output_channels, 1, 1). It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # The number of channels taken in input by the kernel 'input_channels' must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    batch_size, channels, image_height, image_width  = batch_of_images.shape\n",
    "    output_image_height = int(((image_height - kernel_height) / stride) + 1) # new image height # Padding is already added\n",
    "    output_image_width = int(((image_width - kernel_width) / stride) + 1) # new image width\n",
    "    output = np.zeros((batch_size, output_channels, output_image_height, output_image_width)).astype(np.float32) # new image\n",
    "\n",
    "    if input_channels != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({input_channels}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for single_image in range(batch_size):\n",
    "        for single_kernel_channel in range(kernel_channels):\n",
    "            for output_row_idx in range(output_image_height): # which cycles row by row of the new image\n",
    "                for output_col_idx in range(output_image_width): # which cycles column by column of the new image\n",
    "                    output_cell_accumulator = 0.0  # sum for the current output cell (accumulates the convolution result)\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(input_channels): # channels == input_channels\n",
    "                        for kernel_row_idx in range(kernel_height):\n",
    "                            # Position of the kernel over the input image: row\n",
    "                            input_row_idx = (output_row_idx * stride) + kernel_row_idx\n",
    "                            for kernel_col_idx in range(kernel_width):\n",
    "                                # Position of the kernel over the input image: column\n",
    "                                input_col_idx = (output_col_idx * stride) + kernel_col_idx\n",
    "\n",
    "                                # Check if the position is inside the image\n",
    "                                if 0 <= input_row_idx < image_height and 0 <= input_col_idx < image_width:\n",
    "                                    input_element = batch_of_images[single_image, channel, input_row_idx, input_col_idx]\n",
    "                                    kernel_element = kernels[single_kernel_channel, channel, kernel_row_idx, kernel_col_idx]\n",
    "                                    # Compute the convolution sum (to be done for each element of the sliding kernel over the image)\n",
    "                                    output_cell_accumulator += (input_element * kernel_element).astype(np.float32)\n",
    "\n",
    "                    # Assign the result to the output image\n",
    "                    output[single_image, single_kernel_channel, output_row_idx, output_col_idx] = output_cell_accumulator\n",
    "\n",
    "    if biases.all() != 0:\n",
    "        biases = biases.reshape(biases.shape[0],1,1)\n",
    "\n",
    "        if biases.shape[0] != output_channels:\n",
    "            raise ValueError(f\"biases dimension ({biases.shape[0]}) doesn't match kernel's number of channels ({output_channels})\")\n",
    "        \n",
    "        output = output + biases\n",
    "\n",
    "    output = output.astype(np.float32)\n",
    "\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0, output)\n",
    "        mask = output.copy()\n",
    "        mask[mask > 0] = 1\n",
    "\n",
    "        return output, mask\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "example_image = np.arange(1,3*3+1).reshape(1,1,3,3).astype(np.float32)\n",
    "print(\"-------example_image--------\")\n",
    "print(example_image)\n",
    "\n",
    "example_kernel = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------example_kernel-------\")\n",
    "print(example_kernel)\n",
    "\n",
    "example_biases = np.array([1,2]).reshape(2,1,1)\n",
    "outpout, mask = nested_loop_convolution(example_image, example_kernel, biases=example_biases, padding=1, stride=2)\n",
    "print(\"-------Nested-Loop approach-------\")\n",
    "print(outpout)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "example_kernel = torch.from_numpy(example_kernel).float()\n",
    "example_biases = torch.from_numpy(np.array([1,2])).float()\n",
    "testerCNNmodel = TesterCNN(kernels=example_kernel,biases=example_biases, stride=2, padding=1)\n",
    "\n",
    "x = torch.from_numpy(example_image)\n",
    "y = testerCNNmodel(x)\n",
    "print(\"-------PyTorch model-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bad0a3",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff4adf",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Backward\n",
    "\n",
    "We now need to implement the backward pass of a convolutional layer, calculating the gradients of the loss function with respect to the layer's weights (or kernels) ($W^{(i)}$), biases ($b^{(i)}$), and the layer's input (images) ($X^{(i)}$).\n",
    "\n",
    "**Notation:**\n",
    "*   $L$: The Loss function.\n",
    "*   $X^{(i)}$: The input to convolutional layer $i$.\n",
    "*   $W^{(i)}$: The weights (kernels) of layer $i$.\n",
    "*   $b^{(i)}$: The biases of layer $i$.\n",
    "*   $Z^{(i)}$: The pre-activation output of layer $i$ ($Z^{(i)} = \\text{conv}(X^{(i)}, W^{(i)}) + b^{(i)}$).\n",
    "*   $A^{(i)}$: The activated output of layer $i$ (e.g., $A^{(i)} = \\text{ReLU}(Z^{(i)})$).\n",
    "*   $\\frac{\\partial L}{\\partial A^{(i)}}$: The gradient of the loss with respect to the activated output $A^{(i)}$ of the current layer (propagated from the next layer).\n",
    "*   $\\frac{\\partial L}{\\partial Z^{(i)}}$: The gradient of the loss with respect to the pre-activation output $Z^{(i)}$ of the current layer. This is often denoted as $\\delta^{(i)}$ in your original text after passing through the ReLU derivative.\n",
    "*   $\\text{mask}^{(i)}$: The binary mask derived from the ReLU activation in the forward pass (1 if $Z^{(i)} > 0$, else 0).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Gradient through ReLU Activation (Backward ReLU):**\n",
    "    The derivative of ReLU is 1 for positive inputs and 0 otherwise. This is efficiently implemented by element-wise multiplying the incoming gradient $\\frac{\\partial L}{\\partial A^{(i)}}$ with the `mask` computed during the forward pass.\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial Z^{(i)}} = \\frac{\\partial L}{\\partial A^{(i)}} \\odot \\text{mask}^{(i)}\n",
    "    $$\n",
    "    where $\\odot$ denotes the Hadamard (element-wise) product. Henceforth, for brevity, we will denote $\\frac{\\partial L}{\\partial Z^{(i)}}$ as $\\delta_Z^{(i)}$.\n",
    "\n",
    "2.  **Gradient with respect to Weights ($W^{(i)}$):**\n",
    "    To calculate $\\frac{\\partial L}{\\partial W^{(i)}}$, we need to correlate the layer's input $X^{(i)}$ with the pre-activation output gradient $\\delta_Z^{(i)}$. Each weight $W_{f,c,k_y,k_x}$ contributes to multiple elements of the output $Z^{(i)}$. Its gradient is the sum of all these contributions. Mathematically, this is equivalent to a convolution between the input $X^{(i)}$ (appropriately padded as in the forward pass) and the gradient $\\delta_Z^{(i)}$ (which is treated as the \"kernel\" for this convolution).\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(X^{(i)}_{padded}, \\delta_Z^{(i)})\n",
    "    $$\n",
    "    This convolution must always have a stride = 1.\n",
    "* **Dilation of $\\delta_Z^{(i)}$ (if forward stride > 1):** If the forward pass used a stride $S > 1$, $\\delta_Z^{(i)}$ must be dilated by inserting $S-1$ zeros between its elements *before* this convolution for $\\frac{\\partial L}{\\partial W^{(i)}}$. **This technique allows the use of a standard convolution**, where the \"kernel\" ($\\delta_Z^{(i)}$) slides over the \"image\" ($X^{(i)}$), and makes the output's size equal to the one of the original kernel.\n",
    "    \n",
    "    (If $X^{(i)}$ is $(N, C_{in}, H_{in}, W_{in})$ and $\\delta_Z^{(i)}$ is $(N, C_{out}, H_{out}, W_{out})$, the result $\\frac{\\partial L}{\\partial W^{(i)}}$ will have shape $(C_{out}, C_{in}, K_H, K_W)$, as the original kernel.)\n",
    "\n",
    "3.  **Gradient with respect to Input ($X^{(i)}$):**\n",
    "    To propagate the gradient to the previous layer, we also need to calculate $\\frac{\\partial L}{\\partial X^{(i)}}$. This operation is known as a \"Full Convolution\". It involves convolving the gradient $\\delta_Z^{(i)}$ with the kernels $W^{(i)}$ rotated by 180 degrees (or flipped both horizontally and vertically) ($W^{(i)}_{rot180}$).\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial X^{(i)}} = \\text{FullConvolution}(\\delta_Z^{(i)}, W^{(i)}_{rot180})\n",
    "    $$\n",
    "    ***Note:*** *The name \"Full Convolution\" refers to the fact that the first term of the operation is padded as much as possible (a larger padding would make some patches contain only zeros, so it would not make sense).*\n",
    "    \n",
    "    Full Convolution is implemented as follows:\n",
    "    *   **Dilation of $\\delta_Z^{(i)}$:** As for the gradient w.r.t. the weights, if the stride $S$ of the forward pass was greater than 1, $\\delta_Z^{(i)}$ is \"dilated\". Let's call this $\\delta_{Z,dilated}^{(i)}$. If $S=1$, $\\delta_{Z,dilated}^{(i)} = \\delta_Z^{(i)}$.\n",
    "    *   **Padding of $\\delta_{Z,dilated}^{(i)}$:** The dilated gradient is padded. If $P_{fwd}$ was the forward pass padding and $K$ the kernel dimension, the padding here is $K-1-P_{fwd}$ on each side.\n",
    "    *   **Convolution:** The padded $\\delta_{Z,dilated}^{(i)}$ is convolved with $W^{(i)}_{rot180}$. The stride of this convolution is always 1.\n",
    "    The result $\\frac{\\partial L}{\\partial X^{(i)}}$ will have the same spatial dimensions as the original input $X^{(i)}$.\n",
    "\n",
    "4.  **Gradient with respect to Biases ($b^{(i)}$):**\n",
    "    Since the bias $b_c^{(i)}$ (corresponding to filter $c$) is added to all elements of channel $c$ of the pre-activation output $Z^{(i)}$, its gradient is simply the sum of all elements of $\\delta_Z^{(i)}$ (the gradient *before* dilation) over that channel, across the spatial dimensions (height and width) and across all examples in the batch.\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n} \\sum_{h,w} (\\delta_Z^{(i)})_{n,c,h,w}\n",
    "    $$\n",
    "---\n",
    "<div style=\"text-align:center;\">\n",
    "<b>Visual example of forward and backward pass, with padding = 1 and striding = 2:</b>\n",
    "<div style=\"display:flex; flex-direction:row; justify-content:space-between; align-items:flex-start; margin-top:15px;\">\n",
    "<figure style=\"text-align:center; width:50%; flex-grow:1; margin-top:0px;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/padding_strides.gif\", style=\"border-radius:20px; height:200px;\"/>\n",
    "    <figcaption> The input image (blue) is 5x5, and the kernel (gray) is 3x3. The input has padding = 1 a stride = 2 is used. As we can see, the output is 3x3.</figcaption>\n",
    "</figure>\n",
    "<figure style=\"text-align:center; width:50%; flex-grow:1; margin-top:0px;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif\", style=\"border-radius:20px; height:200px;\"/>\n",
    "    <figcaption>Corresponding backward phase: the output gradient of the previous convolution is the 3x3 blue input. It is padded and dilated by stride-1=2-1=1 and convolved with the rotated kernel (gray) to produce the input gradient (5x5 green output).</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<figcaption style=\"text-align:center;\"><i><a href\"https://arxiv.org/pdf/1603.07285\">(Source: A guide to convolution arithmetic for deep learning - Dumoulin & Visin)</a></i>\n",
    "</fgcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a1928d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (1, 1, 7, 7)\n",
      "kernel: (2, 1, 2, 2)\n",
      "d_image: (1, 2, 4, 4)\n",
      "gradient w.r.t. image: (1, 1, 7, 7)\n",
      "gradient w.r.t. kernel: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "def nested_loop_gradient(batch_of_images, d_image, kernels, mask, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution.\n",
    "    The mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image and the gradient of the kernel,\n",
    "    to backpropagate the error.\n",
    "    The gradient of the bias is also returned.\n",
    "    \"\"\" \n",
    "    ############### Gradient of Input Image ###############\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image\n",
    "    # dilated (zeros between matrix elements) by stride-1 and padded by kernel-1 in height and width.\n",
    "    # The kernel is flipped by 180 degrees and the stride is set to 1.\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, image_channels, image_height, iamge_width = batch_of_images.shape\n",
    "\n",
    "    # Backward ReLU\n",
    "    d_image = np.multiply(d_image, mask)\n",
    "\n",
    "    # Dilate the gradient of the output\n",
    "    d_image = dilate(d_image, stride)\n",
    "\n",
    "    d_image = np.pad(d_image,\n",
    "                     (\n",
    "                      (0,0),(0,0),\n",
    "                      (kernel_height-1-padding, kernel_height-1-padding),\n",
    "                      (kernel_width-1-padding, kernel_width-1-padding)\n",
    "                     ))\n",
    "    \n",
    "    batch_size, kernels_number, d_image_height, d_iamge_width = d_image.shape\n",
    "    \n",
    "    # Flip the kernel\n",
    "    flipped_kernel = np.rot90(kernels,2,(-2,-1))\n",
    "\n",
    "    # Computation\n",
    "    gradient_wrt_image = np.zeros_like(batch_of_images)\n",
    "    current_sum = 0.0\n",
    "    for single_image in range(batch_size):\n",
    "        for image_row_idx in range(image_height):\n",
    "            for image_col_idx in range(iamge_width):\n",
    "                for single_kernel in range(kernels_number):\n",
    "                    for input_channel in range(kernel_channels):\n",
    "                        for kernel_row_idx in range(kernel_height):\n",
    "                            y = image_row_idx + kernel_row_idx\n",
    "\n",
    "                            for kernel_col_idx in range(kernel_width):\n",
    "                                x = image_row_idx + kernel_col_idx\n",
    "\n",
    "                                if 0 <= y < d_image.shape[-2] and 0 <= x < d_image.shape[-1]:\n",
    "                                    input_element = d_image[single_image,single_kernel,y,x]\n",
    "                                    kernel_element = flipped_kernel[single_kernel,input_channel,kernel_row_idx,kernel_col_idx] \n",
    "                                else:\n",
    "                                    break\n",
    "\n",
    "                                current_sum += input_element*kernel_element\n",
    "\n",
    "                    gradient_wrt_image[single_image,input_channel,image_row_idx,image_col_idx] = current_sum\n",
    "                    current_sum = 0.0\n",
    "\n",
    "    ############### Gradient of Kernel ###############\n",
    "    # The computation consists in a convolution between the original image and the dilated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    gradient_wrt_kernel = np.zeros_like(kernels)\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    current_sum = 0.0\n",
    "\n",
    "    for single_image in range(batch_size):\n",
    "        for kernel_row_idx in range(kernel_height):\n",
    "            for kernel_col_idx in range(kernel_width):\n",
    "                for input_channel in range(kernel_channels):\n",
    "                    # The number of kernels determines the number of channels of the output tensor\n",
    "                    for outpot_channel in range(kernels_number):\n",
    "                        for d_image_row_idx in range(d_image_height):\n",
    "                            y = kernel_row_idx + d_image_row_idx\n",
    "\n",
    "                            for d_image_col_idx in range(d_iamge_width):\n",
    "                                x = kernel_row_idx + d_image_col_idx\n",
    "\n",
    "                                if 0 <= y < image_height and 0 <= x < iamge_width:\n",
    "                                    input_element = batch_of_images[single_image,input_channel,y,x]\n",
    "                                    kernel_element = d_image[single_image,outpot_channel,d_image_row_idx,d_image_col_idx] \n",
    "                                    current_sum += input_element*kernel_element\n",
    "                                else:\n",
    "                                    break\n",
    "\n",
    "                        gradient_wrt_kernel[outpot_channel,input_channel,kernel_row_idx,kernel_col_idx] = current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "    ############### Gradient of Bias ###############\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gradient_wrt_bias = d_image.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    return gradient_wrt_image, gradient_wrt_kernel, gradient_wrt_bias\n",
    "\n",
    "kernel_channels = 1\n",
    "kernels_number = 2\n",
    "image_side_len = 7\n",
    "kernel_side_len = 2\n",
    "example_stride = 2\n",
    "example_padding = 1\n",
    "\n",
    "example_image = np.arange(1, kernel_channels * image_side_len * image_side_len + 1)\n",
    "example_image = example_image.reshape(1, kernel_channels, image_side_len, image_side_len)\n",
    "\n",
    "example_kernel = np.arange(1, kernels_number * kernel_channels * (kernel_side_len**2) + 1)\n",
    "example_kernel = example_kernel.reshape(kernels_number, kernel_channels, kernel_side_len, kernel_side_len)\n",
    "\n",
    "example_d_image, mask = nested_loop_convolution(example_image, example_kernel, stride=example_stride, padding=example_padding) \n",
    "example_d_image = example_d_image / np.mean(example_d_image)\n",
    "\n",
    "image_grad, kernel_grad, bias_grad = nested_loop_gradient(example_image, example_d_image, example_kernel, mask, stride=example_stride, padding=example_padding)\n",
    "\n",
    "print(f\"image: {example_image.shape}\")\n",
    "print(f\"kernel: {example_kernel.shape}\")\n",
    "print(f\"d_image: {example_d_image.shape}\")\n",
    "print(f\"gradient w.r.t. image: {image_grad.shape}\")\n",
    "print(f\"gradient w.r.t. kernel: {kernel_grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2fbc0",
   "metadata": {},
   "source": [
    "## Im2Col Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae10455",
   "metadata": {},
   "source": [
    "### Im2Col approach: Convolutional Layer - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c44f",
   "metadata": {},
   "source": [
    "`im2col_convolution` implements 2D convolution more efficiently using an Im2Col approach using`sliding_window_view` and optimized matrix multiplication.\n",
    "\n",
    "**Im2Col Core Idea:**\n",
    "\n",
    "* **Input Patch Extraction:**\n",
    "    *   `window_m = ... .reshape((-1,(kw*kh*nc)))`: Flattens each extracted patch into a row vector of size `kw*kh*nc`. `window_m` (our $X_{col}$) thus has shape `(N_patches, patch_size)`.\n",
    "    *   `kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)`: Flattens each filter `(kc, ac, kw, kh)` -> `(kc, ac*kw*kh)`. This is $W_{col}$, shape `(patch_size, C_out)`.\n",
    "    Following the example in the animation below, we have a 4x4 RGB image (so **3x4x4**) that has to be convoluted by a **2x2** kernel (represented as the red outlines sliding over the image). The kernel will slide **9** times over the image and, for each slide, the number of multiplications and values to be summed is 4 (elements of the kernel) times 3 (channels of both the image and the kernel) that equals **12**. Hence, as we can see in the animation below, the flattened patches are vectors of **12** elements, and in total they are **9**, as the slides performed by the kernel.\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/1.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Each patch is extracted and flattened into a vector</figcaption>\n",
    "</figure>\n",
    "\n",
    "* **Kernel Reshaping**:\n",
    "    To perform the convolution as a vetor-matrix multilication (or matrix-matrix multiplication) we then have to flatten the kernel(s) into row vectors\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/10.png\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Kernels are flattened into a vector</figcaption>\n",
    "</figure>\n",
    "\n",
    "* **Convolution as Matrix Multiplication:**\n",
    "    At this point, the convolution boils down to a simple matrix multiplication, with enormous gains in efficiency and simplicity\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/11.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Each element of the output is given by a simple and optimized dot product</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c9deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[  1.   2.   3.   4.   5.]\n",
      "   [  6.   7.   8.   9.  10.]\n",
      "   [ 11.  12.  13.  14.  15.]\n",
      "   [ 16.  17.  18.  19.  20.]\n",
      "   [ 21.  22.  23.  24.  25.]]\n",
      "\n",
      "  [[ 26.  27.  28.  29.  30.]\n",
      "   [ 31.  32.  33.  34.  35.]\n",
      "   [ 36.  37.  38.  39.  40.]\n",
      "   [ 41.  42.  43.  44.  45.]\n",
      "   [ 46.  47.  48.  49.  50.]]\n",
      "\n",
      "  [[ 51.  52.  53.  54.  55.]\n",
      "   [ 56.  57.  58.  59.  60.]\n",
      "   [ 61.  62.  63.  64.  65.]\n",
      "   [ 66.  67.  68.  69.  70.]\n",
      "   [ 71.  72.  73.  74.  75.]]\n",
      "\n",
      "  [[ 76.  77.  78.  79.  80.]\n",
      "   [ 81.  82.  83.  84.  85.]\n",
      "   [ 86.  87.  88.  89.  90.]\n",
      "   [ 91.  92.  93.  94.  95.]\n",
      "   [ 96.  97.  98.  99. 100.]]]]\n",
      "-------ker-------\n",
      "[[[[ 1  2]\n",
      "   [ 3  4]]\n",
      "\n",
      "  [[ 5  6]\n",
      "   [ 7  8]]\n",
      "\n",
      "  [[ 9 10]\n",
      "   [11 12]]\n",
      "\n",
      "  [[13 14]\n",
      "   [15 16]]]\n",
      "\n",
      "\n",
      " [[[17 18]\n",
      "   [19 20]]\n",
      "\n",
      "  [[21 22]\n",
      "   [23 24]]\n",
      "\n",
      "  [[25 26]\n",
      "   [27 28]]\n",
      "\n",
      "  [[29 30]\n",
      "   [31 32]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[ 7689.]]\n",
      "\n",
      "  [[18314.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[ 7689.]]\n",
      "\n",
      "  [[18314.]]]]\n"
     ]
    }
   ],
   "source": [
    "def im2col_convolution(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "\n",
    "    ###### im2col approach steps: ######\n",
    "    # 1. Pad the input image as needed\n",
    "    if padding > 0:\n",
    "        batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    # 2. Extract sliding windows from the input image, considering the kernel size and stride\n",
    "    sliding_windows = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,input_channels,kernel_height,kernel_width))[:,:,::stride,::stride]\n",
    "\n",
    "    # 3. Flatten the sliding windows and the kernel to prepare for matrix multiplication\n",
    "    sliding_windows = sliding_windows.reshape((-1,(kernel_height * kernel_width * input_channels)))\n",
    "\n",
    "    kernels = kernels.reshape((-1,(kernel_height*kernel_width*input_channels))).transpose(1,0)\n",
    "\n",
    "    # Now, the convolution can be performed as a matrix multiplication, where each row of the sliding windows\n",
    "    # corresponds to a flattened patch of the input image, and each column of the kernels corresponds to a flattened kernel (channel-wise)\n",
    "    images_dot_kernels = np.matmul(sliding_windows, kernels).astype(np.float32) # It's called 'images_dot_kernels' because it contains the result of the whole convolution operation\n",
    "\n",
    "    # Compute the output dimensions to reshape the resulting matrix (each row corresponds to a patch)\n",
    "    output_width = int(((image_width - kernel_width) / stride) + 1)\n",
    "    output_height = int(((image_height - kernel_height) / stride) + 1)\n",
    "\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output = images_dot_kernels.reshape(batch_size, output_width, output_height, kernels_number)\n",
    "\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    output = output.transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "    # Add biases if they are provided\n",
    "    if biases.any() != 0:\n",
    "        output = (output + biases.reshape(1,-1,1,1))\n",
    "\n",
    "    # Apply ReLU activation if specified\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0, output)\n",
    "\n",
    "    # Create a mask for the backward operation of ReLU activation\n",
    "    mask = np.copy(output)\n",
    "    mask[mask > 0] = 1\n",
    "\n",
    "    return output, mask\n",
    "\n",
    "img = np.arange(1,4*5*5+1).reshape(1,4,5,5).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,32+1).reshape(2,4,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "\n",
    "s = 4\n",
    "\n",
    "res,mask = nested_loop_convolution(img, ker, bias,padding=0,stride=s)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = im2col_convolution(img, ker, bias,padding=0,stride=s)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c414872",
   "metadata": {},
   "source": [
    "### Im2Col approach: Convolutional Layer - Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e1403",
   "metadata": {},
   "source": [
    "`im2col_gradient` computes the following gradients using again the im2col approach:\n",
    "* $\\frac{\\partial L}{\\partial X}$ (`gi`): gradient w.r.t. the input images;\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial W}$ (`gk`): gradient w.r.t. the kernels;\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial b}$ (`gb`): gradient w.r.t. the biases.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Backward ReLU:** $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\text{mask}$<br>\n",
    "    \"$\\text{mask}$\" is a matrix (tensor) whose elements are 1 if the corrisponding elements in the output of a given layer were > 0. It enables the flowing of the gradient only through elements that contributed to form the output in the first place.\n",
    "\n",
    "2.  **Gradient w.r.t. Input (`gi`):**\n",
    "    $\\frac{\\partial L}{\\partial X} = \\text{FullConv}(\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}, W_{rot180})$<br>\n",
    "    A full convolution is needed, since we must obtain a result with the same size as the input image. As you remember, the output size is given by this formula: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1$, so we must \"invert\" it. In particular, the output tensor must be:\n",
    "    * dilated by $stride - 1$ and\n",
    "    * padded by:\n",
    "    $(kernel\\_height-1-input\\_padding{\\textnormal{, }}kernel\\_width-1-input\\_padding)$\n",
    "\n",
    "2.  **Gradient w.r.t. Kernel (`gk`):**\n",
    "    This is $\\frac{\\partial L}{\\partial W} = \\text{Conv}(X_{padded}, \\frac{\\partial L}{\\partial Z})$.\n",
    "   \n",
    "\n",
    "3.  **Gradient w.r.t. Bias (`gb`):** $\\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} (\\frac{\\partial L}{\\partial Z})_{n,f,h,w}$. (`gb = d_img.sum((-1,-2))`).\n",
    "\n",
    "**But what if, in the forward phase, the stride was greater than 1?**<br>\n",
    "Dilation was already mentioned, but here we will give you again an explanation, but with a detailed visual example. Consider the case in which the stride was $S= 2$ in the forward pass. The output gradient tensor needs to be dilated (in order to obtain a tensor with the same size as the input image) before being convolved with the kernel gradient, to be effectively useful to backpropagate the loss. A visualization of why dilation enables this fundamental result is in the gif below:\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KOHfsOHX5ujcMfr6Xjy6zQ.png\" height=\"250\", style=\"border-radius:20px 0 0 20px;\"/>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QvTW-pNwJAlbfj1LrZe8JA.gif\" height=\"250\", style=\"border-radius:0 20px 20px 0;\"/>\n",
    "    <figcaption>The backpropagation operation is identical to a stride = 1 convolution of a padded, dilated version of the output gradient tensor with a flipped version of the filter.<br><i><a href=\"https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710\">(Source: Backpropagation for Convolution with Strides, Mayank Kaushik)</a></i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c5583e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimAge: (1, 4, 3, 3)\n",
      "imAge: (1, 3, 9, 9)\n",
      "kerNel: (4, 3, 3, 3)\n",
      "dimAge: (1, 4, 3, 3)\n",
      "ggi: (1, 3, 9, 9)\n",
      "ggradient_wrt_kernels: (4, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "def im2col_gradient(batch_of_images, d_image, kernels, mask, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\"\n",
    "    ############ Gradient of Input Image ############\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image dilated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel is rotated by 180 degrees (flipped vertically and horizontally)\n",
    "    # FullConvolution(d_imageDilated, Rotated180Deg(kernel)) with stride 1\n",
    "    output_channels, input_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_image = np.multiply(d_image, mask)\n",
    "\n",
    "    # Delating the gradient of the output\n",
    "    d_image = dilate(d_image, stride)\n",
    "  \n",
    "    # Padding the gradient of the output\n",
    "    d_image_padded = np.pad(d_image,((0,0),(0,0),(kernel_height-1-padding,kernel_height-1-padding),(kernel_width-1-padding,kernel_width-1-padding)))\n",
    "\n",
    "    batch_size, output_channels, d_image_height, d_image_width = d_image.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    kernels = np.rot90(kernels,2,(-2,-1))\n",
    "\n",
    "    sliding_windows_d_image = np.lib.stride_tricks.sliding_window_view(d_image_padded,(1,output_channels,kernel_width,kernel_height))\n",
    "    sliding_windows_d_image = sliding_windows_d_image.reshape(-1,(kernel_width*kernel_height*output_channels)) # window matrix\n",
    "    \n",
    "    # Convolution\n",
    "    kernels = kernels.reshape((-1,(kernel_width*kernel_height*output_channels))).transpose(1,0)\n",
    "\n",
    "    gradient_wrt_images = np.matmul(sliding_windows_d_image, kernels).astype(np.float32).transpose(1,0).reshape(batch_of_images.shape)\n",
    "    \n",
    "    ############## Gradient of Kernel ##############\n",
    "    # The computation consists in a convolution between the original image and the dilated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    sliding_windows_batch_of_images = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,1,d_image_height,d_image_width)).reshape(-1,d_image_height*d_image_width)\n",
    "    \n",
    "    d_image = d_image.reshape(-1, d_image_height * d_image_width).transpose(1,0)\n",
    "    \n",
    "    gradient_wrt_kernels = np.matmul(sliding_windows_batch_of_images, d_image).astype(np.float32).transpose(1,0).reshape(output_channels,input_channels,kernel_height,kernel_width)\n",
    "    \n",
    "    ############### Gradient of Bias ###############\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    grdient_wrt_biases = d_image.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gradient_wrt_images, gradient_wrt_kernels, grdient_wrt_biases\n",
    "\n",
    "input_channels = 3\n",
    "output_channels = 4\n",
    "idim = 9\n",
    "kdim = 3\n",
    "s = 3\n",
    "p = 0\n",
    "\n",
    "imAge = np.arange(1,input_channels*idim*idim+1).reshape(1,input_channels,idim,idim)\n",
    "kerNel = np.arange(1,output_channels*input_channels*(kdim**2)+1).reshape(output_channels,input_channels,kdim,kdim)\n",
    "dimAge,mask = im2col_convolution(imAge,kerNel,stride=s,padding=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "\n",
    "ggi,ggradient_wrt_kernels,ggrdient_wrt_biases = im2col_gradient(imAge,dimAge,kerNel,mask,stride=s,padding=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggradient_wrt_kernels: {ggradient_wrt_kernels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d96bd4",
   "metadata": {},
   "source": [
    "### Optimized version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef184b5",
   "metadata": {},
   "source": [
    "Reading the extremely interesting article [Faster Matrix Multiplications in Numpy](https://www.benjaminjohnston.com.au/matmul) by Benjamin Johnson, we extracted fundamental tips and techniques that allowed us to significantly improve the performances of our im2col approach. Following the order of the steps in the article, we now will briefly present them and apply them to our code.\n",
    "\n",
    "1. **Use BLAS directly, instead of relying on BLAS-based operations of NumPy:** *\"BLAS is a high-performance matrix library. Even though NumPy uses BLAS, I've noticed performance can be improved by calling BLAS directly. Perhaps this is simply because using direct calls to BLAS forces you to shape your data ready for use with BLAS.<br>Replace numpy.matmul with `scipy.linalg.blas.sgemm(...)` for float32 matrix-matrix multiplication and `scipy.linalg.blas.sgemv(...)` for float32 matrix-vector multiplication.\"*\n",
    "2. **Use the fastest BLAS possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67935e54",
   "metadata": {},
   "source": [
    "* Specifi-transposition (to optimize BLAS operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "daa1dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_optimized(batch_of_images, kernels, biases=None, padding=0, stride=1, applyReLU=True):\n",
    "    # Ensure all inputs are float64\n",
    "    batch_of_images = batch_of_images.astype(np.float64, copy=False)\n",
    "    kernels = kernels.astype(np.float64, copy=False)\n",
    "    if biases is not None:\n",
    "        biases = biases.astype(np.float64, copy=False)\n",
    "\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    if kernel_channels != input_channels:\n",
    "        raise ValueError(f\"Numero di canali del kernel ({kernel_channels}) non corrisponde ai canali dell'input ({input_channels})\")\n",
    "\n",
    "    output_height = (image_height - kernel_height + 2 * padding) // stride + 1\n",
    "    output_width = (image_width - kernel_width + 2 * padding) // stride + 1\n",
    "\n",
    "    if padding > 0:\n",
    "        batch_of_images_padded = np.pad(batch_of_images, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    else:\n",
    "        batch_of_images_padded = batch_of_images\n",
    "\n",
    "    patches = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )\n",
    "    patches_strided = patches[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col = patches_strided.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * output_height * output_width,\n",
    "        input_channels * kernel_height * kernel_width\n",
    "    ).astype(np.float64, copy=False)\n",
    "\n",
    "    W_col = kernels.reshape(kernels_number, -1).T.astype(np.float64, copy=False)\n",
    "\n",
    "    output_col = np.matmul(X_col, W_col).astype(np.float64, copy=False)\n",
    "\n",
    "    output = output_col.reshape(batch_size, output_height, output_width, kernels_number)\n",
    "    output = output.transpose(0, 3, 1, 2)\n",
    "\n",
    "    if biases is not None and biases.any() != 0:\n",
    "        output = output + biases.reshape(1, -1, 1, 1)\n",
    "\n",
    "    mask = None\n",
    "\n",
    "    if applyReLU:\n",
    "        output_activated = np.maximum(0, output)\n",
    "        mask = (output_activated > 0).astype(np.float64)\n",
    "        output = output_activated\n",
    "    else:\n",
    "        mask = np.ones_like(output, dtype=np.float64)\n",
    "\n",
    "    return output.astype(np.float64, copy=False), mask.astype(np.float64, copy=False)\n",
    "\n",
    "def im2col_gradient_optimized(\n",
    "    original_forward_input,\n",
    "    d_image,\n",
    "    kernels,\n",
    "    mask,\n",
    "    padding,\n",
    "    stride\n",
    "    ):\n",
    "    # Ensure all inputs are float64\n",
    "    original_forward_input = original_forward_input.astype(np.float64, copy=False)\n",
    "    d_image = d_image.astype(np.float64, copy=False)\n",
    "    kernels = kernels.astype(np.float64, copy=False)\n",
    "    mask = mask.astype(np.float64, copy=False)\n",
    "\n",
    "    gradient_pre_activation = np.multiply(d_image, mask).astype(np.float64, copy=False)\n",
    "\n",
    "    kernels_number, input_channels_kernel, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels_orig, input_height, input_width = original_forward_input.shape\n",
    "\n",
    "    gradient_wrt_biases = np.sum(gradient_pre_activation, axis=(0, 2, 3)).astype(np.float64, copy=False)\n",
    "\n",
    "    if padding > 0:\n",
    "        X_padded_for_dW = np.pad(original_forward_input, ((0,0), (0,0), (padding,padding), (padding,padding)), mode='constant')\n",
    "    else:\n",
    "        X_padded_for_dW = original_forward_input\n",
    "\n",
    "    patches_X_for_dW = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_padded_for_dW,\n",
    "        (1, input_channels_orig, kernel_height, kernel_width)\n",
    "    )[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col_for_dW = patches_X_for_dW.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        input_channels_orig * kernel_height * kernel_width\n",
    "    ).astype(np.float64, copy=False)\n",
    "\n",
    "    dLdZ_col = gradient_pre_activation.transpose(0, 2, 3, 1).reshape(\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        kernels_number\n",
    "    ).astype(np.float64, copy=False)\n",
    "\n",
    "    gradient_wrt_kernels_flat = np.matmul(X_col_for_dW.T, dLdZ_col).astype(np.float64, copy=False)\n",
    "\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(\n",
    "        input_channels_orig, kernel_height, kernel_width, kernels_number\n",
    "    ).transpose(3, 0, 1, 2).astype(np.float64, copy=False)\n",
    "\n",
    "    dilated_grad_pre_activation = dilate(gradient_pre_activation, stride).astype(np.float64, copy=False)\n",
    "\n",
    "    flipped_kernels_for_dX = kernels[:,:,::-1,::-1].transpose(1,0,2,3).astype(np.float64, copy=False)\n",
    "\n",
    "    padding_for_dX_conv = kernel_height - 1 - padding\n",
    "\n",
    "    gradient_wrt_input_raw, _ = im2col_optimized(\n",
    "        dilated_grad_pre_activation,\n",
    "        flipped_kernels_for_dX,\n",
    "        biases=None,\n",
    "        padding=padding_for_dX_conv,\n",
    "        stride=1,\n",
    "        applyReLU=False\n",
    "    )\n",
    "\n",
    "    gradient_wrt_input = gradient_wrt_input_raw[:, :, :input_height, :input_width].astype(np.float64, copy=False)\n",
    "\n",
    "    return gradient_wrt_input, gradient_wrt_kernels, gradient_wrt_biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c4a34",
   "metadata": {},
   "source": [
    "* Direct call to underlying BLAS functions, without NumPy intermediation (works best if the tensors are really large):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3edc08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import sgemm\n",
    "\n",
    "def im2col_optimized_sgemm(batch_of_images, kernels, biases=None, padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "    if kernel_channels != input_channels:\n",
    "        raise ValueError(f\"Numero di canali del kernel ({kernel_channels}) non corrisponde ai canali dell'input ({input_channels})\")\n",
    "    output_height = (image_height - kernel_height + 2 * padding) // stride + 1\n",
    "    output_width = (image_width - kernel_width + 2 * padding) // stride + 1\n",
    "    \n",
    "    current_dtype = batch_of_images.dtype # Per mantenere la precisione originale finché possibile\n",
    "    if current_dtype != np.float32: # sgemm richiede float32\n",
    "        current_dtype = np.float32\n",
    "\n",
    "    if padding > 0:\n",
    "        batch_of_images_padded = np.pad(batch_of_images.astype(current_dtype, copy=False),\n",
    "                                        ((0, 0), (0, 0), (padding, padding), (padding, padding)),\n",
    "                                        mode='constant')\n",
    "    else:\n",
    "        batch_of_images_padded = batch_of_images.astype(current_dtype, copy=False)\n",
    "\n",
    "    patches = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )\n",
    "    patches_strided = patches[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col_shape = (batch_size * output_height * output_width, input_channels * kernel_height * kernel_width)\n",
    "    X_col_transposed_view = patches_strided.transpose(0, 2, 3, 5, 6, 7, 1, 4)\n",
    "    # Assicura C-contiguità per la matrice 'a' di sgemm\n",
    "    X_col = np.ascontiguousarray(X_col_transposed_view.reshape(X_col_shape), dtype=np.float32)\n",
    "\n",
    "    # Prepara i kernel per sgemm con trans_b=True\n",
    "    # kernels_reshaped avrà forma (OC, patch_size) ed è C-contigua\n",
    "    kernels_reshaped_for_sgemm = np.ascontiguousarray(kernels.reshape(kernels_number, -1), dtype=np.float32)\n",
    "\n",
    "    # sgemm calcola alpha * A @ B oppure A.T @ B ecc.\n",
    "    # Vogliamo X_col @ W_col, dove W_col = kernels.reshape(OC, -1).T\n",
    "    # Quindi, W_col ha forma (patch_size, OC).\n",
    "    # Passando kernels_reshaped_for_sgemm (OC, patch_size) e trans_b=True,\n",
    "    # sgemm calcolerà X_col @ kernels_reshaped_for_sgemm.T\n",
    "    output_col = sgemm(alpha=1.0, a=X_col, b=kernels_reshaped_for_sgemm, trans_b=True)\n",
    "\n",
    "    output = output_col.reshape(batch_size, output_height, output_width, kernels_number).transpose(0, 3, 1, 2)\n",
    "\n",
    "    if biases is not None and biases.any() != 0:\n",
    "        output = output + biases.astype(np.float32, copy=False).reshape(1, -1, 1, 1)\n",
    "\n",
    "    mask_type = output.dtype\n",
    "    if applyReLU:\n",
    "        # np.maximum può cambiare il dtype se uno degli argomenti è 0 (int). Forziamo float32.\n",
    "        output = np.maximum(0, output, dtype=np.float32)\n",
    "        mask = (output > 0).astype(mask_type) # Maschera binaria con il dtype originale (prima di forzare float32)\n",
    "    else:\n",
    "        mask = np.ones_like(output, dtype=mask_type)\n",
    "\n",
    "    return output.astype(np.float32, copy=False), mask.astype(np.float32, copy=False)\n",
    "\n",
    "def im2col_gradient_optimized_sgemm(original_forward_input, d_image, kernels, mask, padding, stride):\n",
    "    # Assicura che mask sia float32 se usata in moltiplicazioni\n",
    "    gradient_pre_activation = np.multiply(d_image, mask.astype(d_image.dtype, copy=False)) # dL/dZ\n",
    "\n",
    "    kernels_number, input_channels_kernel, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels_orig, input_height, input_width = original_forward_input.shape\n",
    "\n",
    "    gradient_wrt_biases = np.sum(gradient_pre_activation, axis=(0, 2, 3)).astype(np.float32, copy=False)\n",
    "\n",
    "    current_dtype = original_forward_input.dtype\n",
    "    if current_dtype != np.float32:\n",
    "        current_dtype = np.float32\n",
    "\n",
    "    if padding > 0:\n",
    "        X_padded_for_dW = np.pad(original_forward_input.astype(current_dtype, copy=False),\n",
    "                                 ((0,0), (0,0), (padding,padding), (padding,padding)),\n",
    "                                 mode='constant')\n",
    "    else:\n",
    "        X_padded_for_dW = original_forward_input.astype(current_dtype, copy=False)\n",
    "\n",
    "    patches_X_for_dW = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_padded_for_dW,\n",
    "        (1, input_channels_orig, kernel_height, kernel_width)\n",
    "    )[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col_for_dW_shape = (\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        input_channels_orig * kernel_height * kernel_width\n",
    "    )\n",
    "    X_col_for_dW_transposed_view = patches_X_for_dW.transpose(0, 2, 3, 5, 6, 7, 1, 4)\n",
    "    # Assicura C-contiguità per 'a' in sgemm(trans_a=True)\n",
    "    X_col_for_dW = np.ascontiguousarray(X_col_for_dW_transposed_view.reshape(X_col_for_dW_shape), dtype=np.float32)\n",
    "\n",
    "    dLdZ_col_shape = (\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        kernels_number\n",
    "    )\n",
    "    dLdZ_col_transposed_view = gradient_pre_activation.transpose(0, 2, 3, 1)\n",
    "    # Assicura C-contiguità per 'b' in sgemm(trans_a=True)\n",
    "    # (anche se F-contiguità sarebbe ideale per 'b' se 'a' non è trasposta,\n",
    "    # BLAS gestisce C-contigue per entrambe con trans_a=True)\n",
    "    dLdZ_col = np.ascontiguousarray(dLdZ_col_transposed_view.reshape(dLdZ_col_shape), dtype=np.float32)\n",
    "\n",
    "    # Calcola X_col_for_dW.T @ dLdZ_col\n",
    "    gradient_wrt_kernels_flat = sgemm(alpha=1.0, a=X_col_for_dW, b=dLdZ_col, trans_a=True)\n",
    "\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(\n",
    "        input_channels_orig, kernel_height, kernel_width, kernels_number\n",
    "    ).transpose(3, 0, 1, 2).astype(np.float32, copy=False)\n",
    "\n",
    "    # --- Calcolo di gradient_wrt_input (dL/dX) ---\n",
    "    dilated_grad_pre_activation = dilate(gradient_pre_activation.astype(np.float32, copy=False), stride)\n",
    "\n",
    "    # Ottimizzazione per rotazione kernel: W_rot180 = W[:, :, ::-1, ::-1] (flip per KH, KW)\n",
    "    # Poi transpose per scambiare canali input/output per la convoluzione trasposta: (IC, OC, KH, KW)\n",
    "    flipped_kernels_for_dX = kernels.astype(np.float32, copy=False)[:,:,::-1,::-1].transpose(1,0,2,3)\n",
    "    # Assicura che i kernel flippati siano C-contigui per la chiamata ricorsiva a im2col_optimized_v3\n",
    "    flipped_kernels_for_dX = np.ascontiguousarray(flipped_kernels_for_dX)\n",
    "\n",
    "\n",
    "    padding_for_dX_conv = kernel_height - 1 - padding\n",
    "\n",
    "    gradient_wrt_input_raw, _ = im2col_optimized_sgemm(\n",
    "        dilated_grad_pre_activation,\n",
    "        flipped_kernels_for_dX,\n",
    "        biases=None,\n",
    "        padding=padding_for_dX_conv,\n",
    "        stride=1,\n",
    "        applyReLU=False\n",
    "    )\n",
    "\n",
    "    # Crop per ottenere le dimensioni corrette\n",
    "    gradient_wrt_input = gradient_wrt_input_raw[:, :, :input_height, :input_width].astype(np.float32, copy=False)\n",
    "\n",
    "    return gradient_wrt_input, gradient_wrt_kernels, gradient_wrt_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a64bb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from im2col_cython import im2col_optimized_cython, im2col_gradient_optimized_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfbb16",
   "metadata": {},
   "source": [
    "### Optimized version w/o bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c66c1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_optimized_float64_no_bias(batch_of_images, kernels, biases=None, padding=0, stride=1, applyReLU=True):\n",
    "    # Questa è la tua funzione im2col_optimized, ma forziamo float64 internamente\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    if kernel_channels != input_channels:\n",
    "        raise ValueError(\"Channel mismatch\")\n",
    "\n",
    "    output_height = (image_height - kernel_height + 2 * padding) // stride + 1\n",
    "    output_width = (image_width - kernel_width + 2 * padding) // stride + 1\n",
    "\n",
    "    # Assicura float64 dall'inizio\n",
    "    current_dtype = np.float64\n",
    "    batch_of_images_casted = batch_of_images.astype(current_dtype, copy=False)\n",
    "    kernels_casted = kernels.astype(current_dtype, copy=False)\n",
    "    biases_casted = biases.astype(current_dtype, copy=False) if biases is not None else None\n",
    "\n",
    "    if padding > 0:\n",
    "        batch_of_images_padded = np.pad(batch_of_images_casted, \n",
    "                                        ((0, 0), (0, 0), (padding, padding), (padding, padding)), \n",
    "                                        mode='constant')\n",
    "    else:\n",
    "        batch_of_images_padded = batch_of_images_casted\n",
    "\n",
    "    patches = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )\n",
    "    patches_strided = patches[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col = patches_strided.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * output_height * output_width,\n",
    "        input_channels * kernel_height * kernel_width\n",
    "    ).astype(current_dtype, copy=False) # Assicura che X_col sia float64\n",
    "\n",
    "    W_col = kernels_casted.reshape(kernels_number, -1).T.astype(current_dtype, copy=False) # E W_col\n",
    "    \n",
    "    output_col = np.matmul(X_col, W_col) # Matmul manterrà float64 se gli input lo sono\n",
    "\n",
    "    output = output_col.reshape(batch_size, output_height, output_width, kernels_number).transpose(0, 3, 1, 2)\n",
    "\n",
    "    if biases_casted is not None and biases_casted.any() != 0:\n",
    "        output = output + biases_casted.reshape(1, -1, 1, 1)\n",
    "\n",
    "    mask_type = output.dtype # Sarà float64\n",
    "    if applyReLU:\n",
    "        output_activated = np.maximum(0, output) # Mantiene float64\n",
    "        mask = (output_activated > 0).astype(mask_type)\n",
    "        output = output_activated\n",
    "    else:\n",
    "        mask = np.ones_like(output, dtype=mask_type)\n",
    "        \n",
    "    return output, mask # output e mask saranno float64\n",
    "\n",
    "\n",
    "def im2col_gradient_optimized_float64_no_bias(original_forward_input, d_image, kernels, mask, padding, stride):\n",
    "    # Questa è la tua im2col_gradient_optimized, ma forza float64\n",
    "    current_dtype = np.float64\n",
    "\n",
    "    # Assicura che tutti gli input rilevanti siano float64\n",
    "    original_forward_input_f64 = original_forward_input.astype(current_dtype, copy=False)\n",
    "    d_image_f64 = d_image.astype(current_dtype, copy=False)\n",
    "    kernels_f64 = kernels.astype(current_dtype, copy=False)\n",
    "    mask_f64 = mask.astype(current_dtype, copy=False) # La maschera ora sarà 0.0 o 1.0\n",
    "\n",
    "    gradient_pre_activation = np.multiply(d_image_f64, mask_f64).astype(current_dtype, copy=False)\n",
    "\n",
    "    kernels_number, input_channels_kernel, kernel_height, kernel_width = kernels_f64.shape\n",
    "    batch_size, input_channels_orig, input_height, input_width = original_forward_input_f64.shape\n",
    "\n",
    "    # gradient_wrt_biases = np.sum(gradient_pre_activation, axis=(0, 2, 3)).astype(current_dtype, copy=False)\n",
    "\n",
    "    X_padded_for_dW = original_forward_input_f64\n",
    "    if padding > 0:\n",
    "        X_padded_for_dW = np.pad(original_forward_input_f64, \n",
    "                                 ((0,0), (0,0), (padding,padding), (padding,padding)), \n",
    "                                 mode='constant')\n",
    "\n",
    "    patches_X_for_dW = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_padded_for_dW,\n",
    "        (1, input_channels_orig, kernel_height, kernel_width)\n",
    "    )[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col_for_dW = patches_X_for_dW.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        input_channels_orig * kernel_height * kernel_width\n",
    "    ).astype(current_dtype, copy=False)\n",
    "\n",
    "    dLdZ_col = gradient_pre_activation.transpose(0, 2, 3, 1).reshape(\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        kernels_number\n",
    "    ).astype(current_dtype, copy=False)\n",
    "    \n",
    "    gradient_wrt_kernels_flat = np.matmul(X_col_for_dW.T, dLdZ_col)\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(\n",
    "        input_channels_orig, kernel_height, kernel_width, kernels_number\n",
    "    ).transpose(3, 0, 1, 2).astype(current_dtype, copy=False)\n",
    "\n",
    "    dilated_grad_pre_activation = dilate(gradient_pre_activation, stride) # dilate mantiene dtype\n",
    "    \n",
    "    flipped_kernels_for_dX = np.rot90(kernels_f64, 2, axes=(2, 3)).transpose(1, 0, 2, 3)\n",
    "    flipped_kernels_for_dX = flipped_kernels_for_dX.astype(current_dtype, copy=False)\n",
    "\n",
    "    padding_for_dX_conv = kernel_height - 1 - padding\n",
    "    \n",
    "    gradient_wrt_input_raw, _ = im2col_optimized_float64_no_bias( # Usa la versione float64\n",
    "        dilated_grad_pre_activation, \n",
    "        flipped_kernels_for_dX,      \n",
    "        biases=None,\n",
    "        padding=padding_for_dX_conv, \n",
    "        stride=1, \n",
    "        applyReLU=False\n",
    "    )\n",
    "    gradient_wrt_input = gradient_wrt_input_raw[:, :, :input_height, :input_width].astype(current_dtype, copy=False)\n",
    "    \n",
    "    return gradient_wrt_input, gradient_wrt_kernels #, gradient_wrt_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf7862",
   "metadata": {},
   "source": [
    "## MLP Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249b2ef",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3f21f",
   "metadata": {},
   "source": [
    "`ReLU_SoftMax_FullyConnected` executes the forward pass of a two-layer Multi-Layer Perceptron (one hidden layer, one output layer), typically used for classification after feature extraction by convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68206d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array, w1, b1, w2, b2):\n",
    "    first_layer_output = np.matmul(input_array, w1) + b1\n",
    "    first_layer_activation = np.maximum(0, first_layer_output)\n",
    "    second_layer_output = np.matmul(first_layer_activation, w2) + b2\n",
    "    second_layer_activation = softmax(second_layer_output)\n",
    "    \n",
    "    return first_layer_output, first_layer_activation, second_layer_output, second_layer_activation\n",
    "\n",
    "def ReLU_SoftMax_FC_no_bias(input_array, w1, w2):\n",
    "    first_layer_output = np.matmul(input_array, w1)\n",
    "    first_layer_activation = np.maximum(0, first_layer_output)\n",
    "    second_layer_output = np.matmul(first_layer_activation, w2)\n",
    "    second_layer_activation = softmax(second_layer_output)\n",
    "    \n",
    "    return first_layer_output, first_layer_activation, second_layer_output, second_layer_activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3eaeb8",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7b0a1",
   "metadata": {},
   "source": [
    "`ReLU_SoftMax_FC_Backward` computes gradients for the MLP. Inputs: batch size `bs`, predictions `pred` ($P$), true `labels` ($Y$), weights $W_1, W_2$, hidden activation `fa` ($A_1$), hidden pre-activation `fl` ($Z_1$), MLP input `i_mlp` ($X_{mlp}$).\n",
    "\n",
    "**Gradients (from output layer backwards):**\n",
    "\n",
    "1.  $\\frac{\\partial L}{\\partial Z_2} = P - Y$ (`dL_dz2`)\n",
    "2.  $\\frac{\\partial L}{\\partial W_2} = A_1^T \\frac{\\partial L}{\\partial Z_2}$ (`dL_dw2`)\n",
    "3.  $\\frac{\\partial L}{\\partial b_2} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_2}$ (`dL_db2`)\n",
    "4.  $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} W_2^T$ (`dL_dfa`)\n",
    "5.  $\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\text{ReLU}'(Z_1)$ (`dL_dfl`, where $\\text{ReLU}'(Z_1)$ is 1 if $Z_1 > 0$, else 0)\n",
    "6.  $\\frac{\\partial L}{\\partial W_1} = X_{mlp}^T \\frac{\\partial L}{\\partial Z_1}$ (`dL_dw1`)\n",
    "7.  $\\frac{\\partial L}{\\partial b_1} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_1}$ (`dL_db1`)\n",
    "8.  $\\frac{\\partial L}{\\partial X_{mlp}} = \\frac{\\partial L}{\\partial Z_1} W_1^T$ (`dL_i_mlp`) (gradient to pass to conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10736bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(batch_size, predictions, labels, w1, w2, output_after_activation, layer_output, input_to_mlp):\n",
    "    dL_dz2 = predictions - labels[0:batch_size]\n",
    "    dL_dw2 = np.matmul(output_after_activation.T, dL_dz2)\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_doutput_after_activation = np.matmul(dL_dz2, w2.T)\n",
    "    dReLU = (layer_output > 0).astype(float)\n",
    "    dL_dlayer_output = dL_doutput_after_activation * dReLU\n",
    "    dL_dw1 = np.matmul(input_to_mlp.reshape(batch_size, -1).T, dL_dlayer_output)\n",
    "    dL_db1 = np.sum(dL_dlayer_output, axis=0)\n",
    "    dL_input_to_mlp = np.matmul(dL_dlayer_output, w1.T)\n",
    "    return dL_input_to_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2\n",
    "\n",
    "def ReLU_SoftMax_FC_Backward_no_bias(batch_size, predictions, labels, w1, w2, output_after_activation, layer_output, input_to_mlp):\n",
    "    dL_dz2 = predictions - labels[0:batch_size]\n",
    "    dL_dw2 = np.matmul(output_after_activation.T, dL_dz2)\n",
    "    # dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_doutput_after_activation = np.matmul(dL_dz2, w2.T)\n",
    "    dReLU = (layer_output > 0).astype(float)\n",
    "    dL_dlayer_output = dL_doutput_after_activation * dReLU\n",
    "    dL_dw1 = np.matmul(input_to_mlp.reshape(batch_size, -1).T, dL_dlayer_output)\n",
    "    # dL_db1 = np.sum(dL_dlayer_output, axis=0)\n",
    "    dL_input_to_mlp = np.matmul(dL_dlayer_output, w1.T)\n",
    "    return dL_input_to_mlp, dL_dw1, dL_dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd7105",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf96a8c",
   "metadata": {},
   "source": [
    "`crossEntropy` calculates the Categorical Cross-Entropy loss for a single sample.\n",
    "\n",
    "**Formula:** Given predicted probabilities $P=(p_1, ..., p_K)$ and one-hot true label $Y=(y_1, ..., y_K)$:\n",
    "$$ L(P, Y) = - \\sum_{k=1}^{K} y_k \\log(p_k) $$\n",
    "If class $c$ is the true class ($y_c=1$), $L = - \\log(p_c)$.\n",
    "A small epsilon (`1/100000`) is added to $p$ to prevent $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1291609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(predictions,true_labels):\n",
    "    predictions = np.clip(predictions, 1e-8, 1 - 1e-8)\n",
    "    true_labels = np.clip(true_labels, 1e-8, 1 - 1e-8)\n",
    "    # Calcolo della cross-entropy\n",
    "    cross_entropy_loss = -np.sum(true_labels * np.log(predictions))\n",
    "    # Media della cross-entropy su tutti i campioni\n",
    "    mean_cross_entropy_loss = np.mean(cross_entropy_loss)\n",
    "    return mean_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49079",
   "metadata": {},
   "source": [
    "## Inference: Comparing Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e3c9f",
   "metadata": {},
   "source": [
    "This section compares the inference performance and correctness of three CNN implementations: PyTorch, nested_loops and im2col. All use identical pre-trained weights.\n",
    "\n",
    "**Objectives:**\n",
    "1.  **Correctness:** Verify that all three models yield the same predictions.\n",
    "2.  **Performance measurement:** Compare average inference time per image.\n",
    "\n",
    "The loop iterates through test images, runs each model, records predictions and times. This demonstrates the efficiency gains from optimized libraries (PyTorch) and im2col approach over the naive loops one. The `padding` and `stride` parameters are set to match the PyTorch model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab6c9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 2000/2000 [00:06<00:00, 288.18it/s, average_times=pytorch: 0.0002197458 s, im2c: 0.000188076 s, im2c_opt: 0.0001674559 s, correct_predictions=2.4409763905562225%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time in seconds:\n",
      "PyTorch:\t0.0010982897281646728 s,\n",
      "im2col:\t\t0.0009400037527084351 s, \n",
      "im2col_optim:\t0.0008369446992874146 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "PyTorch",
         "type": "scatter",
         "y": [
          0.0039327144622802734,
          0.0009999275207519531,
          0,
          0,
          0,
          0,
          0.0009963512420654297,
          0.000997781753540039,
          0.0010008811950683594,
          0.0010001659393310547,
          0.00099945068359375,
          0,
          0.0010001659393310547,
          0,
          0.0009989738464355469,
          0.0010673999786376953,
          0.0007357597351074219,
          0.0010006427764892578,
          0.0010008811950683594,
          0.0010004043579101562,
          0.0008833408355712891,
          0.0012276172637939453,
          0.0010006427764892578,
          0.0010008811950683594,
          0.0009992122650146484,
          0.001065969467163086,
          0.0010006427764892578,
          0,
          0,
          0,
          0,
          0.0004558563232421875,
          0,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.002008676528930664,
          0,
          0,
          0,
          0,
          0.0010068416595458984,
          0,
          0,
          0,
          0,
          0.014272212982177734,
          0,
          0,
          0,
          0,
          0.014060258865356445,
          0,
          0,
          0,
          0,
          0,
          0.0009663105010986328,
          0,
          0,
          0,
          0.013677597045898438,
          0,
          0,
          0,
          0,
          0,
          0.000614166259765625,
          0,
          0,
          0,
          0,
          0.0014255046844482422,
          0,
          0,
          0,
          0,
          0.0010099411010742188,
          0,
          0,
          0,
          0,
          0.0010097026824951172,
          0,
          0,
          0,
          0,
          0.014191865921020508,
          0,
          0,
          0,
          0,
          0.014159917831420898,
          0,
          0,
          0,
          0,
          0.013745784759521484,
          0,
          0,
          0,
          0,
          0.014239072799682617,
          0,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010097026824951172,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014166593551635742,
          0,
          0,
          0,
          0,
          0,
          0.0004532337188720703,
          0,
          0,
          0,
          0,
          0.0008678436279296875,
          0,
          0,
          0,
          0,
          0.0015146732330322266,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010082721710205078,
          0,
          0,
          0,
          0,
          0.014261007308959961,
          0.0015521049499511719,
          0,
          0,
          0,
          0.013148069381713867,
          0,
          0,
          0,
          0,
          0.014271736145019531,
          0,
          0,
          0,
          0,
          0,
          0.0014650821685791016,
          0,
          0,
          0,
          0,
          0.001010894775390625,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014123916625976562,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014028072357177734,
          0,
          0,
          0,
          0,
          0,
          0.001310586929321289,
          0,
          0,
          0,
          0,
          0.0009942054748535156,
          0,
          0,
          0,
          0,
          0.0010144710540771484,
          0,
          0,
          0,
          0,
          0.01428675651550293,
          0,
          0,
          0,
          0,
          0,
          0.0003955364227294922,
          0,
          0,
          0,
          0,
          0.0013856887817382812,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014211893081665039,
          0,
          0,
          0,
          0,
          0.014157772064208984,
          0,
          0,
          0,
          0,
          0,
          0.0013976097106933594,
          0,
          0,
          0,
          0,
          0.0009963512420654297,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014626502990722656,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001373291015625,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.014282941818237305,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.0014438629150390625,
          0,
          0,
          0,
          0,
          0.0006287097930908203,
          0,
          0,
          0,
          0.014078617095947266,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014182329177856445,
          0,
          0,
          0,
          0,
          0.014258384704589844,
          0,
          0,
          0,
          0,
          0.014244794845581055,
          0,
          0,
          0,
          0,
          0,
          0.0013310909271240234,
          0,
          0,
          0,
          0,
          0.0009996891021728516,
          0,
          0,
          0,
          0,
          0.0009992122650146484,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.014344453811645508,
          0,
          0,
          0,
          0,
          0.014221429824829102,
          0,
          0,
          0,
          0,
          0,
          0.0004782676696777344,
          0,
          0,
          0,
          0,
          0.0009884834289550781,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014213323593139648,
          0,
          0,
          0,
          0,
          0,
          0.0003426074981689453,
          0,
          0,
          0,
          0,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.014219045639038086,
          0,
          0,
          0,
          0,
          0.014249801635742188,
          0,
          0,
          0,
          0,
          0,
          0.0013701915740966797,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.0010118484497070312,
          0,
          0,
          0,
          0,
          0.014321327209472656,
          0,
          0,
          0,
          0,
          0.01422572135925293,
          0,
          0,
          0,
          0,
          0.014128923416137695,
          0,
          0,
          0,
          0,
          0.014183998107910156,
          0,
          0,
          0,
          0,
          0.014178276062011719,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0,
          0.011759281158447266,
          0.001331329345703125,
          0,
          0.0014028549194335938,
          0,
          0,
          0,
          0.0013537406921386719,
          0,
          0,
          0,
          0.0020110607147216797,
          0,
          0,
          0,
          0.014273881912231445,
          0,
          0,
          0,
          0,
          0.0010156631469726562,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014182806015014648,
          0,
          0,
          0,
          0,
          0,
          0.0006742477416992188,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013795852661132812,
          0,
          0,
          0,
          0,
          0.014220714569091797,
          0,
          0,
          0,
          0,
          0,
          0.001354217529296875,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.0020112991333007812,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014258146286010742,
          0,
          0,
          0,
          0,
          0,
          0.0005075931549072266,
          0,
          0,
          0,
          0.014154672622680664,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010170936584472656,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0005202293395996094,
          0,
          0,
          0,
          0,
          0.0004107952117919922,
          0,
          0,
          0,
          0,
          0.0013623237609863281,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.014317989349365234,
          0,
          0,
          0,
          0,
          0.014229536056518555,
          0,
          0,
          0,
          0,
          0,
          0.0014522075653076172,
          0,
          0,
          0,
          0,
          0.0013840198516845703,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0010006427764892578,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014242887496948242,
          0,
          0,
          0,
          0,
          0.014208793640136719,
          0,
          0,
          0,
          0,
          0,
          0.00042366981506347656,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014034032821655273,
          0,
          0,
          0,
          0,
          0,
          0.0013077259063720703,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014299154281616211,
          0,
          0,
          0,
          0,
          0,
          0.001323699951171875,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.014245748519897461,
          0,
          0,
          0,
          0,
          0.014219999313354492,
          0,
          0,
          0,
          0,
          0.0009961128234863281,
          0,
          0,
          0,
          0.014066219329833984,
          0,
          0,
          0,
          0,
          0.014342069625854492,
          0,
          0,
          0,
          0,
          0.014219284057617188,
          0,
          0,
          0,
          0,
          0.014310121536254883,
          0,
          0,
          0,
          0,
          0.014189958572387695,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014265298843383789,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0010101795196533203,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.01432657241821289,
          0,
          0,
          0,
          0,
          0.014137029647827148,
          0,
          0,
          0,
          0,
          0.014282464981079102,
          0.00039839744567871094,
          0,
          0,
          0,
          0,
          0.00044536590576171875,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0,
          0,
          0,
          0,
          0.0013382434844970703,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001600503921508789,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014248371124267578,
          0,
          0,
          0,
          0,
          0.014194726943969727,
          0,
          0,
          0,
          0,
          0.014140844345092773,
          0,
          0,
          0,
          0,
          0.014212369918823242,
          0,
          0,
          0,
          0,
          0.014210224151611328,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01433253288269043,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.014157533645629883,
          0,
          0,
          0,
          0,
          0,
          0.0003287792205810547,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.014345645904541016,
          0,
          0,
          0,
          0,
          0,
          0.00045180320739746094,
          0,
          0,
          0,
          0,
          0.0013463497161865234,
          0,
          0,
          0,
          0,
          0.0010159015655517578,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014223337173461914,
          0,
          0,
          0,
          0,
          0.014215230941772461,
          0,
          0,
          0.007720232009887695,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0020122528076171875,
          0,
          0,
          0,
          0.014171361923217773,
          0,
          0,
          0,
          0.014221668243408203,
          0.0010030269622802734,
          0.0003800392150878906,
          0,
          0.009273767471313477,
          0.0019986629486083984,
          0,
          0,
          0,
          0.001001596450805664,
          0,
          0,
          0,
          0,
          0.0010025501251220703,
          0,
          0,
          0,
          0.0009975433349609375,
          0.0009839534759521484,
          0,
          0,
          0,
          0.00099945068359375,
          0,
          0,
          0,
          0,
          0.0010013580322265625,
          0.00039505958557128906,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.0009999275207519531,
          0.0010006427764892578,
          0,
          0,
          0,
          0.001997232437133789,
          0.0014889240264892578,
          0,
          0,
          0.010178804397583008,
          0,
          0.0013403892517089844,
          0,
          0,
          0.009371757507324219,
          0.00401759147644043,
          0.0019981861114501953,
          0,
          0.008769035339355469,
          0.001999378204345703,
          0,
          0,
          0.0010111331939697266,
          0.0009953975677490234,
          0,
          0,
          0.009385347366333008,
          0.001043558120727539,
          0.001310586929321289,
          0,
          0,
          0.009428977966308594,
          0.0009987354278564453,
          0,
          0,
          0,
          0.001010894775390625,
          0,
          0,
          0,
          0,
          0.0009860992431640625,
          0.00036525726318359375,
          0,
          0,
          0.01030111312866211,
          0.0009658336639404297,
          0.001298666000366211,
          0,
          0,
          0,
          0.0009570121765136719,
          0,
          0,
          0,
          0.0010101795196533203,
          0.0010173320770263672,
          0,
          0,
          0,
          0.0010209083557128906,
          0.0009772777557373047,
          0,
          0,
          0,
          0.0009982585906982422,
          0.0009822845458984375,
          0,
          0,
          0.009753704071044922,
          0.0009553432464599609,
          0.0013804435729980469,
          0,
          0,
          0.0009570121765136719,
          0.0009775161743164062,
          0,
          0,
          0,
          0.0019989013671875,
          0.0013103485107421875,
          0,
          0,
          0.010674715042114258,
          0.000997304916381836,
          0,
          0,
          0,
          0.0017507076263427734,
          0,
          0,
          0,
          0.0008661746978759766,
          0,
          0,
          0,
          0.0009982585906982422,
          0.0009846687316894531,
          0,
          0,
          0.0010154247283935547,
          0.0009784698486328125,
          0,
          0,
          0.009293556213378906,
          0.0010395050048828125,
          0,
          0,
          0,
          0.009283781051635742,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0009758472442626953,
          0,
          0,
          0,
          0.001024007797241211,
          0.0009572505950927734,
          0,
          0,
          0,
          0.0009996891021728516,
          0.0009772777557373047,
          0,
          0,
          0.009293317794799805,
          0.0009560585021972656,
          0,
          0,
          0,
          0.001008749008178711,
          0.0009987354278564453,
          0.0013747215270996094,
          0,
          0,
          0.0010094642639160156,
          0.001001119613647461,
          0,
          0,
          0.0020062923431396484,
          0.00099945068359375,
          0.0013353824615478516,
          0,
          0,
          0.0009984970092773438,
          0.0010023117065429688,
          0,
          0,
          0.008437395095825195,
          0.001035451889038086,
          0,
          0,
          0,
          0.0010356903076171875,
          0.000997304916381836,
          0,
          0,
          0,
          0.0011310577392578125,
          0.0009551048278808594,
          0,
          0,
          0,
          0.0009582042694091797,
          0.0009484291076660156,
          0,
          0,
          0,
          0.0010025501251220703,
          0.0009810924530029297,
          0,
          0,
          0.009419918060302734,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0010612010955810547,
          0.0009751319885253906,
          0,
          0,
          0,
          0.0009586811065673828,
          0.0009565353393554688,
          0,
          0,
          0.009237051010131836,
          0.0009982585906982422,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0010035037994384766,
          0.0009753704071044922,
          0,
          0,
          0.009348869323730469,
          0.0009844303131103516,
          0.001356363296508789,
          0,
          0,
          0.0009970664978027344,
          0.0009832382202148438,
          0,
          0,
          0.009333133697509766,
          0.0009980201721191406,
          0,
          0,
          0,
          0.0009984970092773438,
          0.0009903907775878906,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0.0009756088256835938,
          0.0010170936584472656,
          0,
          0,
          0.0009799003601074219,
          0,
          0,
          0,
          0.0009772777557373047,
          0.0016307830810546875,
          0.0012066364288330078,
          0.000997781753540039,
          0.0013947486877441406,
          0,
          0.0013086795806884766,
          0,
          0,
          0.0009987354278564453,
          0.0009768009185791016,
          0,
          0,
          0.009300470352172852,
          0.0009765625,
          0.0003886222839355469,
          0,
          0,
          0.01024627685546875,
          0.0009992122650146484,
          0.0009984970092773438,
          0,
          0,
          0.009278297424316406,
          0.0009663105010986328,
          0,
          0,
          0,
          0.0010488033294677734,
          0,
          0,
          0,
          0,
          0.0010399818420410156,
          0.000982522964477539,
          0,
          0.003596782684326172,
          0.0008633136749267578,
          0.010932445526123047,
          0.0007014274597167969,
          0,
          0.0020012855529785156,
          0,
          0,
          0.0009968280792236328,
          0.0005135536193847656,
          0,
          0.009627342224121094,
          0.0010004043579101562,
          0,
          0,
          0,
          0.0010180473327636719,
          0,
          0,
          0,
          0,
          0.0009560585021972656,
          0.0009784698486328125,
          0,
          0,
          0.009267807006835938,
          0,
          0.0009768009185791016,
          0,
          0,
          0.009385824203491211,
          0.00099945068359375,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0.00035834312438964844,
          0,
          0,
          0.0009987354278564453,
          0.0014843940734863281,
          0,
          0,
          0.010405540466308594,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0009989738464355469,
          0.0009756088256835938,
          0,
          0,
          0.009365081787109375,
          0.0009524822235107422,
          0,
          0,
          0.011121034622192383,
          0,
          0,
          0,
          0.014556646347045898,
          0.0009818077087402344,
          0,
          0,
          0.0010111331939697266,
          0.0010116100311279297,
          0,
          0,
          0,
          0.0009534358978271484,
          0.0014176368713378906,
          0,
          0.002004861831665039,
          0.001009225845336914,
          0.0009751319885253906,
          0,
          0,
          0.010258674621582031,
          0.0009775161743164062,
          0.0009777545928955078,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0.0010077953338623047,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0.0009729862213134766,
          0,
          0,
          0,
          0.0010523796081542969,
          0.0009765625,
          0,
          0,
          0.009364843368530273,
          0.0010020732879638672,
          0.0009796619415283203,
          0,
          0,
          0.00973963737487793,
          0.0009989738464355469,
          0.0013055801391601562,
          0,
          0,
          0.0011131763458251953,
          0,
          0,
          0,
          0,
          0.0009548664093017578,
          0.0009989738464355469,
          0,
          0,
          0.009272575378417969,
          0.000997781753540039,
          0,
          0,
          0,
          0.0010080337524414062,
          0,
          0,
          0,
          0,
          0.0009777545928955078,
          0.0009999275207519531,
          0,
          0,
          0.0009996891021728516,
          0.0009999275207519531,
          0,
          0,
          0.0009999275207519531,
          0.00099945068359375,
          0.0020079612731933594,
          0.0020062923431396484,
          0.0010802745819091797,
          0.0009992122650146484,
          0.0021653175354003906,
          0,
          0.001009225845336914,
          0.0019996166229248047,
          0,
          0,
          0.001008749008178711,
          0.002009868621826172,
          0,
          0,
          0.0009982585906982422,
          0,
          0,
          0,
          0.0010116100311279297,
          0.0009729862213134766,
          0,
          0,
          0,
          0.00098419189453125,
          0.00045299530029296875,
          0,
          0,
          0.0010111331939697266,
          0.0009996891021728516,
          0,
          0,
          0,
          0.0019989013671875,
          0.0020220279693603516,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0.009133338928222656,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009524822235107422,
          0.0014913082122802734,
          0,
          0,
          0.010149717330932617,
          0.001001119613647461,
          0,
          0,
          0,
          0.0010113716125488281,
          0,
          0,
          0,
          0,
          0.0009832382202148438,
          0.0009992122650146484,
          0,
          0,
          0.009206533432006836,
          0.0009527206420898438,
          0,
          0,
          0,
          0.009328603744506836,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010089874267578125,
          0.0010006427764892578,
          0,
          0,
          0,
          0.0009608268737792969,
          0.001401662826538086,
          0,
          0,
          0,
          0.00099945068359375,
          0.0006871223449707031,
          0,
          0,
          0,
          0.0009818077087402344,
          0,
          0,
          0,
          0.0009968280792236328,
          0.000995635986328125,
          0,
          0,
          0.009782791137695312,
          0.0010008811950683594,
          0.0004870891571044922,
          0,
          0,
          0.001009225845336914,
          0.0009508132934570312,
          0,
          0,
          0,
          0.0010216236114501953,
          0.0009524822235107422,
          0,
          0,
          0.0010104179382324219,
          0.0009996891021728516,
          0.0014090538024902344,
          0,
          0,
          0.0009982585906982422,
          0,
          0,
          0,
          0.0010094642639160156,
          0.0009818077087402344,
          0,
          0,
          0,
          0.0009810924530029297,
          0.0005087852478027344,
          0,
          0,
          0.011275529861450195,
          0.00103759765625,
          0,
          0,
          0,
          0.0009992122650146484,
          0.0009737014770507812,
          0,
          0,
          0.009125947952270508,
          0,
          0.0004992485046386719,
          0,
          0,
          0.0010106563568115234,
          0.0010020732879638672,
          0,
          0,
          0,
          0.0009491443634033203,
          0,
          0,
          0,
          0,
          0.0010175704956054688,
          0.001470804214477539,
          0,
          0,
          0.0010542869567871094,
          0.0009667873382568359,
          0,
          0,
          0,
          0.0010018348693847656,
          0.0004417896270751953,
          0.0020046234130859375,
          0,
          0.0009987354278564453,
          0.0011181831359863281,
          0,
          0,
          0.0010175704956054688,
          0.0008819103240966797,
          0,
          0,
          0.010118961334228516,
          0.0010008811950683594,
          0.00039577484130859375,
          0,
          0,
          0.0009491443634033203,
          0,
          0,
          0,
          0.001009225845336914,
          0.0009996891021728516,
          0,
          0,
          0,
          0.0009713172912597656,
          0,
          0,
          0,
          0,
          0,
          0,
          0.012903690338134766,
          0.001001596450805664,
          0,
          0,
          0,
          0.01270294189453125,
          0.0003581047058105469,
          0,
          0,
          0,
          0.0009746551513671875,
          0,
          0,
          0,
          0.0010004043579101562,
          0,
          0,
          0,
          0.012598037719726562,
          0.0009989738464355469,
          0,
          0,
          0,
          0.00096893310546875,
          0.0010328292846679688,
          0,
          0,
          0.011629819869995117,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01255345344543457,
          0.0008540153503417969,
          0,
          0,
          0,
          0.013892173767089844,
          0,
          0,
          0,
          0,
          0.014034032821655273,
          0,
          0,
          0,
          0,
          0.013918399810791016,
          0,
          0,
          0,
          0,
          0.0140380859375,
          0.000997304916381836,
          0.0009875297546386719,
          0,
          0.0009570121765136719,
          0.0009865760803222656,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0022161006927490234,
          0,
          0,
          0,
          0.0009198188781738281,
          0.0035343170166015625,
          0,
          0,
          0,
          0.0006244182586669922,
          0,
          0,
          0,
          0.013982295989990234,
          0.0007874965667724609,
          0,
          0,
          0,
          0.013918161392211914,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009679794311523438,
          0,
          0,
          0,
          0,
          0.0007452964782714844,
          0,
          0,
          0,
          0.013853073120117188,
          0,
          0,
          0,
          0,
          0.013939619064331055,
          0,
          0,
          0,
          0,
          0.014078378677368164,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0011470317840576172,
          0,
          0,
          0,
          0,
          0.0011522769927978516,
          0,
          0,
          0,
          0,
          0.0011620521545410156,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014064788818359375,
          0,
          0,
          0,
          0,
          0.013947725296020508,
          0.0007262229919433594,
          0,
          0,
          0,
          0.013774633407592773,
          0.0006396770477294922,
          0,
          0,
          0,
          0,
          0.0016222000122070312,
          0,
          0,
          0,
          0,
          0.0004868507385253906,
          0,
          0,
          0,
          0,
          0.0010106563568115234,
          0,
          0,
          0,
          0,
          0.0011563301086425781,
          0,
          0,
          0,
          0,
          0.001142740249633789,
          0,
          0,
          0,
          0,
          0.014001607894897461,
          0,
          0,
          0,
          0,
          0.013990640640258789,
          0,
          0,
          0,
          0,
          0.0020842552185058594,
          0,
          0,
          0,
          0.013555288314819336,
          0,
          0,
          0,
          0,
          0.01400303840637207,
          0,
          0,
          0,
          0,
          0.01405024528503418,
          0,
          0,
          0,
          0,
          0.013669490814208984,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0010406970977783203,
          0,
          0,
          0,
          0,
          0.002009153366088867,
          0,
          0,
          0,
          0,
          0.0009622573852539062,
          0,
          0,
          0,
          0,
          0.0010619163513183594,
          0,
          0,
          0,
          0,
          0.0009677410125732422,
          0,
          0,
          0,
          0,
          0.0009486675262451172,
          0,
          0,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014028310775756836,
          0,
          0,
          0,
          0,
          0.014060497283935547,
          0,
          0,
          0,
          0,
          0.014008283615112305,
          0.0006551742553710938,
          0,
          0,
          0,
          0,
          0.0007028579711914062,
          0,
          0,
          0,
          0,
          0.0006127357482910156,
          0,
          0,
          0,
          0,
          0.0015063285827636719,
          0,
          0,
          0,
          0,
          0.0006101131439208984,
          0,
          0,
          0,
          0,
          0.0010030269622802734,
          0,
          0,
          0,
          0,
          0.0011491775512695312,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01413583755493164,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014068841934204102,
          0,
          0,
          0,
          0,
          0.014034748077392578,
          0,
          0,
          0
         ]
        },
        {
         "line": {
          "color": "green"
         },
         "mode": "lines+markers",
         "name": "im2col",
         "type": "scatter",
         "y": [
          0.0020074844360351562,
          0.0010006427764892578,
          0,
          0,
          0.005683183670043945,
          0.00099945068359375,
          0.0010006427764892578,
          0.0009999275207519531,
          0.0009996891021728516,
          0.0009999275207519531,
          0.0010023117065429688,
          0,
          0.00099945068359375,
          0.0010330677032470703,
          0.0010023117065429688,
          0.0009980201721191406,
          0.0010290145874023438,
          0.0009989738464355469,
          0.0009989738464355469,
          0.00099945068359375,
          0.0010836124420166016,
          0.0012149810791015625,
          0.0010004043579101562,
          0.0010001659393310547,
          0.0011305809020996094,
          0.0010020732879638672,
          0.0013964176177978516,
          0,
          0,
          0,
          0.013180255889892578,
          0,
          0,
          0,
          0,
          0.014160871505737305,
          0.0009310245513916016,
          0,
          0,
          0,
          0,
          0.0005421638488769531,
          0,
          0,
          0,
          0,
          0.00099945068359375,
          0,
          0,
          0,
          0,
          0.002008676528930664,
          0,
          0,
          0,
          0,
          0.0010082721710205078,
          0,
          0,
          0,
          0,
          0.01424264907836914,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.014031648635864258,
          0,
          0,
          0,
          0,
          0.014025449752807617,
          0,
          0,
          0,
          0,
          0,
          0.001481771469116211,
          0,
          0,
          0,
          0,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0.0010075569152832031,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.0010094642639160156,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014358758926391602,
          0,
          0,
          0,
          0,
          0,
          0.0013358592987060547,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0.0010097026824951172,
          0,
          0,
          0,
          0,
          0.014343500137329102,
          0,
          0,
          0,
          0,
          0.014225959777832031,
          0,
          0,
          0,
          0,
          0.013839006423950195,
          0,
          0,
          0,
          0,
          0,
          0.0004444122314453125,
          0,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0010101795196533203,
          0,
          0,
          0,
          0,
          0.014401674270629883,
          0,
          0,
          0,
          0,
          0,
          0.0014786720275878906,
          0,
          0,
          0,
          0,
          0.001016855239868164,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.014303445816040039,
          0,
          0,
          0,
          0,
          0.0010120868682861328,
          0,
          0,
          0,
          0,
          0.014256715774536133,
          0,
          0,
          0,
          0,
          0.014296293258666992,
          0.0004096031188964844,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0010166168212890625,
          0,
          0,
          0,
          0,
          0.014265060424804688,
          0,
          0,
          0,
          0,
          0.014293432235717773,
          0,
          0,
          0,
          0,
          0,
          0.0005095005035400391,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.014361143112182617,
          0,
          0,
          0,
          0,
          0,
          0.0003876686096191406,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.014314413070678711,
          0,
          0,
          0,
          0,
          0.014662504196166992,
          0,
          0,
          0,
          0,
          0,
          0.0014014244079589844,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0.0010118484497070312,
          0,
          0,
          0,
          0,
          0.0006234645843505859,
          0,
          0,
          0,
          0.014107704162597656,
          0,
          0,
          0,
          0,
          0.01407623291015625,
          0,
          0,
          0,
          0,
          0.0010120868682861328,
          0,
          0,
          0,
          0,
          0.0010118484497070312,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.0010159015655517578,
          0,
          0,
          0,
          0,
          0.0010170936584472656,
          0,
          0,
          0,
          0,
          0.014389514923095703,
          0,
          0,
          0,
          0,
          0,
          0.0004553794860839844,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0013287067413330078,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.014223098754882812,
          0,
          0,
          0,
          0,
          0,
          0.0005185604095458984,
          0,
          0,
          0,
          0,
          0.00040793418884277344,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.014350175857543945,
          0,
          0,
          0,
          0,
          0.014227867126464844,
          0,
          0,
          0,
          0,
          0,
          0.0014302730560302734,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014371871948242188,
          0,
          0,
          0,
          0,
          0,
          0.00039696693420410156,
          0,
          0,
          0,
          0,
          0.00145721435546875,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0.0010161399841308594,
          0,
          0,
          0,
          0,
          0.0010149478912353516,
          0,
          0,
          0,
          0,
          0.01427602767944336,
          0,
          0,
          0,
          0,
          0,
          0.0003230571746826172,
          0,
          0,
          0,
          0,
          0.0014159679412841797,
          0,
          0,
          0,
          0.014834403991699219,
          0,
          0.0019979476928710938,
          0.002449512481689453,
          0,
          0.0010411739349365234,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0005183219909667969,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.000993967056274414,
          0,
          0,
          0,
          0,
          0.0010132789611816406,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.01443338394165039,
          0,
          0,
          0,
          0,
          0.013868331909179688,
          0,
          0,
          0,
          0,
          0.015257596969604492,
          0,
          0,
          0,
          0,
          0.0010161399841308594,
          0,
          0,
          0,
          0,
          0.0010144710540771484,
          0,
          0,
          0,
          0,
          0.014390707015991211,
          0,
          0,
          0,
          0,
          0,
          0.00044083595275878906,
          0,
          0,
          0,
          0,
          0.0013990402221679688,
          0,
          0,
          0,
          0,
          0.0009136199951171875,
          0,
          0,
          0,
          0,
          0.0014917850494384766,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.014266490936279297,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.015363216400146484,
          0,
          0,
          0,
          0,
          0.0015544891357421875,
          0,
          0,
          0,
          0,
          0.0005726814270019531,
          0,
          0,
          0,
          0.014169931411743164,
          0,
          0,
          0,
          0,
          0.013900041580200195,
          0,
          0,
          0,
          0,
          0.014250755310058594,
          0,
          0,
          0,
          0,
          0,
          0.0005955696105957031,
          0,
          0,
          0,
          0,
          0.0013632774353027344,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0,
          0,
          0,
          0,
          0.0010173320770263672,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014301300048828125,
          0,
          0,
          0,
          0,
          0.014200925827026367,
          0,
          0,
          0,
          0,
          0,
          0.0004849433898925781,
          0,
          0,
          0,
          0.014173507690429688,
          0.00043010711669921875,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010190010070800781,
          0,
          0,
          0,
          0,
          0.0010111331939697266,
          0,
          0,
          0,
          0,
          0.01431417465209961,
          0,
          0,
          0,
          0,
          0.014181375503540039,
          0,
          0,
          0,
          0,
          0.0010149478912353516,
          0,
          0,
          0,
          0,
          0.014400243759155273,
          0,
          0,
          0,
          0,
          0,
          0.0003955364227294922,
          0,
          0,
          0,
          0,
          0.0014338493347167969,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014304161071777344,
          0,
          0,
          0,
          0,
          0,
          0.0013594627380371094,
          0,
          0,
          0,
          0,
          0.0009963512420654297,
          0,
          0,
          0,
          0,
          0.0010156631469726562,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0005354881286621094,
          0,
          0,
          0,
          0.0010132789611816406,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.0011343955993652344,
          0,
          0,
          0,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0010006427764892578,
          0,
          0,
          0,
          0,
          0.0014524459838867188,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014292478561401367,
          0,
          0,
          0,
          0,
          0,
          0.0004017353057861328,
          0,
          0,
          0,
          0,
          0.0014729499816894531,
          0,
          0,
          0,
          0.014180421829223633,
          0.0004000663757324219,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001619100570678711,
          0,
          0,
          0,
          0.014014959335327148,
          0,
          0,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.001425027847290039,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010216236114501953,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.0010120868682861328,
          0,
          0,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0.013613700866699219,
          0,
          0,
          0,
          0,
          0.001016378402709961,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.014338254928588867,
          0,
          0,
          0,
          0,
          0,
          0.0013859272003173828,
          0,
          0,
          0,
          0,
          0.0013124942779541016,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.014360904693603516,
          0,
          0,
          0,
          0,
          0.014224767684936523,
          0,
          0,
          0,
          0,
          0,
          0.0004942417144775391,
          0,
          0,
          0,
          0,
          0.0014145374298095703,
          0,
          0,
          0,
          0,
          0.0014805793762207031,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.0010149478912353516,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0.0004680156707763672,
          0,
          0,
          0,
          0.00200653076171875,
          0,
          0,
          0,
          0.0010111331939697266,
          0.00101470947265625,
          0,
          0,
          0.0010097026824951172,
          0.00099945068359375,
          0,
          0,
          0.01041412353515625,
          0.0010008811950683594,
          0,
          0,
          0,
          0.0009965896606445312,
          0.0009999275207519531,
          0,
          0,
          0.00928497314453125,
          0.0010006427764892578,
          0.0014238357543945312,
          0,
          0,
          0.0010137557983398438,
          0.0010006427764892578,
          0,
          0,
          0,
          0.002011537551879883,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010275840759277344,
          0.0009975433349609375,
          0,
          0,
          0.009675741195678711,
          0.0009980201721191406,
          0.000997781753540039,
          0,
          0,
          0.009200096130371094,
          0.0010013580322265625,
          0,
          0,
          0,
          0.0010135173797607422,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.003879070281982422,
          0.0020017623901367188,
          0,
          0.0020110607147216797,
          0.0015571117401123047,
          0,
          0.009074211120605469,
          0.0008144378662109375,
          0.0013055801391601562,
          0,
          0,
          0.0010101795196533203,
          0.0009579658508300781,
          0,
          0,
          0,
          0.0010340213775634766,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0010223388671875,
          0,
          0,
          0,
          0.009186267852783203,
          0.0010037422180175781,
          0,
          0,
          0,
          0.0010118484497070312,
          0.0010230541229248047,
          0,
          0,
          0,
          0.0010197162628173828,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010209083557128906,
          0.0010099411010742188,
          0,
          0,
          0.009297370910644531,
          0.0009784698486328125,
          0.0010230541229248047,
          0,
          0,
          0.009252548217773438,
          0.0009982585906982422,
          0.0013852119445800781,
          0,
          0,
          0.0010411739349365234,
          0.0010230541229248047,
          0,
          0,
          0,
          0.001047372817993164,
          0.0013990402221679688,
          0,
          0,
          0.010181665420532227,
          0.0009989738464355469,
          0,
          0,
          0,
          0.00106048583984375,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013782978057861328,
          0,
          0,
          0,
          0.013932228088378906,
          0.002023458480834961,
          0.0010199546813964844,
          0,
          0,
          0.0010225772857666016,
          0.0009975433349609375,
          0,
          0,
          0.001008749008178711,
          0.0009868144989013672,
          0.0014281272888183594,
          0,
          0,
          0.00103759765625,
          0.0009996891021728516,
          0,
          0,
          0,
          0.002000570297241211,
          0.0013229846954345703,
          0,
          0,
          0.010417461395263672,
          0.0010082721710205078,
          0.00099945068359375,
          0,
          0,
          0.010611772537231445,
          0.0009987354278564453,
          0.0003268718719482422,
          0,
          0,
          0.0010106563568115234,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0009984970092773438,
          0.0010106563568115234,
          0,
          0,
          0,
          0.0010001659393310547,
          0.0009989738464355469,
          0.002006053924560547,
          0.0020051002502441406,
          0.001302957534790039,
          0.002000093460083008,
          0.002006053924560547,
          0.002536296844482422,
          0.003776073455810547,
          0.0009999275207519531,
          0.00036454200744628906,
          0,
          0,
          0.0010075569152832031,
          0.002000093460083008,
          0,
          0,
          0,
          0.0009818077087402344,
          0.0014309883117675781,
          0,
          0,
          0,
          0.00087738037109375,
          0.0009989738464355469,
          0,
          0,
          0.009512186050415039,
          0.0009996891021728516,
          0.001374959945678711,
          0,
          0,
          0.010282278060913086,
          0.000997781753540039,
          0.00031065940856933594,
          0,
          0,
          0.0010709762573242188,
          0.001001596450805664,
          0,
          0,
          0,
          0.0010280609130859375,
          0.0009746551513671875,
          0,
          0,
          0,
          0.0019483566284179688,
          0.0013225078582763672,
          0,
          0,
          0.010264158248901367,
          0.0009992122650146484,
          0.00141143798828125,
          0,
          0,
          0.0010111331939697266,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010004043579101562,
          0.0009527206420898438,
          0,
          0,
          0.009244441986083984,
          0.0009951591491699219,
          0.0013043880462646484,
          0,
          0,
          0.0010080337524414062,
          0.0010259151458740234,
          0,
          0,
          0,
          0.0010030269622802734,
          0.0013051033020019531,
          0,
          0,
          0.0010082721710205078,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0009999275207519531,
          0.0013875961303710938,
          0,
          0.009343147277832031,
          0,
          0,
          0,
          0.014590024948120117,
          0.0010344982147216797,
          0,
          0,
          0.009641885757446289,
          0.002017974853515625,
          0,
          0,
          0.009136676788330078,
          0.0010280609130859375,
          0,
          0.0009958744049072266,
          0.0010008811950683594,
          0.001016855239868164,
          0.001024007797241211,
          0,
          0,
          0,
          0.0010023117065429688,
          0.0010018348693847656,
          0,
          0,
          0,
          0.000997781753540039,
          0,
          0,
          0,
          0.001054525375366211,
          0.001001596450805664,
          0.0003352165222167969,
          0,
          0,
          0.001008749008178711,
          0.0009996891021728516,
          0,
          0.0020074844360351562,
          0,
          0.0009586811065673828,
          0.0009558200836181641,
          0,
          0,
          0,
          0.001970052719116211,
          0.0009992122650146484,
          0,
          0.0019989013671875,
          0,
          0.0019383430480957031,
          0.002802610397338867,
          0.007142782211303711,
          0.0020003318786621094,
          0,
          0.010139942169189453,
          0.0010213851928710938,
          0,
          0,
          0.0010838508605957031,
          0.0010194778442382812,
          0,
          0,
          0,
          0.0009784698486328125,
          0.001983165740966797,
          0,
          0,
          0.00931096076965332,
          0.0010204315185546875,
          0.0013561248779296875,
          0,
          0,
          0.0010111331939697266,
          0.0010249614715576172,
          0.00032258033752441406,
          0,
          0,
          0.0010416507720947266,
          0.0010030269622802734,
          0,
          0,
          0,
          0.0010325908660888672,
          0.001024007797241211,
          0,
          0,
          0,
          0.0010027885437011719,
          0,
          0,
          0,
          0.0010614395141601562,
          0.0010008811950683594,
          0,
          0,
          0,
          0.0009989738464355469,
          0.0009982585906982422,
          0,
          0,
          0.0010099411010742188,
          0.001055002212524414,
          0,
          0,
          0.0010106563568115234,
          0,
          0,
          0,
          0.0009982585906982422,
          0.0010025501251220703,
          0,
          0,
          0.0010013580322265625,
          0.0009868144989013672,
          0,
          0,
          0.009285211563110352,
          0,
          0,
          0.005467414855957031,
          0,
          0.000997304916381836,
          0.0013704299926757812,
          0,
          0,
          0.0010106563568115234,
          0.0009996891021728516,
          0.0003097057342529297,
          0,
          0,
          0.0010101795196533203,
          0.0010004043579101562,
          0,
          0,
          0,
          0.00103759765625,
          0.0019648075103759766,
          0,
          0,
          0.010623931884765625,
          0.0010004043579101562,
          0.0010213851928710938,
          0,
          0,
          0.00930929183959961,
          0.0009486675262451172,
          0.00099945068359375,
          0,
          0,
          0.0010101795196533203,
          0.0009982585906982422,
          0,
          0,
          0,
          0.001058340072631836,
          0.0010251998901367188,
          0,
          0,
          0,
          0.000997304916381836,
          0.0009992122650146484,
          0,
          0,
          0,
          0.002002716064453125,
          0.0014598369598388672,
          0,
          0,
          0.0010364055633544922,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010235309600830078,
          0.0010237693786621094,
          0,
          0,
          0.010153770446777344,
          0.0009992122650146484,
          0.0015723705291748047,
          0,
          0.009086847305297852,
          0.0010004043579101562,
          0.0016236305236816406,
          0.002007722854614258,
          0,
          0.0009989738464355469,
          0.0009989738464355469,
          0,
          0,
          0.0010247230529785156,
          0.0013501644134521484,
          0,
          0,
          0.0020012855529785156,
          0.001024007797241211,
          0,
          0,
          0.001001119613647461,
          0.0013480186462402344,
          0,
          0,
          0.001001119613647461,
          0,
          0,
          0,
          0.0010211467742919922,
          0.001001119613647461,
          0,
          0,
          0.010586261749267578,
          0.0009975433349609375,
          0,
          0,
          0,
          0.0010266304016113281,
          0.0009982585906982422,
          0,
          0,
          0.009842872619628906,
          0.0010457038879394531,
          0.0005402565002441406,
          0,
          0,
          0.001001596450805664,
          0.0009818077087402344,
          0,
          0,
          0.001046895980834961,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0010013580322265625,
          0,
          0,
          0,
          0.001009225845336914,
          0.0009975433349609375,
          0,
          0,
          0,
          0.0009963512420654297,
          0.0009613037109375,
          0,
          0,
          0.01019287109375,
          0.0010480880737304688,
          0.0014345645904541016,
          0,
          0,
          0.0010101795196533203,
          0.0010218620300292969,
          0.0014238357543945312,
          0,
          0,
          0.0010383129119873047,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0010235309600830078,
          0.0009996891021728516,
          0,
          0,
          0.009328603744506836,
          0.0010027885437011719,
          0,
          0,
          0,
          0.0010097026824951172,
          0.0010225772857666016,
          0,
          0,
          0,
          0.0010263919830322266,
          0.0009999275207519531,
          0,
          0,
          0.010509729385375977,
          0.0010023117065429688,
          0.0017514228820800781,
          0,
          0,
          0.001008749008178711,
          0.0010502338409423828,
          0,
          0,
          0,
          0.0010018348693847656,
          0.0014007091522216797,
          0,
          0,
          0.010300159454345703,
          0.0009772777557373047,
          0.0010030269622802734,
          0,
          0,
          0.0010209083557128906,
          0.001024484634399414,
          0,
          0,
          0,
          0.0010018348693847656,
          0.0005474090576171875,
          0,
          0,
          0.0010204315185546875,
          0.0015971660614013672,
          0,
          0,
          0.009991168975830078,
          0.0009982585906982422,
          0,
          0,
          0,
          0.0010180473327636719,
          0.0009610652923583984,
          0,
          0,
          0.010123491287231445,
          0.0010497570037841797,
          0.0014102458953857422,
          0,
          0,
          0.001010894775390625,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0010197162628173828,
          0.0010223388671875,
          0,
          0,
          0,
          0.0009996891021728516,
          0.0009815692901611328,
          0,
          0,
          0.00921177864074707,
          0.0010197162628173828,
          0,
          0,
          0,
          0.001953601837158203,
          0.0010190010070800781,
          0,
          0,
          0.009222269058227539,
          0.00099945068359375,
          0,
          0.0020058155059814453,
          0.002003908157348633,
          0.0010001659393310547,
          0.0019176006317138672,
          0,
          0,
          0.0009810924530029297,
          0.0014314651489257812,
          0,
          0,
          0.0010271072387695312,
          0.0010526180267333984,
          0,
          0,
          0,
          0.0010018348693847656,
          0.001318216323852539,
          0,
          0,
          0.0010004043579101562,
          0.0010013580322265625,
          0,
          0,
          0.010257244110107422,
          0,
          0,
          0,
          0.014845848083496094,
          0,
          0,
          0,
          0.0010104179382324219,
          0.0010249614715576172,
          0,
          0,
          0,
          0.00099945068359375,
          0,
          0,
          0,
          0.01226949691772461,
          0.0010030269622802734,
          0,
          0,
          0,
          0.0010020732879638672,
          0,
          0,
          0,
          0.0010085105895996094,
          0,
          0,
          0,
          0,
          0.0010006427764892578,
          0,
          0,
          0,
          0.001008749008178711,
          0,
          0,
          0,
          0,
          0.0020003318786621094,
          0,
          0,
          0,
          0.001010894775390625,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0010342597961425781,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0010442733764648438,
          0.0010247230529785156,
          0.0010118484497070312,
          0.0009772777557373047,
          0.0009984970092773438,
          0.001005411148071289,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0,
          0.002009153366088867,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0.0020074844360351562,
          0,
          0,
          0.0006504058837890625,
          0,
          0.002007722854614258,
          0,
          0.006320476531982422,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0010409355163574219,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0007262229919433594,
          0,
          0,
          0,
          0.013913631439208984,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.0010466575622558594,
          0,
          0,
          0,
          0,
          0.0011539459228515625,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.00141143798828125,
          0,
          0,
          0,
          0,
          0.0010037422180175781,
          0,
          0,
          0,
          0,
          0.0014500617980957031,
          0,
          0,
          0,
          0,
          0.001142740249633789,
          0,
          0,
          0,
          0,
          0.0011641979217529297,
          0,
          0,
          0,
          0,
          0.0011615753173828125,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013900995254516602,
          0,
          0,
          0,
          0,
          0.013915538787841797,
          0,
          0,
          0,
          0,
          0.01403665542602539,
          0.0005617141723632812,
          0,
          0,
          0,
          0,
          0.001003265380859375,
          0,
          0,
          0,
          0,
          0.0010192394256591797,
          0,
          0,
          0,
          0,
          0.0011539459228515625,
          0,
          0,
          0,
          0,
          0.002007722854614258,
          0,
          0,
          0,
          0,
          0.0009236335754394531,
          0,
          0,
          0,
          0.001008749008178711,
          0,
          0,
          0,
          0,
          0.0010428428649902344,
          0,
          0,
          0,
          0,
          0.0010366439819335938,
          0,
          0,
          0,
          0,
          0.0010075569152832031,
          0,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.0016295909881591797,
          0,
          0,
          0,
          0,
          0.0006692409515380859,
          0,
          0,
          0,
          0,
          0.0006546974182128906,
          0,
          0,
          0,
          0,
          0.0016093254089355469,
          0,
          0,
          0,
          0,
          0.0010471343994140625,
          0,
          0,
          0,
          0.01356649398803711,
          0.0011930465698242188,
          0,
          0,
          0,
          0.013434171676635742,
          0.0011641979217529297,
          0,
          0,
          0,
          0.011228084564208984,
          0,
          0,
          0,
          0,
          0.0010085105895996094,
          0,
          0,
          0,
          0,
          0.001028299331665039,
          0,
          0,
          0,
          0,
          0.0010433197021484375,
          0,
          0,
          0,
          0,
          0.0140533447265625,
          0,
          0,
          0,
          0,
          0.013875484466552734,
          0,
          0,
          0,
          0,
          0.013900995254516602,
          0,
          0,
          0,
          0,
          0.01400303840637207,
          0,
          0,
          0,
          0,
          0.013928413391113281,
          0.000995635986328125,
          0,
          0,
          0,
          0,
          0.0016634464263916016,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009946823120117188,
          0,
          0,
          0,
          0,
          0.0011467933654785156,
          0,
          0,
          0,
          0,
          0.00115966796875,
          0,
          0,
          0,
          0,
          0.0011441707611083984,
          0,
          0,
          0
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "im2col Optimized",
         "type": "scatter",
         "y": [
          0.0009999275207519531,
          0.0008985996246337891,
          0.0040814876556396484,
          0,
          0.0010075569152832031,
          0.0009999275207519531,
          0,
          0.0009992122650146484,
          0.0009992122650146484,
          0.0010006427764892578,
          0.0009984970092773438,
          0.0010013580322265625,
          0.0009989738464355469,
          0.0009672641754150391,
          0.0013091564178466797,
          0.0008730888366699219,
          0.0009996891021728516,
          0.0010006427764892578,
          0.00099945068359375,
          0.0010001659393310547,
          0.001001596450805664,
          0.0009992122650146484,
          0.0009982585906982422,
          0.0010006427764892578,
          0.0008990764617919922,
          0.0011608600616455078,
          0,
          0,
          0,
          0,
          0.0010077953338623047,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013724088668823242,
          0,
          0,
          0,
          0,
          0,
          0.0004582405090332031,
          0,
          0,
          0,
          0,
          0.000537872314453125,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010082721710205078,
          0,
          0,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.0010080337524414062,
          0,
          0,
          0,
          0,
          0.001008749008178711,
          0,
          0,
          0,
          0,
          0.014298677444458008,
          0,
          0,
          0,
          0,
          0.014117002487182617,
          0.0004546642303466797,
          0,
          0,
          0,
          0,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0.000997781753540039,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010085105895996094,
          0,
          0,
          0,
          0,
          0.0010075569152832031,
          0,
          0,
          0,
          0,
          0.013805150985717773,
          0,
          0,
          0,
          0,
          0,
          0.00046133995056152344,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010099411010742188,
          0,
          0,
          0,
          0,
          0.0010085105895996094,
          0,
          0,
          0,
          0,
          0.0010077953338623047,
          0,
          0,
          0,
          0,
          0.014101505279541016,
          0,
          0,
          0,
          0,
          0,
          0.00038695335388183594,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0013685226440429688,
          0,
          0,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0,
          0.001008749008178711,
          0,
          0,
          0,
          0,
          0.014095306396484375,
          0,
          0,
          0,
          0,
          0,
          0.0014636516571044922,
          0,
          0,
          0,
          0,
          0.0009963512420654297,
          0,
          0,
          0,
          0,
          0.0010156631469726562,
          0,
          0,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.00037097930908203125,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001016855239868164,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014239788055419922,
          0,
          0,
          0,
          0,
          0,
          0.0014200210571289062,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.014181375503540039,
          0,
          0,
          0,
          0,
          0,
          0.0009942054748535156,
          0,
          0,
          0,
          0,
          0.0012981891632080078,
          0,
          0,
          0,
          0,
          0.0010149478912353516,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.014307260513305664,
          0,
          0,
          0,
          0,
          0,
          0.00039649009704589844,
          0,
          0,
          0,
          0,
          0.0014233589172363281,
          0,
          0,
          0,
          0.014289617538452148,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0005502700805664062,
          0,
          0,
          0,
          0,
          0.0014271736145019531,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009942054748535156,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.014288187026977539,
          0,
          0,
          0,
          0,
          0.014176607131958008,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0014128684997558594,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010144710540771484,
          0,
          0,
          0,
          0,
          0.014201879501342773,
          0,
          0,
          0,
          0,
          0.014105081558227539,
          0,
          0,
          0,
          0,
          0,
          0.0013835430145263672,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010166168212890625,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014319896697998047,
          0,
          0,
          0,
          0,
          0,
          0.0014007091522216797,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010132789611816406,
          0,
          0,
          0,
          0,
          0.014217138290405273,
          0,
          0,
          0,
          0,
          0.014311790466308594,
          0,
          0,
          0,
          0,
          0,
          0.000335693359375,
          0,
          0,
          0,
          0,
          0.0015082359313964844,
          0,
          0,
          0,
          0,
          0.0015153884887695312,
          0,
          0,
          0,
          0,
          0.00140380859375,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.014261722564697266,
          0,
          0,
          0,
          0,
          0.014327049255371094,
          0,
          0,
          0,
          0.016324281692504883,
          0.0031671524047851562,
          0,
          0.0020008087158203125,
          0,
          0.006682157516479492,
          0.001798391342163086,
          0,
          0,
          0.011260271072387695,
          0,
          0,
          0,
          0.014162302017211914,
          0,
          0,
          0,
          0,
          0.0019834041595458984,
          0,
          0,
          0,
          0.013596057891845703,
          0.00041031837463378906,
          0,
          0,
          0,
          0,
          0.0014081001281738281,
          0,
          0,
          0,
          0,
          0.0013179779052734375,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.0010120868682861328,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0009949207305908203,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010156631469726562,
          0,
          0,
          0,
          0,
          0.014241695404052734,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013780832290649414,
          0,
          0,
          0,
          0,
          0,
          0.0013768672943115234,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0,
          0,
          0,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0.014194011688232422,
          0,
          0,
          0,
          0,
          0.01407766342163086,
          0,
          0,
          0,
          0,
          0.0011479854583740234,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.014382362365722656,
          0,
          0,
          0,
          0,
          0.013978242874145508,
          0,
          0,
          0,
          0,
          0,
          0.00038886070251464844,
          0,
          0,
          0,
          0,
          0.0013592243194580078,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010118484497070312,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.014251232147216797,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0004870891571044922,
          0,
          0,
          0,
          0,
          0.0013725757598876953,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.0010113716125488281,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.014309406280517578,
          0,
          0,
          0,
          0,
          0.014238357543945312,
          0,
          0,
          0,
          0,
          0,
          0.0014348030090332031,
          0,
          0,
          0,
          0,
          0.0013353824615478516,
          0,
          0,
          0,
          0,
          0.0009982585906982422,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.0142669677734375,
          0,
          0,
          0,
          0,
          0,
          0.0004200935363769531,
          0,
          0,
          0,
          0,
          0.001384735107421875,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0.014298677444458008,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.0013573169708251953,
          0,
          0,
          0,
          0,
          0.0014863014221191406,
          0,
          0,
          0,
          0,
          0.0013892650604248047,
          0,
          0,
          0,
          0,
          0.0015132427215576172,
          0,
          0,
          0,
          0,
          0.0015609264373779297,
          0,
          0,
          0,
          0,
          0.000457763671875,
          0,
          0,
          0,
          0.014225959777832031,
          0,
          0,
          0,
          0,
          0,
          0.00040721893310546875,
          0,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010187625885009766,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.014162063598632812,
          0,
          0,
          0,
          0,
          0.014273405075073242,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014339208602905273,
          0,
          0,
          0,
          0,
          0.014248371124267578,
          0,
          0,
          0,
          0,
          0.001010894775390625,
          0,
          0,
          0,
          0,
          0.0003695487976074219,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0014185905456542969,
          0,
          0,
          0,
          0,
          0.000990152359008789,
          0,
          0,
          0,
          0,
          0.0014605522155761719,
          0,
          0,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.00099945068359375,
          0,
          0,
          0,
          0,
          0.0013670921325683594,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0014941692352294922,
          0,
          0,
          0,
          0,
          0.0004787445068359375,
          0,
          0,
          0,
          0,
          0.000995635986328125,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.014334917068481445,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009949207305908203,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.014245748519897461,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01425313949584961,
          0,
          0,
          0,
          0,
          0,
          0.001401662826538086,
          0,
          0,
          0,
          0,
          0.001425027847290039,
          0,
          0,
          0,
          0,
          0.0014028549194335938,
          0,
          0,
          0,
          0.006516695022583008,
          0.0004436969757080078,
          0,
          0,
          0,
          0,
          0.0003914833068847656,
          0,
          0,
          0,
          0.014276742935180664,
          0,
          0,
          0,
          0,
          0.0004355907440185547,
          0,
          0,
          0,
          0.0009970664978027344,
          0.00098419189453125,
          0,
          0,
          0.000997781753540039,
          0.0013470649719238281,
          0,
          0,
          0.0010113716125488281,
          0,
          0,
          0,
          0.010246753692626953,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0.000997304916381836,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.000997781753540039,
          0,
          0,
          0.009214162826538086,
          0.0009658336639404297,
          0,
          0,
          0,
          0,
          0.0009996891021728516,
          0.0014126300811767578,
          0,
          0,
          0.0010135173797607422,
          0.0009949207305908203,
          0,
          0,
          0,
          0.0009937286376953125,
          0,
          0,
          0,
          0,
          0.0056149959564208984,
          0.0031931400299072266,
          0.0018281936645507812,
          0,
          0.000997781753540039,
          0,
          0,
          0.0010142326354980469,
          0.0011305809020996094,
          0,
          0,
          0,
          0.0009963512420654297,
          0.0010223388671875,
          0,
          0,
          0,
          0,
          0.0013265609741210938,
          0,
          0,
          0.010252237319946289,
          0.0010280609130859375,
          0.00099945068359375,
          0,
          0,
          0.0010089874267578125,
          0.0010459423065185547,
          0,
          0,
          0,
          0.0010552406311035156,
          0.0009961128234863281,
          0,
          0,
          0,
          0,
          0.0010242462158203125,
          0,
          0,
          0.010203361511230469,
          0.0009784698486328125,
          0.0009942054748535156,
          0,
          0,
          0.0010101795196533203,
          0.0010287761688232422,
          0.00041294097900390625,
          0,
          0,
          0.001009225845336914,
          0.0010194778442382812,
          0,
          0,
          0,
          0.0009751319885253906,
          0,
          0,
          0,
          0.008941650390625,
          0,
          0,
          0,
          0,
          0.0010094642639160156,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009679794311523438,
          0,
          0,
          0,
          0.009865283966064453,
          0,
          0,
          0,
          0.0011208057403564453,
          0,
          0,
          0,
          0,
          0.000993490219116211,
          0,
          0,
          0,
          0.002017498016357422,
          0.0013427734375,
          0,
          0,
          0.0009980201721191406,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009946823120117188,
          0.0010175704956054688,
          0,
          0,
          0.010156869888305664,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0.0010089874267578125,
          0,
          0,
          0,
          0.0009999275207519531,
          0.0010254383087158203,
          0,
          0,
          0,
          0.0010390281677246094,
          0.0010230541229248047,
          0,
          0,
          0,
          0,
          0.0009891986846923828,
          0,
          0,
          0.009199857711791992,
          0.0009989738464355469,
          0.0009999275207519531,
          0,
          0,
          0.0010080337524414062,
          0.0009989738464355469,
          0.002007722854614258,
          0,
          0.001009225845336914,
          0.0010001659393310547,
          0.0020072460174560547,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.0009982585906982422,
          0,
          0,
          0,
          0.010323524475097656,
          0.0010449886322021484,
          0.0010020732879638672,
          0,
          0,
          0.0010104179382324219,
          0.0010578632354736328,
          0,
          0,
          0,
          0.0009958744049072266,
          0.0010230541229248047,
          0,
          0,
          0,
          0.0009620189666748047,
          0.0014200210571289062,
          0,
          0,
          0.010164499282836914,
          0.0009720325469970703,
          0.0014255046844482422,
          0,
          0,
          0.010343074798583984,
          0,
          0,
          0,
          0,
          0.0010099411010742188,
          0.0010445117950439453,
          0,
          0,
          0,
          0.000997304916381836,
          0.0010001659393310547,
          0,
          0,
          0.01063084602355957,
          0.0009996891021728516,
          0.0014235973358154297,
          0,
          0,
          0.0010104179382324219,
          0.0010042190551757812,
          0,
          0,
          0,
          0.0010001659393310547,
          0.0009925365447998047,
          0,
          0,
          0.009346723556518555,
          0.0010190010070800781,
          0,
          0,
          0,
          0.0010194778442382812,
          0,
          0,
          0,
          0.010243892669677734,
          0.0010495185852050781,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0.0010178089141845703,
          0.0009636878967285156,
          0,
          0,
          0.0010097026824951172,
          0.0009810924530029297,
          0,
          0,
          0.0010089874267578125,
          0.0009713172912597656,
          0.001047372817993164,
          0.0009741783142089844,
          0,
          0.0009932518005371094,
          0.0009975433349609375,
          0,
          0,
          0.008630514144897461,
          0.0010232925415039062,
          0.00037169456481933594,
          0,
          0,
          0.0010502338409423828,
          0.0010225772857666016,
          0,
          0,
          0,
          0.0009529590606689453,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0.0009999275207519531,
          0,
          0,
          0.004368782043457031,
          0.001001119613647461,
          0.0014193058013916016,
          0,
          0,
          0.01024770736694336,
          0,
          0.001018524169921875,
          0.006762027740478516,
          0.003000497817993164,
          0,
          0.001998424530029297,
          0,
          0.0009963512420654297,
          0.0016326904296875,
          0,
          0.0009837150573730469,
          0.0009970664978027344,
          0,
          0,
          0,
          0.0012965202331542969,
          0,
          0,
          0,
          0.0010464191436767578,
          0.00034117698669433594,
          0,
          0,
          0.0010097026824951172,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0009965896606445312,
          0,
          0,
          0,
          0,
          0.000993490219116211,
          0,
          0,
          0,
          0,
          0.0009672641754150391,
          0.0009963512420654297,
          0,
          0,
          0.01020193099975586,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009462833404541016,
          0.0013530254364013672,
          0,
          0,
          0.011090755462646484,
          0.0010004043579101562,
          0.00031495094299316406,
          0,
          0,
          0.0010449886322021484,
          0.001608133316040039,
          0,
          0,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0.010604381561279297,
          0.0010008811950683594,
          0.0014526844024658203,
          0,
          0,
          0.0011992454528808594,
          0.0009992122650146484,
          0,
          0,
          0.002512216567993164,
          0.0010790824890136719,
          0,
          0,
          0,
          0.0010211467742919922,
          0.0010209083557128906,
          0,
          0,
          0,
          0.0010423660278320312,
          0.0010228157043457031,
          0,
          0,
          0.010263442993164062,
          0.0009629726409912109,
          0,
          0,
          0,
          0.0010106563568115234,
          0.0010302066802978516,
          0.0003180503845214844,
          0,
          0,
          0.001008749008178711,
          0.0010411739349365234,
          0.0003032684326171875,
          0,
          0,
          0.0009965896606445312,
          0.0010223388671875,
          0,
          0,
          0,
          0.0010008811950683594,
          0.0009975433349609375,
          0,
          0,
          0,
          0.0009999275207519531,
          0,
          0,
          0,
          0.00915074348449707,
          0.0010182857513427734,
          0,
          0,
          0,
          0.001027822494506836,
          0.0013704299926757812,
          0,
          0,
          0.01018524169921875,
          0.0009772777557373047,
          0.0004980564117431641,
          0,
          0,
          0.001010894775390625,
          0.00099945068359375,
          0,
          0,
          0.0010097026824951172,
          0.0010001659393310547,
          0.0020062923431396484,
          0,
          0.005105733871459961,
          0,
          0.00146484375,
          0,
          0,
          0.0009026527404785156,
          0,
          0,
          0,
          0.000997304916381836,
          0.0004012584686279297,
          0,
          0.009149551391601562,
          0.0010004043579101562,
          0,
          0,
          0.00935220718383789,
          0,
          0,
          0,
          0,
          0.0010223388671875,
          0,
          0,
          0,
          0.0010089874267578125,
          0.001003265380859375,
          0,
          0,
          0.010166168212890625,
          0.0009720325469970703,
          0.0013914108276367188,
          0,
          0,
          0.0010113716125488281,
          0.0009517669677734375,
          0,
          0,
          0.008620500564575195,
          0.0010232925415039062,
          0.0014367103576660156,
          0,
          0,
          0.0009860992431640625,
          0.00099945068359375,
          0,
          0,
          0.010222434997558594,
          0.000997781753540039,
          0,
          0,
          0,
          0.0009989738464355469,
          0.0010478496551513672,
          0,
          0,
          0,
          0.0010008811950683594,
          0.0014641284942626953,
          0,
          0,
          0.0010097026824951172,
          0.0009500980377197266,
          0,
          0,
          0,
          0.001046895980834961,
          0.000995635986328125,
          0,
          0,
          0,
          0.0009918212890625,
          0.0010182857513427734,
          0,
          0,
          0.010133743286132812,
          0.0009748935699462891,
          0.0009999275207519531,
          0,
          0,
          0.001008749008178711,
          0.0010216236114501953,
          0,
          0,
          0,
          0.0019989013671875,
          0.0009984970092773438,
          0,
          0,
          0.009022951126098633,
          0.0009708404541015625,
          0,
          0,
          0,
          0.0011188983917236328,
          0.0010197162628173828,
          0,
          0,
          0,
          0.0009982585906982422,
          0.0009865760803222656,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0.0010106563568115234,
          0.0010178089141845703,
          0.00036835670471191406,
          0,
          0,
          0.000978231430053711,
          0.0009760856628417969,
          0,
          0,
          0.00924229621887207,
          0.0009996891021728516,
          0,
          0,
          0,
          0.000978231430053711,
          0,
          0,
          0,
          0.0010116100311279297,
          0.0010006427764892578,
          0,
          0,
          0,
          0.0009813308715820312,
          0.0015273094177246094,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0.0010161399841308594,
          0,
          0,
          0,
          0.0009770393371582031,
          0.0013797283172607422,
          0,
          0,
          0.01023101806640625,
          0.0009982585906982422,
          0.0014538764953613281,
          0,
          0,
          0.002007007598876953,
          0.0009770393371582031,
          0,
          0,
          0.009104490280151367,
          0,
          0.0014171600341796875,
          0,
          0,
          0.0010101795196533203,
          0.0009982585906982422,
          0.0020058155059814453,
          0,
          0,
          0.0009996891021728516,
          0.00046539306640625,
          0,
          0.009259700775146484,
          0.001123189926147461,
          0,
          0,
          0,
          0.0009906291961669922,
          0.0009713172912597656,
          0,
          0,
          0.00925135612487793,
          0.0010519027709960938,
          0,
          0,
          0,
          0,
          0.0013985633850097656,
          0,
          0,
          0.0010099411010742188,
          0,
          0,
          0,
          0.0009961128234863281,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0.0010111331939697266,
          0,
          0,
          0,
          0.012723445892333984,
          0.0009732246398925781,
          0,
          0,
          0,
          0.0009996891021728516,
          0,
          0,
          0,
          0.01268148422241211,
          0.0009989738464355469,
          0,
          0,
          0,
          0.001032114028930664,
          0,
          0,
          0,
          0.012717008590698242,
          0.0009970664978027344,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0010173320770263672,
          0,
          0,
          0,
          0,
          0.0009741783142089844,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.0009658336639404297,
          0.0010004043579101562,
          0.000995635986328125,
          0.0010013580322265625,
          0,
          0.0009927749633789062,
          0,
          0,
          0,
          0,
          0.0016832351684570312,
          0,
          0,
          0,
          0,
          0.0007927417755126953,
          0,
          0,
          0,
          0,
          0.000652313232421875,
          0,
          0,
          0,
          0,
          0.0007796287536621094,
          0.0020072460174560547,
          0,
          0.0020599365234375,
          0.0036568641662597656,
          0,
          0.002008199691772461,
          0,
          0,
          0.0011200904846191406,
          0,
          0,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0,
          0.001664876937866211,
          0,
          0,
          0,
          0,
          0.0017528533935546875,
          0,
          0,
          0,
          0.013855457305908203,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.0009999275207519531,
          0,
          0,
          0,
          0,
          0.0009865760803222656,
          0,
          0,
          0,
          0,
          0.0014197826385498047,
          0,
          0,
          0,
          0,
          0.0017461776733398438,
          0,
          0,
          0,
          0.013687372207641602,
          0,
          0,
          0,
          0,
          0.014202594757080078,
          0.0006670951843261719,
          0,
          0,
          0,
          0.01374959945678711,
          0,
          0,
          0,
          0,
          0,
          0.0015034675598144531,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0011534690856933594,
          0,
          0,
          0,
          0,
          0.0011529922485351562,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01392817497253418,
          0.0005469322204589844,
          0,
          0,
          0,
          0.013915777206420898,
          0.0005548000335693359,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0012173652648925781,
          0,
          0,
          0,
          0.013427972793579102,
          0,
          0,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0,
          0.0009679794311523438,
          0,
          0,
          0,
          0,
          0.0009722709655761719,
          0,
          0,
          0,
          0,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0.0011174678802490234,
          0,
          0,
          0,
          0.013483285903930664,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01398324966430664,
          0,
          0,
          0,
          0,
          0.014061689376831055,
          0,
          0,
          0,
          0,
          0.013949871063232422,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0011146068572998047,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0,
          0.0009806156158447266,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0011610984802246094,
          0,
          0,
          0,
          0,
          0.0011544227600097656,
          0,
          0,
          0,
          0,
          0.0011560916900634766,
          0,
          0,
          0,
          0,
          0.0011570453643798828,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013539791107177734,
          0,
          0,
          0,
          0,
          0,
          0.0014545917510986328,
          0,
          0,
          0,
          0,
          0.0005700588226318359,
          0,
          0,
          0,
          0,
          0.0014903545379638672,
          0,
          0,
          0,
          0,
          0.0009930133819580078,
          0,
          0,
          0,
          0,
          0.0010056495666503906,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "height": 500,
        "hovermode": "x unified",
        "legend": {
         "x": 0,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Inference Time Comparison (Interactive)"
        },
        "width": 1400,
        "xaxis": {
         "title": {
          "text": "Batch Index"
         }
        },
        "yaxis": {
         "title": {
          "text": "Inference Time (s)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics of Inference Times:\n",
      "           PyTorch       im2col  im2col Optimized\n",
      "count  2000.000000  2000.000000       2000.000000\n",
      "mean      0.001098     0.000940          0.000837\n",
      "std       0.003252     0.002821          0.002657\n",
      "min       0.000000     0.000000          0.000000\n",
      "25%       0.000000     0.000000          0.000000\n",
      "50%       0.000000     0.000000          0.000000\n",
      "75%       0.000856     0.000999          0.000967\n",
      "max       0.014627     0.015363          0.016324\n",
      "\n",
      "Number of contiguous arrays:\n",
      "im2col: 0, im2col_optimized: 0\n",
      "im2col (not contiguous): 0, im2col_optimized (not contiguous): 0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyoff\n",
    "import pandas as pd\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "dict_times[\"ccm\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "dict_pred[\"ccm\"]=[]\n",
    "\n",
    "c_continuity_ckeck = {\n",
    "    'im2col_yes' : 0,\n",
    "    'im2col_no' : 0,\n",
    "    'im2col_optimized_yes' : 0,\n",
    "    'im2col_optimized_no' : 0,\n",
    "}\n",
    "\n",
    "correct = 0\n",
    "skip = True\n",
    "loop = tqdm(range(0, test_labels.shape[0], 5), desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "\n",
    "    ############### CNN nested loops Implementation #####################\n",
    "    # start_time = time.time()\n",
    "    # c1s,mask1s = nested_loop_convolution(c0.astype(np.float32),np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    # c2s,mask2s = nested_loop_convolution(c1s.astype(np.float32),np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    # c3s,mask3s = nested_loop_convolution(c2s.astype(np.float32),np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    # imlps = c3s.reshape(1,-1)\n",
    "    # _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    # predicted2 = np.argmax(res,1)\n",
    "    # end_time = time.time()\n",
    "    # dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    # dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "\n",
    "    ############### CNN im2col Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = im2col_convolution(c0.astype(np.float32),np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2f,mask2f = im2col_convolution(c1f.astype(np.float32),np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3f,mask3f = im2col_convolution(c2f.astype(np.float32),np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted3 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "\n",
    "    ############## CNN optimized im2col Implementation ###########\n",
    "    start_time = time.time()\n",
    "    c1c,mask1c = im2col_optimized(c0.astype(np.float32),np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2c,mask2c = im2col_optimized(c1c.astype(np.float32),np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3c,mask3c = im2col_optimized(c2c.astype(np.float32),np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpc = c3c.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpc,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted4 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"ccm\"].append(end_time-start_time)\n",
    "    dict_pred[\"ccm\"].append(np.array(predicted4))\n",
    "\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    t = int(predicted1[0])\n",
    "    # s = int(predicted2[0])\n",
    "    f = int(predicted3[0])\n",
    "    c = int(predicted4[0])\n",
    "\n",
    "    if t == s and t == f:\n",
    "        correct += 1\n",
    "\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    tat = round(sum(dict_times['ctorch'])/(i+1),10)\n",
    "    # sat = round(sum(dict_times['cslow'])/(i+1),10)\n",
    "    fat = round(sum(dict_times['cfast'])/(i+1),10)\n",
    "    cat = round(sum(dict_times['ccm'])/(i+1),10)\n",
    "\n",
    "    # loop.set_postfix(average_times =f\"t: {tat} s, s: {sat} s, f: {fat} s, c: {cat} s\" , correct_predictions=f\"{(correct/(i+1)*100)}%\")\n",
    "    loop.set_postfix(average_times =f\"pytorch: {tat} s, im2c: {fat} s, im2c_opt: {cat} s\" , correct_predictions=f\"{100*correct/(i+1)}%\")\n",
    "\n",
    "tat = np.mean(dict_times['ctorch'])\n",
    "# sat = np.mean(dict_times['cslow'])\n",
    "fat = np.mean(dict_times['cfast'])\n",
    "cat = np.mean(dict_times['ccm'])\n",
    "\n",
    "# print(f\"Average inference time in seconds:\\nPyTorch:\\t{tat} s,\\nnested_loops:\\t{sat} s,\\nim2col:\\t\\t{fat} s, \\nim2col_optim:\\t{cat} s\")\n",
    "print(f\"Average inference time in seconds:\\nPyTorch:\\t{tat} s,\\nim2col:\\t\\t{fat} s, \\nim2col_optim:\\t{cat} s\")\n",
    "\n",
    "# Plot times altogether\n",
    "\n",
    "# Create interactive traces for each method\n",
    "trace_torch = go.Scatter(\n",
    "    y=dict_times['ctorch'],\n",
    "    mode='lines+markers',\n",
    "    name='PyTorch',\n",
    "    line=dict(color='blue')\n",
    ")\n",
    "# trace_slow = go.Scatter(\n",
    "#     y=dict_times['cslow'],\n",
    "#     mode='lines+markers',\n",
    "#     name='Nested Loops',\n",
    "#     line=dict(color='orange')\n",
    "# )\n",
    "trace_fast = go.Scatter(\n",
    "    y=dict_times['cfast'],\n",
    "    mode='lines+markers',\n",
    "    name='im2col',\n",
    "    line=dict(color='green')\n",
    ")\n",
    "trace_ccm = go.Scatter(\n",
    "    y=dict_times['ccm'],\n",
    "    mode='lines+markers',\n",
    "    name='im2col Optimized',\n",
    "    line=dict(color='red')\n",
    ")\n",
    "\n",
    "data = [trace_torch, trace_fast, trace_ccm]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Inference Time Comparison (Interactive)',\n",
    "    xaxis=dict(title='Batch Index'),\n",
    "    yaxis=dict(title='Inference Time (s)'),\n",
    "    width=1400,\n",
    "    height=500,\n",
    "    legend=dict(x=0, y=1),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "pyoff.iplot(fig)\n",
    "\n",
    "# Also plot some boxplots for better visualization\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.boxplot([dict_times['ctorch'], dict_times['cfast'], dict_times['ccm']],\n",
    "            tick_labels=['PyTorch', 'im2col', 'im2col Optimized'],\n",
    "            showfliers=False)\n",
    "plt.ylabel('Inference Time (s)')\n",
    "plt.title('Boxplot of Inference Times')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "df_times = pd.DataFrame({\n",
    "    'PyTorch': dict_times['ctorch'],\n",
    "    # 'Nested Loops': dict_times['cslow'],\n",
    "    'im2col': dict_times['cfast'],\n",
    "    'im2col Optimized': dict_times['ccm']\n",
    "})\n",
    "print(\"\\nDescriptive Statistics of Inference Times:\")\n",
    "print(df_times.describe())\n",
    "\n",
    "# Print the number of contiguous arrays\n",
    "print(\"\\nNumber of contiguous arrays:\")\n",
    "print(f\"im2col: {c_continuity_ckeck['im2col_yes']}, im2col_optimized: {c_continuity_ckeck['im2col_optimized_yes']}\")\n",
    "print(f\"im2col (not contiguous): {c_continuity_ckeck['im2col_no']}, im2col_optimized (not contiguous): {c_continuity_ckeck['im2col_optimized_no']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abe784",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f8f80",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a476a",
   "metadata": {},
   "source": [
    "### NumPy Model Training: Weights Initialization\n",
    "\n",
    "For training our NumPy CNNs from scratch, weights and biases are initialized randomly.\n",
    "The shapes are taken from `numpy_weights` (derived from the PyTorch model) to maintain architectural consistency. `np.random.rand()` provides initial values (uniform in [0,1)). While more advanced initializers exist, this suffices for observing basic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faa5546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382d816",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf4d4f",
   "metadata": {},
   "source": [
    "### Training the \"Slow\" NumPy CNN (Single Image Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b10d0",
   "metadata": {},
   "source": [
    "This tests training the loop-based `nested_loop_convolution` and `Slow_ReLU_Gradient` implementations on a single image.\n",
    "\n",
    "**Per-Epoch Steps:**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> nested_loop_convolution (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> nested_loop_convolution (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> nested_loop_convolution (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa` (probabilities)\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** Gradients are computed using `ReLU_SoftMax_FC_Backward` for MLP, then `Slow_ReLU_Gradient` is called sequentially for conv layers, propagating gradients backward.\n",
    "4.  **Weight Update:** Parameters updated via $W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W_{old}}$.\n",
    "\n",
    "The loss is plotted to observe learning. The padding and stride parameters in `nested_loop_convolution` calls are set to match the PyTorch model architecture, ensuring the flattened features `imlps` have the correct dimension (2048) for the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:30<00:00,  4.54s/it, avgBackward=3.3668 s, avgForward=1.168 s, pendence=[-0.02896908]] \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlsElEQVR4nO3dC4ylZX0/8Gdmdm477MzOLrddF3ClKBSQWEuJl6Z/hKBIEdpGS0Mt1bYaug1Sa4skBUqoXVFjiIYgkhYwCpYmBZtiNYWiVLkLtNQaBCWwlZuyOzN7nev553nPec+cmZ3LuV8/n+Tdc8573pk9775z9nzneX7P83RlMplMAACok+56/UUAAJHwAQDUlfABANSV8AEA1JXwAQDUlfABANSV8AEA1JXwAQDU1ZrQZObm5sKLL74Y1q1bF7q6uhr9cgCAIsQ5S3fv3h02b94curu7Wyt8xOBx1FFHNfplAABl2LFjR9iyZUtrhY/Y4pG++OHh4Ua/HACgCBMTE0njQfo53lLhI+1qicFD+ACA1lJMyYSCUwCgroQPAKCuhA8AoK6EDwCgroQPAKCuhA8AoK6EDwCgroQPAKCuhA8AoK6EDwCgroQPAKCuhA8AoK6abmG5Wnl5/EC4+fvPhdAVwuVnn9DolwMAHatjWj72Ts2EG+//abjt4Rca/VIAoKN1TPjYsLYvud19YCZMz841+uUAQMfqmPAxPNgburqy93ftm2r0ywGAjtUx4aOnuyusH+xN7o/tm270ywGAjtUx4SMaHcp2vezcq+UDABqlo8JHWvexS/gAgIbpqPCxPhc+dqr5AICG6ajwsWFIzQcANFpHhQ81HwDQeB0VPtR8AEDjdVT4GFXzAQAN15HdLlo+AKBxOrLgdJeCUwBomI7sdtHyAQCN05HhY/fkTJiasbgcADRCR4WPuLhcd25xuTFFpwDQEB0VPpLF5dKuF3UfANAQHRU+otG12aJTE40BQGN0XPjYkA631e0CAA3RceEjv7iclg8AaIiOCx/pFOsKTgGgMToufMwvLqfgFAAaoePCx/wsp1o+AKAROi58qPkAgMbquPCh5gMAGqtzaz6EDwBoiM6d50PBKQA0RMfOcLpnciZMzsw2+uUAQMfpuPAxPFC4uJzWDwCot44LH93dXWE0v7icug8AqLeOCx8LJxoTPgCg3jozfOTqPhSdAkD9dWj4MNwWABqlI8NHOtx2TLcLANRdR4YPE40BQON0ZPhIp1jfpeUDAOquI8PH+lzB6U7zfABA3XV2zYduFwCou44MH+b5AIDG6cjwoeYDABqno+f52Ds1a3E5AKizjgwf6wbWhJ7c6nIWlwOA+urI8JFdXC434kXXCwDUVUeGjyi/sq3wAQB11fHhwyynAFBfnRs+hnIr26r5AIC66tjwkU40ptsFAOqrY8NHvttF+ACAuur48LFLzQcA1FXnhg9TrANAQ3Rs+NiQKzg1yRgA1FfHhg81HwDQGB0bPvKjXdR8AEBddWz4WJ9r+dg3NRsOTFtcDgDqpWPDx7DF5QCgITo2fHR1xcXl1H0AQL11bPgoHPGi7gMA6qejw0da96HlAwCaOHzcf//94dxzzw2bN29Oui7uuuuuBc9nMplw5ZVXhk2bNoXBwcFw5plnhmeeeSY0ow258DGm5QMAmjd87N27N5xyyinh+uuvX/L5z3zmM+ELX/hC+NKXvhQefvjhMDQ0FN797neHAwcOhOad5VTBKQDUy5pSv+Dss89OtqXEVo/rrrsu/PVf/3U477zzkn1f+cpXwhFHHJG0kFxwwQWhmaj5AIAWr/l47rnnwssvv5x0taRGRkbCaaedFh588MElv2ZycjJMTEws2OrFaBcAaPHwEYNHFFs6CsXH6XOLbd++PQko6XbUUUeFerGyLQB04GiXyy+/PIyPj+e3HTt21O3vNsU6ALR4+DjyyCOT21deeWXB/vg4fW6x/v7+MDw8vGCrd8HpLgWnANCa4WPr1q1JyLj33nvz+2INRxz18ra3vS00m9G12YJTNR8A0MSjXfbs2ROeffbZBUWmTz75ZNiwYUM4+uijw6WXXhr+9m//Nhx33HFJGLniiiuSOUHOP//80GzSlo/909nF5QZ6exr9kgCg7ZUcPh577LFw+umn5x9//OMfT24vuuiicMstt4S/+qu/SuYC+chHPhLGxsbCO9/5zvCtb30rDAwMhGazrn9NWNPdFWbmMkndx6aRwUa/JABoe12ZODlHE4ndNHHUSyw+rUf9x6mfuif8fPdkuPuSd4YTN4/U/O8DgHZUyud3w0e7NMsU64pOAaA+Oj58rE+LTg23BYC66Pjwkc71YXE5AKiPjg8f84vLCR8AUA8dHz7maz6EDwCoh44PH/M1HwpOAaAeOj58qPkAgPrq+PCh5gMA6qvjw4eaDwCor44PH6O58GGeDwCoD+FjKFtwemB6Luyfmm30ywGAttfx4eOQ/jWht6cruR8XlwMAaqvjw0dXV9d814u6DwCouY4PH1EaPrR8AEDtCR8FdR+7TDQGADUnfBRMNGa4LQDUnvBRONxW+ACAmhM+Cls+1HwAQM0JH8niclo+AKBehI+k5SNbcDqm4BQAak74UPMBAHUlfKj5AIC6Ej60fABAXQkfySRj2fAxOWNxOQCoNeEjhDDU1xP6erL/FDt1vQBATQkf6eJy6RTrul4AoKaEjxx1HwBQH8JHjpVtAaA+hI8ci8sBQH0IHzlpzcdOs5wCQE0JH4u7XbR8AEBNCR85aj4AoD6EjxxTrANAfQgfi2Y53blXzQcA1JLwkbNBzQcA1IXwkbN+bW6G031TIZPJNPrlAEDbEj4W1Xwki8tNW1wOAGpF+MhZGxeXW5NbXE7XCwDUjPBRsLjcfN2HolMAqBXhY4m6j52G2wJAzQgfS9R9jAkfAFAzwseSc30IHwBQK8JHAXN9AEDtCR8FRtV8AEDNCR9LdLvs2me0CwDUivCx1OJyul0AoGaEjwKjuZoPBacAUDvCxxLhI67vAgDUhvBRYHQoXVxu2uJyAFAjwscSNR9TM3Nh35TF5QCgFoSPAoO9PaHf4nIAUFPCx+LF5fLDbYUPAKgF4WOR9fmiU3N9AEAtCB+LbEiLTnW7AEBNCB+LmOsDAGpL+FhEzQcA1JbwsWzNh/ABALUgfCyyIbey7a69Ck4BoBaEj2VWtlXzAQC1IXwsouYDAGpL+FjEaBcAqC3hY5lulzGLywFATQgfi2zItXxMzc6FvRaXA4CqEz4WGezrCQO92X8Ws5wCQPUJH0tQ9wEAtSN8rBA+jHgBgOoTPpZguC0A1I7wseJEY2Y5BYBqEz5WnGJdywcAVJvwsQSLywFAC4WP2dnZcMUVV4StW7eGwcHBcOyxx4ZrrrmmpSbsUvMBALWzptrf8Nprrw033HBDuPXWW8OJJ54YHnvssfChD30ojIyMhEsuuSS0AovLAUALhY8HHnggnHfeeeGcc85JHr/+9a8Pt99+e3jkkUdCq81yukvBKQA0f7fL29/+9nDvvfeGH//4x8nj//qv/wrf+973wtlnn73k8ZOTk2FiYmLB1mjr04JT3S4A0PwtH5/85CeTAHH88ceHnp6epAbkU5/6VLjwwguXPH779u3h6quvDs1a8xFrVbq6uhr9kgCgbVS95eOOO+4IX/va18Jtt90WHn/88aT243Of+1xyu5TLL788jI+P57cdO3aEZpnhdHo2E/ZMzjT65QBAW6l6y8df/uVfJq0fF1xwQfL45JNPDs8//3zSwnHRRRcddHx/f3+yNdvicoO9PWH/9GxS97FuINsNAwA0YcvHvn37Qnf3wm8bu1/m5uZCKxlV9wEArdHyce655yY1HkcffXQy1PaJJ54In//858OHP/zh0EricNsXxw+EncIHADR3+PjiF7+YTDL2p3/6p+HVV18NmzdvDh/96EfDlVdeGVpJvujUXB8A0NzhY926deG6665LtlaWFp2aaAwAqsvaLstQ8wEAtSF8rDLF+q59ZjkFgGoSPpah5gMAakP4WIaaDwCoDeGjiCnWAYDqET5WXVxOzQcAVJPwUUTNR1xcDgCoDuFjlZqPmblM2G1xOQCoGuFjGQO9PWFtX09y34gXAKge4aOI1g91HwBQPcLHCkaHckWnWj4AoGqEjxWY6wMAqk/4WIG5PgCg+oSPomo+hA8AqBbho6huFwWnAFAtwscKNig4BYCqEz5WMJqr+dip2wUAqkb4KKbmQ8sHAFSN8LECk4wBQPUJH0UOtbW4HABUh/CxgvVrswWns3OZMHHA4nIAUA3CxyqLyw1ZXA4Aqkr4WMV6E40BQFUJH6swxToAVJfwUexcH2Y5BYCqED5WsSFXdKrmAwCqQ/hYhZoPAKgu4WMVaj4AoLqEj6JrPoQPAKgG4WMVG/Lruyg4BYBqED5WMZoWnOp2AYCqED6K7HYRPgCgOoSPogtOp8PcnMXlAKBSwkcJi8vttrgcAFRM+FhF/5qCxeV0vQBAxYSPUobbCh8AUDHho5S6D3N9AEDFhI8ijObm+jDRGABUTvgoginWAaB6hI8SRrzE4bYAQGWEj5KmWNfyAQCVEj6KYHE5AKge4aMIaj4AoHqEjyKo+QCA6hE+imCeDwCoHuGjlILTfVMWlwOACgkfRVifCx8xd0wc0PUCAJUQPorQt6Y7HNK/Jrmv7gMAKiN8FGl0KFt0argtAFRG+CiSicYAoDqEj1InGjPXBwBURPgocWXbMeEDACoifJQYPnbuVXAKAJUQPoq0IVdwquYDACojfBRJzQcAVIfwUeJoFzUfAFAZ4aPEWU7N8wEAlRE+Sl1czgynAFAR4aPEGU5jt8usxeUAoGzCR4lDbZPF5fZr/QCAcgkfRert6Q7r8ovLqfsAgHIJH2UMtxU+AKB8wkc5c32Y5RQAyiZ8lGDDWrOcAkClhI8yik51uwBA+YSPEphiHQAqJ3yUM9GYbhcAKJvwUUa3i4JTACif8FGC0VzBqcXlAKB8wkcJ1HwAQJOGj5/97Gfh93//98PGjRvD4OBgOPnkk8Njjz0WWp2aDwCoXHa+8CratWtXeMc73hFOP/308G//9m/hsMMOC88880wYHR0N7VLzMbZ/Ollcrqe7q9EvCQBaTtXDx7XXXhuOOuqocPPNN+f3bd26NbSD9bmaj0xucbm0GwYAaGC3y7/8y7+EX/3VXw3vf//7w+GHHx7e8pa3hJtuumnZ4ycnJ8PExMSCrakXlxvI5jV1HwDQJOHjpz/9abjhhhvCcccdF7797W+Hiy++OFxyySXh1ltvXfL47du3h5GRkfwWW02amboPAKhMVyYTOxGqp6+vL2n5eOCBB/L7Yvh49NFHw4MPPrhky0fcUrHlIwaQ8fHxMDw8HJrN+dd/Pzy5Yyx8+YNvDWedeGSjXw4ANIX4+R0bEYr5/K56y8emTZvCL//yLy/Yd8IJJ4QXXnhhyeP7+/uTF1m4tUTLh24XAChL1cNHHOny9NNPL9j34x//OBxzzDGhnYpOd+0zyykANEX4+PM///Pw0EMPhb/7u78Lzz77bLjtttvCl7/85bBt27bQDjakK9uq+QCA5ggfp556arjzzjvD7bffHk466aRwzTXXhOuuuy5ceOGFoa1mORU+AKA55vmIfvM3fzPZ2pGaDwCojLVdylxcTs0HAJRH+ChzinU1HwBQHuGjzG4XM5wCQHmEjzILTsdzi8sBAKURPkq0fnB+cbkYQACA0ggfJVrT0x2G08Xl1H0AQMmEjzIYbgsA5RM+ymCiMQAon/BRwRTrY1o+AKBkwkcZ1ufCx869Ck4BoFTCRxk2DKWznGr5AIBSCR9lUPMBAOUTPsqg5gMAyid8VFTzIXwAQKmEj4rm+VBwCgClEj4qKDjV8gEApRM+yjCa63aJa7vMzM41+uUAQEsRPsowkltcLrK4HACURvgoc3G5NICY6wMASiN8VFh0apZTACiN8FGm0bWKTgGgHMJHhUWnJhoDgNIIH5VOsS58AEBJhI9KJxrT7QIAJRE+Kux2UXAKAKURPiqc5VTNBwCURviodHE54QMASiJ8lEnNBwCUR/iouOZD+ACAUggfFbZ8TByYsbgcAJRA+ChTXNulqyt7f8zicgBQNOGjTD3dXfOLy+l6AYCiCR8V2KDuAwBKJnxUYYr1Xft0uwBAsYSPKqxsu8tcHwBQNOGjAobbAkDphI8KmGgMAEonfFSh5sMU6wBQPOGjCjUfYwpOAaBowkcF1HwAQOmEj2rUfOh2AYCiCR/VqPnQ8gEARRM+qjDD6e4DM2Ha4nIAUBThowLDhYvLKToFgKIIHxUuLrc+XVxO3QcAFEX4qJC6DwAojfBRpbqPMS0fAFAU4aNC6/Nzfaj5AIBiCB8V2jCk5gMASiF8VEjNBwCURvioUs2Hlg8AKI7wUaX1XXZp+QCAoggf1ep2MckYABRF+KhWwamWDwAoivBRrW4XNR8AUBTho0rhw+JyAFAc4aMKi8t15xaX0/oBAKsTPqqxuFx+xIuiUwBYjfBRBaNrs0WnJhoDgNUJH1WwITfc1uJyALA64aOai8sJHwCwKuGjmlOs63YBgFUJH1VdXE7BKQCsRvio4iynaj4AYHXCRxWo+QCA4gkfVaDmAwCKJ3xUdWVb4QMAViN8VHOeDwWnALAq4aOKM5zunpwJUzMWlwOAlQgfVTA8ML+4nBEvANDg8PHpT386dHV1hUsvvTS0q+7urjBqxAsAND58PProo+HGG28Mb37zm0OnFJ1a2RYAGhQ+9uzZEy688MJw0003hdHR0dApdR+7tHwAQGPCx7Zt28I555wTzjzzzBWPm5ycDBMTEwu2VpTvdjHXBwCsaE2oga9//evh8ccfT7pdVrN9+/Zw9dVXh3YZbmuiMQCoc8vHjh07wsc+9rHwta99LQwMDKx6/OWXXx7Gx8fzW/z6Vq75eGniQJieNdwWAJbTlclkMqGK7rrrrvBbv/VboaenJ79vdnY2GfHS3d2ddLMUPrdY7HYZGRlJgsjw8HBoFTfd/9PwqW/+aEENyKGH9Ge3dfG2L/e476D9/WuW//cAgFZQyud31btdzjjjjPDUU08t2PehD30oHH/88eGyyy5bMXi0stOPPyzc9sgL4YWd+8LsXCbs2jedbM+8umfVr103sCYclg8kBeHkkP6wMRdWkufX9YW1fTXpKQOAuqn6J9m6devCSSedtGDf0NBQ2Lhx40H728kvHb4u3PeJ/xfmkuAxFX6xJ26Tyfbz3ZMLHr9WcH96NhN2H5hJtp/+Yu+qf8/avp4kkGwcyraaxNvkca5VJdm/LnsbW1/W9JhHDoDm4tfoGkw4FoNA3N4U1q14bOzxmtg/E36eCyLJtiio/DzeT/ZNhsmZubBvajbs27k/7Ni5f9XX0tWVHYWzcahvPqAk9/sXBphccDmkf03SPQYALVXzUalWrfmotXiZ9k7NhteSUDKV3L62d2r+ce5+2qoSZ1ot9cr29XQno3bilg0n8X4aVOb3p/vWCSsANEPNB7URP+Rjy0Tcjtk4tOrx2bqTGEhyAaUgnLy2d3GAmQp74qJ4s3Ph5YkDyVaM3p6uXFjJtpzkg0uudSW9n73tD8ODwgoAwkfb6unuyhethlW6f6L9sVVl72QySVoMJDtzISW9H/fHALMzHrNnKmmFifUqr0xMJlsx1sQ1cGIYWTsfVOI2mgsp+dvYVXRI9rZvjZoVgHYjfJAY7OsJW/rWhi2ja4s6/sD07MKQkgsoafdPPsTkHsewMjOXSYpv41as2LWTBJaltlyIKQwtceRQrLsBoHkJH5RloLcnvG79YLIVG1bSbqB4G0PJclv6/FwmhN2TM8kWhzAX2+ITR/nEIJJsQ73ZgJJ/HANLb1gfg0vu8fCA7iCAehI+qFtY2TQymGzFiEOWJw5MJ60nccr69DYW0ibdQLmAkj4X78eRQLHWJTtaqPhp7mN30Po0sCRBZT6wxNskqOQCSzbE9IbhgV4tLABlEj5oSvGDPX7Yxy0cFopuXRlLJnebDyrxNk72lraoJJO/FbSw7Mt1B5UaWGLuGBnMBpY0uGTDSW8SYOb3zbfCxPsxhAF0OuGDthE/2I8cidvqawotDiwxjIzF1pQ0uOzNhZi0hSXZP50cE+tXYpdQOottKQZ7e5KAkgSVfGtKNqDEMJM+jkFlZDB7P+43WRzQToQPOlo5gWVqZi4JIdnwMbXofrZlJT7O7s/uG9s/nXQJ7Z+eDfvHZ8OL48UNZ07FQtoYSNYPZltQklahpOWlN4yk94eygSV7nNACNC/hA0oUh/8ePjyQbMVKZrM9MHNwaMm1psRwknYZjRfcj9PuR+kU/DvC6jPbrhRaYiDJtrBk9yWP07BScJzuIaCWhA+ogziaJv3gP2Zj8V83MzuXhJa0BWV8fy6w7J8O47kgkw0uaQtL9rbS0BIDVgwk84FlPrykQSXtJkrPK25x5JDWFmA1wgc0sfhBns5rUooYWpIWlFwwSVtTsgEmu+X35x6PF3QPxa6lV3dPJlup4iy8hYEkv+UCy/BSzwku0FGED2hD8UM8XeCwnDWE0paUiXyAyQWW/VNJSMmHmVwLTHwcvy6KU/XH7WdjpbW2rBRc4tT8hcElDnXO3l+Tv6+rCFqH8AEsuYbQltHSvnY6dhGlrSgF21L7sttMVYNL7CpKW1DSoBKDyYLwkn+88Ll1A73JBHVAfQgfQFX0ltnaUkxwiXUvscUlTjwXt+xzM7nnppMVnGNXUanT9xeKgSsGlzSYDBe0qqT7YwHv/L75Y+J+XUZQPOEDaOngEmfD3TM1Mx9UCkLJxOIAs+Bx9vbA9NyCVpdSh0Gnhvp6lgwlaWiJrSuL9yWhJtkXu426TfNPxxA+gJafDTf5wB/oLbmrKIotJruTFpVsgMmGlpn58HIgO3ooe//gY+IsuVHsOorbS2WGlzjN/3xQmQ8vhaElDS7Duf3p8+lz/WvUvdAahA+go8VakXJbXdIuoz0xlCwRWuLjfLApeJyEmdxtfBxnzI3T/KeLK1ZyLguDSbZ+pzCoZJ+f35c+n36dFhjqQfgAqLDLKFmQsMTh0ItHGO1eIpykLS1pSImP888X7E8LdmMrTqnrFC3VAnNwOMk+ToqR0/DSv9S+3vz9+O8CyxE+AJpkhNGmkfK+R5ybJW19iXUraSgpDC3Z/em+g+/H59MWmHLWLVqsf033QQEmDSqxGym9v+C5NMjkQkx8HFtzaD/CB0CLi8OEk0nc1vaW/T0KW2CyQWY+lMSAEvftzgWY5H4aaBbti+sXRZMzc2Fyz2T4xZ7yRh+lYvjIB5N8OJkPLvnHyf0YanrCIQXhJT1mbW9PUh9EcxA+AFjQAhPKbIFJZ9fdOzm7oBVmz+R8K0thC0wywii9zd3fnbtNQ0zsSto5U1ktTCqe21B/TxiKrS3J/YUBZcHj3POxVWbxcYJM5YQPAKomzncysra7olaYfIiZmi0IKAcHlvh4by64pKElH2Jy4Sd+j9gtFaUhJ4TKWmNiPW4MIIWBZagvvZ8NN2l4KdxXeOwh+cDT05GjlIQPAJozxAxmZ62tROxOinO5pMEjDSsxoOydWhhgltsfW3LSwt4YZOKkdunQ6kqDTNTb05UNJ7lQkrbOxPtr+xaGlzifzNCiYJM9Jvt4bV8MM80/Ykn4AKBtxQ/hwb6eZDtsXXnDqVcLMoUhpXBfDDF70n25Vpm4b2/umHSCu+nZTH7hx2qII5YODio9+XCztr8nbBoZDNtO/6Wq/H1lvcaG/c0A0KFBprBrae+iAFMYXgoDzb7c4/R+Gmyy9+fDTByxlC5RsJw3HDYkfABAp1lTpa6lwjCzb3o+zCwMLoWhZjaZVK6RhA8AaJMwMxy3geqEmVoyewsAUFfCBwBQV8IHAFBXwgcAUFfCBwBQV8IHAFBXwgcAUFfCBwBQV8IHAFBXwgcAUFfCBwBQV8IHAFBXwgcA0Nmr2mYymeR2YmKi0S8FAChS+rmdfo63VPjYvXt3cnvUUUc1+qUAAGV8jo+MjKx4TFemmIhSR3Nzc+HFF18M69atC11dXVVPZTHU7NixIwwPD4d21knn2mnn61zbVyedr3NtPzFOxOCxefPm0N3d3VotH/EFb9mypaZ/R7z47fwD0Knn2mnn61zbVyedr3NtL6u1eKQUnAIAdSV8AAB11VHho7+/P1x11VXJbbvrpHPttPN1ru2rk87XuXa2pis4BQDaW0e1fAAAjSd8AAB1JXwAAHUlfAAAddV24eP6668Pr3/968PAwEA47bTTwiOPPLLi8f/0T/8Ujj/++OT4k08+OXzzm98MzW779u3h1FNPTWaBPfzww8P5558fnn766RW/5pZbbklmjC3c4jm3gr/5m7856LXHa9Zu1zWKP7uLzzVu27Zta/nrev/994dzzz03mf0wvs677rprwfOx9v3KK68MmzZtCoODg+HMM88MzzzzTNXf881wvtPT0+Gyyy5LfjaHhoaSY/7gD/4gmd252u+FZri2f/iHf3jQ637Pe97Tktd2tXNd6v0bt89+9rMtd11rqa3Cxz/+4z+Gj3/848mQpscffzyccsop4d3vfnd49dVXlzz+gQceCL/3e78X/uiP/ig88cQTyYd43P7nf/4nNLPvfve7yYfRQw89FP793/89+Y/srLPOCnv37l3x6+LMei+99FJ+e/7550OrOPHEExe89u9973vLHtuq1zV69NFHF5xnvL7R+9///pa/rvHnM74n4wfKUj7zmc+EL3zhC+FLX/pSePjhh5MP5fj+PXDgQNXe881yvvv27Ute7xVXXJHc/vM//3PyC8T73ve+qr4XmuXaRjFsFL7u22+/fcXv2azXdrVzLTzHuP3DP/xDEiZ+53d+p+Wua01l2siv/dqvZbZt25Z/PDs7m9m8eXNm+/btSx7/gQ98IHPOOecs2HfaaadlPvrRj2ZayauvvhqHS2e++93vLnvMzTffnBkZGcm0oquuuipzyimnFH18u1zX6GMf+1jm2GOPzczNzbXVdY0/r3feeWf+cTy/I488MvPZz342v29sbCzT39+fuf3226v2nm+W813KI488khz3/PPPV+290CznetFFF2XOO++8kr5PK1zbYq5rPO93vetdKx5zVQtc12prm5aPqamp8IMf/CBpqi1cJyY+fvDBB5f8mri/8PgoJuvljm9W4+Pjye2GDRtWPG7Pnj3hmGOOSRY4Ou+888IPf/jD0Cpi83ts5nzDG94QLrzwwvDCCy8se2y7XNf4M/3Vr341fPjDH15xkcVWvq6p5557Lrz88ssLrltcIyI2tS933cp5zzf7+zhe5/Xr11ftvdBMvvOd7yTdxG9605vCxRdfHF577bVlj22Xa/vKK6+Eu+++O2mFXc0zLXpdy9U24eMXv/hFmJ2dDUccccSC/fFx/E9tKXF/Kcc3o7gK8KWXXhre8Y53hJNOOmnZ4+IbPjb/feMb30g+0OLXvf3tbw//93//F5pd/ACKtQ3f+ta3wg033JB8UP36r/96snpiu17XKPYlj42NJf3l7XhdC6XXppTrVs57vlnFrqVYAxK7C1daeKzU90KziF0uX/nKV8K9994brr322qTr+Oyzz06uXztf21tvvTWpzfvt3/7tFY87rUWvayWablVbShNrP2Itw2r9g29729uSLRU/oE444YRw4403hmuuuSY0s/ifVOrNb35z8kaNv+nfcccdRf1G0ar+/u//Pjn3+NtQO15XsmLN1gc+8IGk4DZ+8LTje+GCCy7I349FtvG1H3vssUlryBlnnBHaVfzFILZirFYEfnaLXtdKtE3Lx6GHHhp6enqSZq5C8fGRRx655NfE/aUc32z+7M/+LPzrv/5ruO+++8KWLVtK+tre3t7wlre8JTz77LOh1cRm6Te+8Y3LvvZWv65RLBq95557wh//8R93xHVNr00p162c93yzBo94vWNxcanLra/2XmhWsWshXr/lXnc7XNv//M//TIqIS30Pt/J17cjw0dfXF9761rcmzXqp2AQdHxf+Zlgo7i88Por/ASx3fLOIvyHF4HHnnXeG//iP/whbt24t+XvEJs2nnnoqGdbYamKNw09+8pNlX3urXtdCN998c9I/fs4553TEdY0/w/FDpfC6TUxMJKNelrtu5bznmzF4xL7+GDQ3btxY9fdCs4rdgrHmY7nX3erXNm25jOcQR8Z0ynUtSaaNfP3rX0+q42+55ZbM//7v/2Y+8pGPZNavX595+eWXk+c/+MEPZj75yU/mj//+97+fWbNmTeZzn/tc5kc/+lFScdzb25t56qmnMs3s4osvTkY4fOc738m89NJL+W3fvn35Yxaf69VXX5359re/nfnJT36S+cEPfpC54IILMgMDA5kf/vCHmWb3F3/xF8m5Pvfcc8k1O/PMMzOHHnpoMsqnna5rYVX/0UcfnbnssssOeq6Vr+vu3bszTzzxRLLF/3o+//nPJ/fT0R2f/vSnk/frN77xjcx///d/J6MEtm7dmtm/f3/+e8RRA1/84heLfs836/lOTU1l3ve+92W2bNmSefLJJxe8jycnJ5c939XeC814rvG5T3ziE5kHH3wwed333HNP5ld+5Vcyxx13XObAgQMtd21X+zmOxsfHM2vXrs3ccMMNS36Pd7XIda2ltgofUbyg8T/uvr6+ZKjWQw89lH/uN37jN5IhX4XuuOOOzBvf+Mbk+BNPPDFz9913Z5pd/IFfaovDLpc710svvTT/73LEEUdk3vve92Yef/zxTCv43d/93cymTZuS1/66170uefzss8+23XVNxTARr+fTTz990HOtfF3vu+++JX9u0/OJw22vuOKK5Dzih84ZZ5xx0L/BMccck4TJYt/zzXq+8UNmufdx/Lrlzne190Iznmv8peiss87KHHbYYckvAfGc/uRP/uSgENEq13a1n+PoxhtvzAwODibDxZdyTItc11rqin+U1lYCAFC+tqn5AABag/ABANSV8AEA1JXwAQDUlfABANSV8AEA1JXwAQDUlfABANSV8AEA1JXwAQDUlfABANSV8AEAhHr6//PEVAen3JijAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToBeTrained = True\n",
    "if ToBeTrained:\n",
    "    avg_loss = []\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    numEpochs = 20\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    loop = tqdm(range(numEpochs))\n",
    "    for i in loop:\n",
    "        c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "        \n",
    "        # Forward\n",
    "        sfts = time.time() # slow forward time start\n",
    "        c1s,mask1s = nested_loop_convolution(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "        c2s,mask2s = nested_loop_convolution(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "        c3s,mask3s = nested_loop_convolution(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "\n",
    "        imlps = c3s.reshape(1,-1)\n",
    "        fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "        sfte = time.time() # slow forward time end\n",
    "        sft = sfte - sfts\n",
    "        forward_time.append(sft)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa,train_labels[0])\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        sbts = time.time() # slow backward time start\n",
    "        dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "        dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "        gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "\n",
    "        gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "        gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "        sbte = time.time() # slow backward time end\n",
    "        sbt = sbte - sbts\n",
    "        backward_time.append(sbt)\n",
    "\n",
    "        # Weights update\n",
    "        w1 -= lr*dL_dw1\n",
    "        b1 -= lr*dL_db1\n",
    "        w2 -= lr*dL_dw2\n",
    "        b2 -= lr*dL_db2\n",
    "        k3 -= lr*gk3\n",
    "        k2 -= lr*gk2\n",
    "        k1 -= lr*gk1\n",
    "        bc3 -= lr*gb3.reshape(-1)\n",
    "        bc2 -= lr*gb2.reshape(-1)\n",
    "        bc1 -= lr*gb1.reshape(-1)\n",
    "        \n",
    "        if len(avg_loss) >= 2:\n",
    "            loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "    plt.plot(avg_loss)\n",
    "    plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef545289",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 3.6265 s\n",
    "- average backward time : 9.8262 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"images\\Slow Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea8a5",
   "metadata": {},
   "source": [
    "### Test for Fast approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1f5ac",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a336a9",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "756e2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights():\n",
    "    weights = {}\n",
    "\n",
    "    weights['k1'] = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "    weights['bc1'] = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "    weights['k2'] = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "    weights['bc2'] = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "    weights['k3'] = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "    weights['bc3'] = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "    weights['w1'] = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "    weights['b1'] = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "    weights['w2'] = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "    weights['b2'] = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def initialize_weights_no_bias():\n",
    "    weights = {}\n",
    "\n",
    "    weights['k1'] = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "    weights['k2'] = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "    weights['k3'] = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "    weights['w1'] = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "    weights['w2'] = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d6af1",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fff279",
   "metadata": {},
   "source": [
    "### Training the \"Fast\" NumPy CNN (Single Image Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b626671",
   "metadata": {},
   "source": [
    "This tests training using the Im2Col-based `im2col_convolution` and the revised `Fast_ReLU_Gradient` (from cell `c808bdb6`) on a single image.\n",
    "\n",
    "**Per-Epoch Steps (differences from \"Slow\" are conv/grad functions):**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> im2col_convolution (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> im2col_convolution (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> im2col_convolution (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa`\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** `ReLU_SoftMax_FC_Backward` for MLP, then `Fast_ReLU_Gradient` (using `sliding_window_view` for `gi` and `gk`) for conv layers.\n",
    "4.  **Weight Update:** Standard gradient descent.\n",
    "\n",
    "The loss is plotted. Consistent padding/stride ensures correct feature dimensions for the MLP. This setup tests the learning capability and performance of the more optimized NumPy convolution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 500\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "\n",
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)\n",
    "\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = im2col_convolution(c0.astype(np.float32),k1,bc1,padding=0,stride=2)\n",
    "    c2s,mask2s = im2col_convolution(c1s.astype(np.float32),k2,bc2,padding=1,stride=2)\n",
    "    c3s,mask3s = im2col_convolution(c2s.astype(np.float32),k3,bc3,padding=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = im2col_gradient(c2s,dL_i_mlp,k3,mask3s,padding=0,stride=2)\n",
    "    gi2,gk2,gb2 = im2col_gradient(c1s,gi3,k2,mask2s,padding=1,stride=2)\n",
    "    gi1,gk1,gb1 = im2col_gradient(c0,gi2,k1,mask1s,padding=0,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632686e",
   "metadata": {},
   "source": [
    "No bias version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "283c5900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: 6.785682802873736, 52.89028727703807, 106.03498594875933, 415.73973189549963, 28.95009463759042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 68.89it/s, avgBackward=0.006081899007161458 s, avgForward=0.0019659201304117837 s, pendence=0.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 ReLU Masks ---\n",
      "  mask1s active: 29.59%\n",
      "  mask2s active: 35.94%\n",
      "  mask3s active: 68.75%\n",
      "  MLP ReLU active: 100.00%\n",
      "\n",
      "sa[0]: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "--- Epoch 1 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 9.66e+01, sum: 2.03e+03\n",
      "  gi3 (to conv2) norm: 3.27e+03, sum: 1.70e+05\n",
      "  gi2 (to conv1) norm: 1.05e+05, sum: 5.61e+06\n",
      "  gi1 (to input) norm: 4.69e+05, sum: 6.98e+06\n",
      "\n",
      "--- Epoch 1 Gradients BEFORE update ---\n",
      "  gk1 norm: 5.68e+05, sum: 6.41e+06\n",
      "  gk2 norm: 7.42e+04, sum: 6.49e+06\n",
      "  gk3 norm: 5.59e+04, sum: 6.48e+06\n",
      "  dL_dw1 norm: 9.08e+05, sum: 6.88e+06\n",
      "  dL_dw2 norm: 4.01e+07, sum: 2.33e-10\n",
      "\n",
      "Weights after first update:\n",
      "  k1 norm: 2.809095466308095, sum: 0.35322747536575383\n",
      "  k2 norm: 51.63819625825995, sum: 4029.3705267815194\n",
      "  k3 norm: 104.26338406392159, sum: 16359.483954756197\n",
      "  w1 norm: 412.98911754900695, sum: 255819.38111464973\n",
      "  w2 norm: 402.2759134791379, sum: 1234.8840510032333\n",
      "Difference in weights: 4.9493080153972615, 52.28030073529911, 105.72654268105933, 415.75788440885435, 402.2865900209918\n",
      "\n",
      "--- Epoch 2 ReLU Masks ---\n",
      "  mask1s active: 13.74%\n",
      "  mask2s active: 35.94%\n",
      "  mask3s active: 68.75%\n",
      "  MLP ReLU active: 100.00%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "--- Epoch 2 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 2 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 4.9493080153972615, 52.28030073529911, 105.72654268105933, 415.75788440885435, 402.2865900209918\n",
      "\n",
      "--- Epoch 3 ReLU Masks ---\n",
      "  mask1s active: 13.74%\n",
      "  mask2s active: 35.94%\n",
      "  mask3s active: 68.75%\n",
      "  MLP ReLU active: 100.00%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "--- Epoch 3 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 3 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 4.9493080153972615, 52.28030073529911, 105.72654268105933, 415.75788440885435, 402.2865900209918\n",
      "\n",
      "--- Epoch 4 ReLU Masks ---\n",
      "  mask1s active: 13.74%\n",
      "  mask2s active: 35.94%\n",
      "  mask3s active: 68.75%\n",
      "  MLP ReLU active: 100.00%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "--- Epoch 4 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 4 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 4.9493080153972615, 52.28030073529911, 105.72654268105933, 415.75788440885435, 402.2865900209918\n",
      "\n",
      "--- Epoch 5 ReLU Masks ---\n",
      "  mask1s active: 13.74%\n",
      "  mask2s active: 35.94%\n",
      "  mask3s active: 68.75%\n",
      "  MLP ReLU active: 100.00%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "--- Epoch 5 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 5 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 4.9493080153972615, 52.28030073529911, 105.72654268105933, 415.75788440885435, 402.2865900209918\n",
      "\n",
      "--- Epoch 6 ReLU Masks ---\n",
      "  mask1s active: 13.74%\n",
      "  mask2s active: 35.94%\n",
      "  mask3s active: 68.75%\n",
      "  MLP ReLU active: 100.00%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "--- Epoch 6 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 6 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 4.9493080153972615, 52.28030073529911, 105.72654268105933, 415.75788440885435, 402.2865900209918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 6\n",
    "bs = 1\n",
    "lr = 0.00001\n",
    "\n",
    "# fix a seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "k1 = np.random.rand(*numpy_weights['k1'].shape)\n",
    "k2 = np.random.rand(*numpy_weights['k2'].shape)\n",
    "k3 = np.random.rand(*numpy_weights['k3'].shape)\n",
    "w1 = np.random.rand(*numpy_weights['w1'].shape)\n",
    "w2 = np.random.rand(*numpy_weights['w2'].shape)\n",
    "\n",
    "print(f\"Initial weights: {np.linalg.norm(k1-numpy_weights['k1'])}, {np.linalg.norm(k2-numpy_weights['k2'])}, {np.linalg.norm(k3-numpy_weights['k3'])}, {np.linalg.norm(w1-numpy_weights['w1'])}, {np.linalg.norm(w2-numpy_weights['w2'])}\")\n",
    "\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = (train_images[0].reshape(1,1,28,28) / 255.0).astype(np.float64)\n",
    "    current_train_label = train_labels[0].astype(np.float64)\n",
    "\n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = im2col_optimized_float64_no_bias(c0.astype(np.float64),k1,None,padding=0,stride=2)\n",
    "    c2s,mask2s = im2col_optimized_float64_no_bias(c1s.astype(np.float64),k2,None,padding=1,stride=2)\n",
    "    c3s,mask3s = im2col_optimized_float64_no_bias(c2s.astype(np.float64),k3,None,padding=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FC_no_bias(imlps,w1,w2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "\n",
    "    print(f\"\\n--- Epoch {i+1} ReLU Masks ---\")\n",
    "    print(f\"  mask1s active: {np.mean(mask1s)*100:.2f}%\")\n",
    "    print(f\"  mask2s active: {np.mean(mask2s)*100:.2f}%\")\n",
    "    print(f\"  mask3s active: {np.mean(mask3s)*100:.2f}%\")\n",
    "    # Anche per la ReLU nel MLP (fl > 0)\n",
    "    print(f\"  MLP ReLU active: {np.mean((fl > 0))*100:.2f}%\\n\")\n",
    "\n",
    "    print(f\"sa[0]: {sa[0]}\")\n",
    "    print(f\"train_labels[0]: {current_train_label}\")\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa[0], current_train_label)\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    print(f\"\\n--- Epoch {i+1} Intermediate Gradients ---\")\n",
    "    dL_i_mlp,dL_dw1,dL_dw2 = ReLU_SoftMax_FC_Backward_no_bias(bs,sa,train_labels[0].reshape(1, -1),w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "    print(f\"  dL_i_mlp (to conv3) norm: {np.linalg.norm(dL_i_mlp):.2e}, sum: {np.sum(dL_i_mlp):.2e}\")\n",
    "\n",
    "    gi3,gk3 = im2col_gradient_optimized_float64_no_bias(c2s,dL_i_mlp,k3,mask3s,padding=0,stride=2)\n",
    "    print(f\"  gi3 (to conv2) norm: {np.linalg.norm(gi3):.2e}, sum: {np.sum(gi3):.2e}\")\n",
    "\n",
    "    gi2,gk2 = im2col_gradient_optimized_float64_no_bias(c1s,gi3,k2,mask2s,padding=1,stride=2)\n",
    "    print(f\"  gi2 (to conv1) norm: {np.linalg.norm(gi2):.2e}, sum: {np.sum(gi2):.2e}\")\n",
    "\n",
    "    gi1,gk1 = im2col_gradient_optimized_float64_no_bias(c0,gi2,k1,mask1s,padding=0,stride=2)\n",
    "    print(f\"  gi1 (to input) norm: {np.linalg.norm(gi1):.2e}, sum: {np.sum(gi1):.2e}\")\n",
    "\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Epoch {i+1} Gradients BEFORE update ---\")\n",
    "    print(f\"  gk1 norm: {np.linalg.norm(gk1):.2e}, sum: {np.sum(gk1):.2e}\")\n",
    "    print(f\"  gk2 norm: {np.linalg.norm(gk2):.2e}, sum: {np.sum(gk2):.2e}\")\n",
    "    print(f\"  gk3 norm: {np.linalg.norm(gk3):.2e}, sum: {np.sum(gk3):.2e}\")\n",
    "    print(f\"  dL_dw1 norm: {np.linalg.norm(dL_dw1):.2e}, sum: {np.sum(dL_dw1):.2e}\")\n",
    "    print(f\"  dL_dw2 norm: {np.linalg.norm(dL_dw2):.2e}, sum: {np.sum(dL_dw2):.2e}\")\n",
    "\n",
    "    # Gradient clipping\n",
    "    # max_grad_norm = 5\n",
    "    # all_grads_flat = np.concatenate([\n",
    "    #     gk1.ravel(), gk2.ravel(), gk3.ravel(),\n",
    "    #     dL_dw1.ravel(), dL_dw2.ravel()\n",
    "    # ])\n",
    "    # norm = np.linalg.norm(all_grads_flat)\n",
    "    # if norm > max_grad_norm:\n",
    "    #     clip_factor = max_grad_norm / (norm + 1e-6) # Aggiungi epsilon per evitare divisione per zero\n",
    "    #     gk1 *= clip_factor\n",
    "    #     gk2 *= clip_factor\n",
    "    #     gk3 *= clip_factor\n",
    "    #     dL_dw1 *= clip_factor\n",
    "    #     dL_dw2 *= clip_factor\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    # b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    # b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    # bc3 -= lr*gb3\n",
    "    # bc2 -= lr*gb2\n",
    "    # bc1 -= lr*gb1\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"\\nWeights after first update:\")\n",
    "        print(f\"  k1 norm: {np.linalg.norm(k1)}, sum: {np.sum(k1)}\")\n",
    "        print(f\"  k2 norm: {np.linalg.norm(k2)}, sum: {np.sum(k2)}\")\n",
    "        print(f\"  k3 norm: {np.linalg.norm(k3)}, sum: {np.sum(k3)}\")\n",
    "        print(f\"  w1 norm: {np.linalg.norm(w1)}, sum: {np.sum(w1)}\")\n",
    "        print(f\"  w2 norm: {np.linalg.norm(w2)}, sum: {np.sum(w2)}\")\n",
    "    \n",
    "    print(f\"Difference in weights: {np.linalg.norm(k1-numpy_weights['k1'])}, {np.linalg.norm(k2-numpy_weights['k2'])}, {np.linalg.norm(k3-numpy_weights['k3'])}, {np.linalg.norm(w1-numpy_weights['w1'])}, {np.linalg.norm(w2-numpy_weights['w2'])}\")\n",
    "\n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{np.mean(forward_time)} s\", avgBackward=f\"{np.mean(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d9ac7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: 7.315055162435693, 52.85940395184716, 106.201916573569, 416.42291107765806, 29.498981615590733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 68.58it/s, avgBackward=0.005353411038716634 s, avgForward=0.002889712651570638 s, pendence=0.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 ReLU Masks ---\n",
      "  mask1s active: 100.00%\n",
      "  mask2s active: 100.00%\n",
      "  mask3s active: 100.00%\n",
      "  MLP ReLU active: 100.00%\n",
      "\n",
      "sa[0]: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Loss: 18.4207\n",
      "\n",
      "--- Epoch 1 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 1.87e+02, sum: 7.68e+03\n",
      "  gi3 (to conv2) norm: 1.55e+04, sum: 9.86e+05\n",
      "  gi2 (to conv1) norm: 6.14e+05, sum: 4.84e+07\n",
      "  gi1 (to input) norm: 3.52e+06, sum: 9.81e+07\n",
      "\n",
      "--- Epoch 1 Gradients BEFORE update ---\n",
      "  gk1 norm: 2.38e+06, sum: 2.69e+07\n",
      "  gk2 norm: 7.85e+05, sum: 6.62e+07\n",
      "  gk3 norm: 3.76e+05, sum: 6.69e+07\n",
      "  dL_dw1 norm: 1.43e+06, sum: 6.61e+07\n",
      "  dL_dw2 norm: 9.93e+07, sum: 1.86e-09\n",
      "\n",
      "Weights after first update:\n",
      "  k1 norm: 2377.4178445994216, sum: -26868.608666005945\n",
      "  b_conv1 norm: 2.6445735723052763, sum: 12.827401165819861\n",
      "  k2 norm: 744.0920090682417, sum: -62066.437334830174\n",
      "  b_conv2 norm: 4.802111713899647, sum: 32.64921560393243\n",
      "  k3 norm: 292.6565219677304, sum: -50485.11364611013\n",
      "  b_conv3 norm: 6.876599140545479, sum: 68.00971438038222\n",
      "  w1 norm: 1462.4350596322486, sum: 190374.48206884798\n",
      "  b1 norm: 8.737290110630958, sum: 116.42483684590431\n",
      "  w2 norm: 99270.54923082996, sum: 1263.7121751595305\n",
      "  b2 norm: 1.6439909046811938, sum: 4.514597602471728\n",
      "Difference in weights: 2378.425972479004, 744.2645471653262, 292.03209933716164, 1462.8500319561292, 99270.53306620455\n",
      "\n",
      "--- Epoch 2 ReLU Masks ---\n",
      "  mask1s active: 70.41%\n",
      "  mask2s active: 10.94%\n",
      "  mask3s active: 75.00%\n",
      "  MLP ReLU active: 64.40%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Loss: 0.0000\n",
      "\n",
      "--- Epoch 2 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 2 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 2378.425972479004, 744.2645471653262, 292.03209933716164, 1462.8500319561292, 99270.53306620455\n",
      "\n",
      "--- Epoch 3 ReLU Masks ---\n",
      "  mask1s active: 70.41%\n",
      "  mask2s active: 10.94%\n",
      "  mask3s active: 75.00%\n",
      "  MLP ReLU active: 64.40%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Loss: 0.0000\n",
      "\n",
      "--- Epoch 3 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 3 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 2378.425972479004, 744.2645471653262, 292.03209933716164, 1462.8500319561292, 99270.53306620455\n",
      "\n",
      "--- Epoch 4 ReLU Masks ---\n",
      "  mask1s active: 70.41%\n",
      "  mask2s active: 10.94%\n",
      "  mask3s active: 75.00%\n",
      "  MLP ReLU active: 64.40%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Loss: 0.0000\n",
      "\n",
      "--- Epoch 4 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 4 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 2378.425972479004, 744.2645471653262, 292.03209933716164, 1462.8500319561292, 99270.53306620455\n",
      "\n",
      "--- Epoch 5 ReLU Masks ---\n",
      "  mask1s active: 70.41%\n",
      "  mask2s active: 10.94%\n",
      "  mask3s active: 75.00%\n",
      "  MLP ReLU active: 64.40%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Loss: 0.0000\n",
      "\n",
      "--- Epoch 5 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 5 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 2378.425972479004, 744.2645471653262, 292.03209933716164, 1462.8500319561292, 99270.53306620455\n",
      "\n",
      "--- Epoch 6 ReLU Masks ---\n",
      "  mask1s active: 70.41%\n",
      "  mask2s active: 10.94%\n",
      "  mask3s active: 75.00%\n",
      "  MLP ReLU active: 64.40%\n",
      "\n",
      "sa[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Loss: 0.0000\n",
      "\n",
      "--- Epoch 6 Intermediate Gradients ---\n",
      "  dL_i_mlp (to conv3) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi3 (to conv2) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi2 (to conv1) norm: 0.00e+00, sum: 0.00e+00\n",
      "  gi1 (to input) norm: 0.00e+00, sum: 0.00e+00\n",
      "\n",
      "--- Epoch 6 Gradients BEFORE update ---\n",
      "  gk1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk2 norm: 0.00e+00, sum: 0.00e+00\n",
      "  gk3 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw1 norm: 0.00e+00, sum: 0.00e+00\n",
      "  dL_dw2 norm: 0.00e+00, sum: 0.00e+00\n",
      "Difference in weights: 2378.425972479004, 744.2645471653262, 292.03209933716164, 1462.8500319561292, 99270.53306620455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 6\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "\n",
    "# fix a seed for reproducibility\n",
    "np.random.seed(4567)\n",
    "\n",
    "# k1 = np.random.normal(0,1,(numpy_weights['k1'].shape))\n",
    "# b_conv1 = np.random.normal(0,1,(numpy_weights['b_conv1'].shape))\n",
    "# k2 = np.random.normal(0,1,(numpy_weights['k2'].shape))\n",
    "# b_conv2 = np.random.normal(0,1,(numpy_weights['b_conv2'].shape))\n",
    "# k3 = np.random.normal(0,1,(numpy_weights['k3'].shape))\n",
    "# b_conv3 = np.random.normal(0,1,(numpy_weights['b_conv3'].shape))\n",
    "# w1 = np.random.normal(0,1,(numpy_weights['w1'].shape))\n",
    "# b1 = np.random.normal(0,1,(numpy_weights['b1'].shape))\n",
    "# w2 = np.random.normal(0,1,(numpy_weights['w2'].shape))\n",
    "# b2 = np.random.normal(0,1,(numpy_weights['b2'].shape))\n",
    "\n",
    "k1 = np.random.rand(*numpy_weights['k1'].shape)\n",
    "b_conv1 = np.random.rand(*numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(*numpy_weights['k2'].shape)\n",
    "b_conv2 = np.random.rand(*numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(*numpy_weights['k3'].shape)\n",
    "b_conv3 = np.random.rand(*numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(*numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(*numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(*numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(*numpy_weights['b2'].shape)\n",
    "\n",
    "print(f\"Initial weights: {np.linalg.norm(k1-numpy_weights['k1'])}, {np.linalg.norm(k2-numpy_weights['k2'])}, {np.linalg.norm(k3-numpy_weights['k3'])}, {np.linalg.norm(w1-numpy_weights['w1'])}, {np.linalg.norm(w2-numpy_weights['w2'])}\")\n",
    "\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = (train_images[0].reshape(1,1,28,28) / 255.0).astype(np.float64)\n",
    "    current_train_label = train_labels[0].astype(np.float64)\n",
    "\n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = im2col_optimized(c0.astype(np.float64),k1,b_conv1,padding=0,stride=2)\n",
    "    c2s,mask2s = im2col_optimized(c1s.astype(np.float64),k2,b_conv2,padding=1,stride=2)\n",
    "    c3s,mask3s = im2col_optimized(c2s.astype(np.float64),k3,b_conv3,padding=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "\n",
    "    print(f\"\\n--- Epoch {i+1} ReLU Masks ---\")\n",
    "    print(f\"  mask1s active: {np.mean(mask1s)*100:.2f}%\")\n",
    "    print(f\"  mask2s active: {np.mean(mask2s)*100:.2f}%\")\n",
    "    print(f\"  mask3s active: {np.mean(mask3s)*100:.2f}%\")\n",
    "    # Anche per la ReLU nel MLP (fl > 0)\n",
    "    print(f\"  MLP ReLU active: {np.mean((fl > 0))*100:.2f}%\\n\")\n",
    "\n",
    "    print(f\"sa[0]: {sa[0]}\")\n",
    "    print(f\"train_labels[0]: {current_train_label}\")\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa[0], current_train_label)\n",
    "\n",
    "    print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    print(f\"\\n--- Epoch {i+1} Intermediate Gradients ---\")\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0].reshape(1, -1),w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "    print(f\"  dL_i_mlp (to conv3) norm: {np.linalg.norm(dL_i_mlp):.2e}, sum: {np.sum(dL_i_mlp):.2e}\")\n",
    "\n",
    "    gi3,gk3,gb3 = im2col_gradient_optimized(c2s,dL_i_mlp,k3,mask3s,padding=0,stride=2)\n",
    "    print(f\"  gi3 (to conv2) norm: {np.linalg.norm(gi3):.2e}, sum: {np.sum(gi3):.2e}\")\n",
    "\n",
    "    gi2,gk2,gb2 = im2col_gradient_optimized(c1s,gi3,k2,mask2s,padding=1,stride=2)\n",
    "    print(f\"  gi2 (to conv1) norm: {np.linalg.norm(gi2):.2e}, sum: {np.sum(gi2):.2e}\")\n",
    "\n",
    "    gi1,gk1,gb1 = im2col_gradient_optimized(c0,gi2,k1,mask1s,padding=0,stride=2)\n",
    "    print(f\"  gi1 (to input) norm: {np.linalg.norm(gi1):.2e}, sum: {np.sum(gi1):.2e}\")\n",
    "\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Epoch {i+1} Gradients BEFORE update ---\")\n",
    "    print(f\"  gk1 norm: {np.linalg.norm(gk1):.2e}, sum: {np.sum(gk1):.2e}\")\n",
    "    print(f\"  gk2 norm: {np.linalg.norm(gk2):.2e}, sum: {np.sum(gk2):.2e}\")\n",
    "    print(f\"  gk3 norm: {np.linalg.norm(gk3):.2e}, sum: {np.sum(gk3):.2e}\")\n",
    "    print(f\"  dL_dw1 norm: {np.linalg.norm(dL_dw1):.2e}, sum: {np.sum(dL_dw1):.2e}\")\n",
    "    print(f\"  dL_dw2 norm: {np.linalg.norm(dL_dw2):.2e}, sum: {np.sum(dL_dw2):.2e}\")\n",
    "\n",
    "    # Gradient clipping\n",
    "    # max_grad_norm = 5\n",
    "    # all_grads_flat = np.concatenate([\n",
    "    #     gk1.ravel(), gk2.ravel(), gk3.ravel(),\n",
    "    #     dL_dw1.ravel(), dL_dw2.ravel()\n",
    "    # ])\n",
    "    # norm = np.linalg.norm(all_grads_flat)\n",
    "    # if norm > max_grad_norm:\n",
    "    #     clip_factor = max_grad_norm / (norm + 1e-6) # Aggiungi epsilon per evitare divisione per zero\n",
    "    #     gk1 *= clip_factor\n",
    "    #     gk2 *= clip_factor\n",
    "    #     gk3 *= clip_factor\n",
    "    #     dL_dw1 *= clip_factor\n",
    "    #     dL_dw2 *= clip_factor\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"\\nWeights after first update:\")\n",
    "        print(f\"  k1 norm: {np.linalg.norm(k1)}, sum: {np.sum(k1)}\")\n",
    "        print(f\"  b_conv1 norm: {np.linalg.norm(b_conv1)}, sum: {np.sum(b_conv1)}\")\n",
    "        print(f\"  k2 norm: {np.linalg.norm(k2)}, sum: {np.sum(k2)}\")\n",
    "        print(f\"  b_conv2 norm: {np.linalg.norm(b_conv2)}, sum: {np.sum(b_conv2)}\")\n",
    "        print(f\"  k3 norm: {np.linalg.norm(k3)}, sum: {np.sum(k3)}\")\n",
    "        print(f\"  b_conv3 norm: {np.linalg.norm(b_conv3)}, sum: {np.sum(b_conv3)}\")\n",
    "        print(f\"  w1 norm: {np.linalg.norm(w1)}, sum: {np.sum(w1)}\")\n",
    "        print(f\"  b1 norm: {np.linalg.norm(b1)}, sum: {np.sum(b1)}\")\n",
    "        print(f\"  w2 norm: {np.linalg.norm(w2)}, sum: {np.sum(w2)}\")\n",
    "        print(f\"  b2 norm: {np.linalg.norm(b2)}, sum: {np.sum(b2)}\")\n",
    "    \n",
    "    print(f\"Difference in weights: {np.linalg.norm(k1-numpy_weights['k1'])}, {np.linalg.norm(k2-numpy_weights['k2'])}, {np.linalg.norm(k3-numpy_weights['k3'])}, {np.linalg.norm(w1-numpy_weights['w1'])}, {np.linalg.norm(w2-numpy_weights['w2'])}\")\n",
    "\n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{np.mean(forward_time)} s\", avgBackward=f\"{np.mean(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b37373",
   "metadata": {},
   "source": [
    "### Compare all implementations (training and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc4da756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training PyTorch CNN ---\n",
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x16 and 2048x250)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 335\u001b[0m\n\u001b[0;32m    332\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    333\u001b[0m pytorch_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 335\u001b[0m trained_pytorch_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pytorch_cnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_images_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_images_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE_PYTORCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# 2. Train Nested-Loops NumPy CNN (Can be very slow)\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m########## WARNING: Nested Loops NumPy training can be extremely slow!!!\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# trained_weights_nested_loops = train_numpy_cnn(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# 5. Train Im2Col Optimized Float64 NumPy CNN\u001b[39;00m\n\u001b[0;32m    373\u001b[0m trained_weights_im2col_opt_float64 \u001b[38;5;241m=\u001b[39m train_numpy_cnn(\n\u001b[0;32m    374\u001b[0m     train_images_subset, train_labels_subset, test_images_subset, test_labels_subset,\n\u001b[0;32m    375\u001b[0m     conv_forward_fn\u001b[38;5;241m=\u001b[39mim2col_optimized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mEPOCHS, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE_NUMPY, learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE\n\u001b[0;32m    380\u001b[0m )\n",
      "Cell \u001b[1;32mIn[109], line 58\u001b[0m, in \u001b[0;36mtrain_pytorch_cnn\u001b[1;34m(train_images_np, train_labels_np, test_images_np, test_labels_np, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     56\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m tri[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     57\u001b[0m sample_label \u001b[38;5;241m=\u001b[39m trl[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 58\u001b[0m sample_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m initial_loss \u001b[38;5;241m=\u001b[39m criterion(sample_output, torch\u001b[38;5;241m.\u001b[39mmax(sample_label, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Francesco\\anaconda3\\envs\\SEAI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Francesco\\anaconda3\\envs\\SEAI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[90], line 85\u001b[0m, in \u001b[0;36mSimpleCNN_no_bias.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# MLP\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu4(x)\n\u001b[0;32m     87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\Francesco\\anaconda3\\envs\\SEAI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Francesco\\anaconda3\\envs\\SEAI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Francesco\\anaconda3\\envs\\SEAI\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x16 and 2048x250)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- PyTorch CNNDataset (from notebook cell 5ff84faf, potentially commented out) ---\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, digits, labels, transform=None):\n",
    "        assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "        self.digits = digits\n",
    "        self.labels = labels\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.digits)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digits[idx]\n",
    "        label = self.labels[idx]\n",
    "        # PyTorch Conv2d expects (B, C, H, W)\n",
    "        # MNIST images are (H, W), need to add channel dim\n",
    "        digit = digit.unsqueeze(0) # (H, W) -> (1, H, W)\n",
    "        return digit, label\n",
    "\n",
    "# --- Training Functions ---\n",
    "\n",
    "def train_pytorch_cnn(train_images_np, train_labels_np, test_images_np, test_labels_np, num_epochs=5, batch_size=128, learning_rate=0.001):\n",
    "    print(\"\\n--- Training PyTorch CNN ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data\n",
    "    tri = torch.from_numpy(train_images_np).float() / 255.0\n",
    "    trl = torch.from_numpy(train_labels_np).float() # Already one-hot\n",
    "    tsi = torch.from_numpy(test_images_np).float() / 255.0\n",
    "    tsl = torch.from_numpy(test_labels_np).float()   # Already one-hot\n",
    "\n",
    "    train_dataset = CNNDataset(tri, trl)\n",
    "    test_dataset = CNNDataset(tsi, tsl)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SimpleCNN_no_bias(num_classes=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample_input = tri[0].unsqueeze(0).to(device)\n",
    "        sample_label = trl[0].unsqueeze(0).to(device)\n",
    "        sample_output = model(sample_input)\n",
    "        initial_loss = criterion(sample_output, torch.max(sample_label, 1)[1])\n",
    "        print(f\"Initial loss: {initial_loss.item():.4f}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        test_loss_val = 0.0 # Renamed to avoid conflict with outer scope 'loss'\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                current_test_loss = criterion(outputs, labels) # Renamed\n",
    "                test_loss_val += current_test_loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, true_labels_indices = torch.max(labels, 1)\n",
    "                total += true_labels_indices.size(0)\n",
    "                correct += (predicted == true_labels_indices).sum().item()\n",
    "        \n",
    "        avg_test_loss = test_loss_val / len(test_loader.dataset)\n",
    "        accuracy = 100 * correct / total\n",
    "        test_accuracies.append(accuracy)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    print(\"PyTorch Training Complete.\")\n",
    "    \n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(range(1, num_epochs + 1), train_losses, marker='o')\n",
    "    # plt.title('PyTorch Training Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.grid(True)\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(range(1, num_epochs + 1), test_accuracies, marker='o', color='r')\n",
    "    # plt.title('PyTorch Test Accuracy')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Accuracy (%)')\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_numpy_cnn(train_images_np, train_labels_np, test_images_np, test_labels_np,\n",
    "                      conv_forward_fn, conv_backward_fn,\n",
    "                      model_name, initial_np_weights,\n",
    "                      num_epochs=5, batch_size=64, learning_rate=0.001):\n",
    "    print(f\"\\n--- Training NumPy CNN: {model_name} ---\")\n",
    "\n",
    "    k1, bc1 = np.copy(initial_np_weights['k1']), np.copy(initial_np_weights['bc1'])\n",
    "    k2, bc2 = np.copy(initial_np_weights['k2']), np.copy(initial_np_weights['bc2'])\n",
    "    k3, bc3 = np.copy(initial_np_weights['k3']), np.copy(initial_np_weights['bc3'])\n",
    "    w1, b1_fc = np.copy(initial_np_weights['w1']), np.copy(initial_np_weights['b1'])\n",
    "    w2, b2_fc = np.copy(initial_np_weights['w2']), np.copy(initial_np_weights['b2'])\n",
    "\n",
    "    num_samples = train_images_np.shape[0]\n",
    "    epoch_losses = []\n",
    "\n",
    "    c0 = train_images_np[0].reshape(1, 1, 28, 28) // 255\n",
    "    c1, mask1 = conv_forward_fn(c0, k1, bc1, padding=0, stride=2, applyReLU=True)\n",
    "    c2, mask2 = conv_forward_fn(c1, k2, bc2, padding=1, stride=2, applyReLU=True)\n",
    "    c3, mask3 = conv_forward_fn(c2, k3, bc3, padding=0, stride=2, applyReLU=True)\n",
    "\n",
    "    input_to_mlp = c3.reshape(1, -1)\n",
    "    fl, fa, sl, sa = ReLU_SoftMax_FullyConnected(input_to_mlp, w1, b1_fc, w2, b2_fc)\n",
    "\n",
    "    initial_loss = crossEntropy(sa[0], train_labels_np[0])\n",
    "    print(f\"{initial_loss:.4f}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(range(0, num_samples, batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        num_batches_processed = 0\n",
    "\n",
    "        for i in progress_bar:\n",
    "            num_batches_processed += 1\n",
    "            batch_images = train_images[i:i+batch_size].reshape(batch_size, 1, 28, 28) // 255\n",
    "            batch_labels = train_labels[i:i+batch_size]\n",
    "            \n",
    "            c0 = batch_images.astype(np.float64)\n",
    "\n",
    "            # --- Forward Pass ---\n",
    "            c1, mask1 = conv_forward_fn(c0, k1, bc1, padding=0, stride=2, applyReLU=True)\n",
    "            c2, mask2 = conv_forward_fn(c1, k2, bc2, padding=1, stride=2, applyReLU=True)\n",
    "            c3, mask3 = conv_forward_fn(c2, k3, bc3, padding=0, stride=2, applyReLU=True)\n",
    "\n",
    "            input_to_mlp = c3.reshape(batch_size, -1)\n",
    "            fl, fa, sl, sa = ReLU_SoftMax_FullyConnected(input_to_mlp, w1, b1, w2, b2)\n",
    "            \n",
    "            batch_loss_val = 0\n",
    "\n",
    "            for j_idx in range(batch_size):\n",
    "                batch_loss_val += crossEntropy(sa[j_idx], batch_labels[j_idx])\n",
    "            batch_loss_val /= batch_size\n",
    "            current_epoch_loss += batch_loss_val\n",
    "\n",
    "            # --- Backward Pass ---\n",
    "            dL_i_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2 = ReLU_SoftMax_FC_Backward(batch_size, sa, batch_labels, w1, w2, fa, fl, input_to_mlp)\n",
    "\n",
    "            dL_dA3 = dL_i_mlp.reshape(c3.shape) \n",
    "\n",
    "            gi3, gk3, gb3 = conv_backward_fn(c2, dL_dA3, k3, mask3, padding=0, stride=2)\n",
    "            gi2, gk2, gb2 = conv_backward_fn(c1, gi3, k2, mask2, padding=1, stride=2)\n",
    "            gi1, gk1, gb1 = conv_backward_fn(c0, gi2, k1, mask1, padding=0, stride=2)\n",
    "\n",
    "            # Normalize gradients by batch size\n",
    "            dL_dw1 /= batch_size\n",
    "            dL_db1 /= batch_size\n",
    "            dL_dw2 /= batch_size\n",
    "            dL_db2 /= batch_size\n",
    "            gk1 /= batch_size\n",
    "            gb1 /= batch_size\n",
    "            gk2 /= batch_size\n",
    "            gb2 /= batch_size\n",
    "            gk3 /= batch_size\n",
    "            gb3 /= batch_size\n",
    "\n",
    "            # --- Weight Update (Simple SGD) ---\n",
    "            w1  -= learning_rate * dL_dw1\n",
    "            b1_fc  -= learning_rate * dL_db1.reshape(b1_fc.shape) # FC biases are (1, out_features)\n",
    "            w2  -= learning_rate * dL_dw2\n",
    "            b2_fc  -= learning_rate * dL_db2.reshape(b2_fc.shape) # FC biases are (1, out_features)\n",
    "            \n",
    "            k3  -= learning_rate * gk3\n",
    "            bc3 -= learning_rate * gb3\n",
    "            k2  -= learning_rate * gk2\n",
    "            bc2 -= learning_rate * gb2\n",
    "            k1  -= learning_rate * gk1\n",
    "            bc1 -= learning_rate * gb1\n",
    "            \n",
    "            # if i % 10 == 0: # Stampa ogni 10 batch\n",
    "            #     print(f\"\\nBatch {i}\")\n",
    "            #     print(f\"  Norma K1: {np.linalg.norm(k1):.4e}, Norma grad_k1: {np.linalg.norm(gk1):.4e}\")\n",
    "            #     print(f\"  Norma K2: {np.linalg.norm(k2):.4e}, Norma grad_k2: {np.linalg.norm(gk2):.4e}\")\n",
    "            #     print(f\"  Norma K3: {np.linalg.norm(k3):.4e}, Norma grad_k3: {np.linalg.norm(gk3):.4e}\")\n",
    "            #     print(f\"  Norma W1: {np.linalg.norm(w1):.4e}, Norma dL_dw1: {np.linalg.norm(dL_dw1):.4e}\")\n",
    "            #     print(f\"  Norma W2: {np.linalg.norm(w2):.4e}, Norma dL_dw2: {np.linalg.norm(dL_dw2):.4e}\")\n",
    "            #     print(f\"  Loss Batch: {batch_loss_val:.4f}\")\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{batch_loss_val:.4f}\")\n",
    "\n",
    "        avg_epoch_loss = current_epoch_loss / num_batches_processed if num_batches_processed > 0 else 0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s - Avg Training Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        test_loss_val = 0.0\n",
    "        total_test = 0\n",
    "        correct_test = 0\n",
    "\n",
    "        test_images_eval = test_images_np / 255.0 \n",
    "        test_labels_eval = test_labels_np # Assumendo one-hot\n",
    "\n",
    "        for i_test in range(0, len(test_images_eval), batch_size):\n",
    "            test_batch_images = test_images_eval[i_test : i_test+batch_size]\n",
    "            test_batch_labels = test_labels_eval[i_test : i_test+batch_size]\n",
    "\n",
    "            c0_test = test_batch_images.reshape(-1, 1, 28, 28).astype(np.float64)\n",
    "            current_test_bs = c0_test.shape[0]\n",
    "\n",
    "            # Forward pass\n",
    "            c1_test, _ = conv_forward_fn(c0_test, k1, bc1, padding=0, stride=2, applyReLU=True)\n",
    "            c2_test, _ = conv_forward_fn(c1_test, k2, bc2, padding=1, stride=2, applyReLU=True)\n",
    "            c3_test, _ = conv_forward_fn(c2_test, k3, bc3, padding=0, stride=2, applyReLU=True)\n",
    "            input_to_mlp_test = c3_test.reshape(current_test_bs, -1)\n",
    "            _, _, _, sa_test = ReLU_SoftMax_FC_no_bias(input_to_mlp_test, w1, w2)\n",
    "\n",
    "            # Calcolo loss vettorializzato\n",
    "            batch_test_loss_sum = np.sum([\n",
    "                crossEntropy(sa_test[j], test_batch_labels[j])\n",
    "                for j in range(current_test_bs)\n",
    "            ])\n",
    "            test_loss_val += batch_test_loss_sum\n",
    "\n",
    "            # Calcolo accuracy vettorializzato\n",
    "            predicted_test = np.argmax(sa_test, axis=1)\n",
    "            true_labels_test_indices = np.argmax(test_batch_labels, axis=1)\n",
    "            total_test += current_test_bs\n",
    "            correct_test += np.sum(predicted_test == true_labels_test_indices)\n",
    "\n",
    "        avg_test_loss = test_loss_val / total_test if total_test > 0 else 0\n",
    "        accuracy_test = 100 * correct_test / total_test if total_test > 0 else 0\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy_test:.2f}%\")\n",
    "\n",
    "    print(f\"{model_name} NumPy Training Complete.\")\n",
    "\n",
    "    # plt.figure(figsize=(6, 4))\n",
    "    # plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "    # plt.title(f'{model_name} NumPy Training Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    trained_weights = {'k1': k1, 'k2': k2, 'k3': k3, 'w1': w1, 'w2': w2}\n",
    "    return trained_weights\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    # Use a subset for faster demonstration\n",
    "    num_train_samples = 1280 # Approx 20 batches of 64, or 10 batches of 128\n",
    "    num_test_samples = 200\n",
    "    \n",
    "    train_images_subset = train_images[:num_train_samples]\n",
    "    train_labels_subset = train_labels[:num_train_samples]\n",
    "    test_images_subset = test_images[:num_test_samples]\n",
    "    test_labels_subset = test_labels[:num_test_samples]\n",
    "\n",
    "    EPOCHS = 3\n",
    "    BATCH_SIZE_PYTORCH = 1\n",
    "    BATCH_SIZE_NUMPY = 1 \n",
    "    LEARNING_RATE = 0.01\n",
    "\n",
    "    shapes_source = {}\n",
    "    temp_pytorch_model_for_shapes = SimpleCNN(num_classes=10)\n",
    "    shapes_source['k1'] = temp_pytorch_model_for_shapes.conv1.weight.data.numpy()\n",
    "    shapes_source['b_conv1'] = temp_pytorch_model_for_shapes.conv1.bias.data.numpy()\n",
    "    shapes_source['k2'] = temp_pytorch_model_for_shapes.conv2.weight.data.numpy()\n",
    "    shapes_source['b_conv2'] = temp_pytorch_model_for_shapes.conv2.bias.data.numpy()\n",
    "    shapes_source['k3'] = temp_pytorch_model_for_shapes.conv3.weight.data.numpy()\n",
    "    shapes_source['b_conv3'] = temp_pytorch_model_for_shapes.conv3.bias.data.numpy()\n",
    "    shapes_source['w1'] = temp_pytorch_model_for_shapes.fc1.weight.data.T.numpy() # Già trasposta\n",
    "    shapes_source['b1'] = temp_pytorch_model_for_shapes.fc1.bias.data.numpy().reshape(1, -1)\n",
    "    shapes_source['w2'] = temp_pytorch_model_for_shapes.fc2.weight.data.T.numpy() # Già trasposta\n",
    "    shapes_source['b2'] = temp_pytorch_model_for_shapes.fc2.bias.data.numpy().reshape(1, -1)\n",
    "\n",
    "    # 2. Inizializzazione Pesi (Identica)\n",
    "    np_weights = initialize_weights()\n",
    "\n",
    "    # 1. Train PyTorch CNN\n",
    "    pytorch_model = SimpleCNN(num_classes=10)\n",
    "\n",
    "    # Copia i pesi da NumPy a PyTorch\n",
    "    with torch.no_grad():\n",
    "        pytorch_model.conv1.weight.data = torch.from_numpy(np_weights['k1'])\n",
    "        pytorch_model.conv1.bias.data = torch.from_numpy(np_weights['bc1'])\n",
    "        pytorch_model.conv2.weight.data = torch.from_numpy(np_weights['k2'])\n",
    "        pytorch_model.conv2.bias.data = torch.from_numpy(np_weights['bc2'])\n",
    "        pytorch_model.conv3.weight.data = torch.from_numpy(np_weights['k3'])\n",
    "        pytorch_model.conv3.bias.data = torch.from_numpy(np_weights['bc3'])\n",
    "        pytorch_model.fc1.weight.data = torch.from_numpy(np_weights['w1'].T) # PyTorch si aspetta (out, in)\n",
    "        pytorch_model.fc1.bias.data = torch.from_numpy(np_weights['b1'].squeeze())\n",
    "        pytorch_model.fc2.weight.data = torch.from_numpy(np_weights['w2'].T) # PyTorch si aspetta (out, in)\n",
    "        pytorch_model.fc2.bias.data = torch.from_numpy(np_weights['b2'].squeeze())\n",
    "\n",
    "    # Device per PyTorch\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pytorch_model.to(device)\n",
    "\n",
    "    trained_pytorch_model = train_pytorch_cnn(\n",
    "        train_images_subset, train_labels_subset,\n",
    "        test_images_subset, test_labels_subset,\n",
    "        num_epochs=EPOCHS, batch_size=BATCH_SIZE_PYTORCH, learning_rate=LEARNING_RATE\n",
    "    )\n",
    "\n",
    "    # 2. Train Nested-Loops NumPy CNN (Can be very slow)\n",
    "    ########## WARNING: Nested Loops NumPy training can be extremely slow!!!\n",
    "    # trained_weights_nested_loops = train_numpy_cnn(\n",
    "    #     train_images_subset, train_labels_subset,\n",
    "    #     conv_forward_fn=nested_loop_convolution,\n",
    "    #     conv_backward_fn=nested_loop_gradient,\n",
    "    #     model_name=\"Nested Loops\",\n",
    "    #     initial_np_weights=initialize_weights(),\n",
    "    #     num_epochs=EPOCHS, batch_size=BATCH_SIZE_NUMPY, learning_rate=LEARNING_RATE\n",
    "    # )\n",
    "\n",
    "    # 3. Train Im2Col NumPy CNN\n",
    "    # trained_weights_im2col = train_numpy_cnn(\n",
    "    #     train_images_subset, train_labels_subset, test_images_subset, test_labels_subset,\n",
    "    #     conv_forward_fn=im2col_convolution,\n",
    "    #     conv_backward_fn=im2col_gradient,\n",
    "    #     model_name=\"Im2Col\",\n",
    "    #     initial_np_weights=initialize_weights(),\n",
    "    #     num_epochs=EPOCHS, batch_size=BATCH_SIZE_NUMPY, learning_rate=LEARNING_RATE\n",
    "    # )\n",
    "\n",
    "    # 4. Train Im2Col Optimized NumPy CNN\n",
    "    # trained_weights_im2col_opt = train_numpy_cnn(\n",
    "    #     train_images_subset, train_labels_subset, test_images_subset, test_labels_subset,\n",
    "    #     conv_forward_fn=im2col_optimized,\n",
    "    #     conv_backward_fn=im2col_gradient_optimized,\n",
    "    #     model_name=\"Im2Col Optimized\",\n",
    "    #     initial_np_weights=initialize_weights(),\n",
    "    #     num_epochs=EPOCHS, batch_size=BATCH_SIZE_NUMPY, learning_rate=LEARNING_RATE\n",
    "    # )\n",
    "\n",
    "    # 5. Train Im2Col Optimized Float64 NumPy CNN\n",
    "    trained_weights_im2col_opt_float64 = train_numpy_cnn(\n",
    "        train_images_subset, train_labels_subset, test_images_subset, test_labels_subset,\n",
    "        conv_forward_fn=im2col_optimized,\n",
    "        conv_backward_fn=im2col_gradient_optimized,\n",
    "        model_name=\"Im2Col Optimized Float64 w/o Bias\",\n",
    "        initial_np_weights=np_weights,\n",
    "        num_epochs=EPOCHS, batch_size=BATCH_SIZE_NUMPY, learning_rate=LEARNING_RATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf4ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFRONTO DETTAGLIATO NUMPY vs PYTORCH (CON BIAS, SINGOLO STEP + AGGIORNAMENTO PESI) ---\n",
      "\n",
      "--- ESECUZIONE FORWARD PASS ---\n",
      "Loss NumPy: 18.42068203, Loss PyTorch: 11281405.71527064\n",
      "ATTENZIONE: LOSS INIZIALI DIVERSE, IL CONFRONTO DEGLI AGGIORNAMENTI POTREBBE NON ESSERE SIGNIFICATIVO\n",
      "\n",
      "--- ESECUZIONE BACKWARD PASS ---\n",
      "\n",
      "--- CONFRONTO GRADIENTI DEI PARAMETRI (PRIMA DELL'AGGIORNAMENTO) ---\n",
      "\n",
      "Confronto Gradiente per k1:\n",
      "  Shape NumPy: (32, 1, 2, 2), Shape PyTorch: (32, 1, 2, 2)\n",
      "  NumPy Sum: 8.826419e+06, PyTorch Sum: 8.826419e+06\n",
      "  Gradiente k1: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per bc1:\n",
      "  Shape NumPy: (32,), Shape PyTorch: (32,)\n",
      "  NumPy Sum: 1.490863e+07, PyTorch Sum: 1.490863e+07\n",
      "  Gradiente bc1: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per k2:\n",
      "  Shape NumPy: (64, 32, 2, 2), Shape PyTorch: (64, 32, 2, 2)\n",
      "  NumPy Sum: 2.236263e+07, PyTorch Sum: 2.236263e+07\n",
      "  Gradiente k2: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per bc2:\n",
      "  Shape NumPy: (64,), Shape PyTorch: (64,)\n",
      "  NumPy Sum: 3.042440e+05, PyTorch Sum: 3.042440e+05\n",
      "  Gradiente bc2: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per k3:\n",
      "  Shape NumPy: (128, 64, 2, 2), Shape PyTorch: (128, 64, 2, 2)\n",
      "  NumPy Sum: 2.252955e+07, PyTorch Sum: 2.252955e+07\n",
      "  Gradiente k3: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per bc3:\n",
      "  Shape NumPy: (128,), Shape PyTorch: (128,)\n",
      "  NumPy Sum: 2.371693e+03, PyTorch Sum: 2.371693e+03\n",
      "  Gradiente bc3: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per w1:\n",
      "  Shape NumPy: (2048, 250), Shape PyTorch: (2048, 250)\n",
      "  NumPy Sum: 2.211461e+07, PyTorch Sum: 2.211461e+07\n",
      "  Gradiente w1: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per b1:\n",
      "  Shape NumPy: (250,), Shape PyTorch: (250,)\n",
      "  NumPy Sum: 2.357506e+00, PyTorch Sum: 2.357506e+00\n",
      "  Gradiente b1: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per w2:\n",
      "  Shape NumPy: (250, 10), Shape PyTorch: (250, 10)\n",
      "  NumPy Sum: -5.587935e-09, PyTorch Sum: 9.313226e-10\n",
      "  Gradiente w2: CORRISPONDE\n",
      "\n",
      "Confronto Gradiente per b2:\n",
      "  Shape NumPy: (10,), Shape PyTorch: (10,)\n",
      "  NumPy Sum: 0.000000e+00, PyTorch Sum: 0.000000e+00\n",
      "  Gradiente b2: CORRISPONDE\n",
      "\n",
      "--- AGGIORNAMENTO PESI ---\n",
      "\n",
      "--- CONFRONTO PESI AGGIORNATI ---\n",
      "\n",
      "Confronto Peso Aggiornato per k1:\n",
      "  Shape NumPy: (32, 1, 2, 2), Shape PyTorch: (32, 1, 2, 2)\n",
      "  NumPy Sum: -8.820247e+04, PyTorch Sum: -8.820247e+04\n",
      "  Peso Aggiornato k1: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per bc1:\n",
      "  Shape NumPy: (32,), Shape PyTorch: (32,)\n",
      "  NumPy Sum: -1.490717e+05, PyTorch Sum: -1.490717e+05\n",
      "  Peso Aggiornato bc1: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per k2:\n",
      "  Shape NumPy: (64, 32, 2, 2), Shape PyTorch: (64, 32, 2, 2)\n",
      "  NumPy Sum: -2.195721e+05, PyTorch Sum: -2.195721e+05\n",
      "  Peso Aggiornato k2: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per bc2:\n",
      "  Shape NumPy: (64,), Shape PyTorch: (64,)\n",
      "  NumPy Sum: -3.012095e+03, PyTorch Sum: -3.012095e+03\n",
      "  Peso Aggiornato bc2: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per k3:\n",
      "  Shape NumPy: (128, 64, 2, 2), Shape PyTorch: (128, 64, 2, 2)\n",
      "  NumPy Sum: -2.088872e+05, PyTorch Sum: -2.088872e+05\n",
      "  Peso Aggiornato k3: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per bc3:\n",
      "  Shape NumPy: (128,), Shape PyTorch: (128,)\n",
      "  NumPy Sum: 4.144057e+01, PyTorch Sum: 4.144057e+01\n",
      "  Peso Aggiornato bc3: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per w1:\n",
      "  Shape NumPy: (2048, 250), Shape PyTorch: (2048, 250)\n",
      "  NumPy Sum: 3.504411e+04, PyTorch Sum: 3.504411e+04\n",
      "  Peso Aggiornato w1: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per b1:\n",
      "  Shape NumPy: (1, 250), Shape PyTorch: (1, 250)\n",
      "  NumPy Sum: 1.270068e+02, PyTorch Sum: 1.270068e+02\n",
      "  Peso Aggiornato b1: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per w2:\n",
      "  Shape NumPy: (250, 10), Shape PyTorch: (250, 10)\n",
      "  NumPy Sum: 1.249688e+03, PyTorch Sum: 1.249688e+03\n",
      "  Peso Aggiornato w2: CORRISPONDE\n",
      "\n",
      "Confronto Peso Aggiornato per b2:\n",
      "  Shape NumPy: (1, 10), Shape PyTorch: (1, 10)\n",
      "  NumPy Sum: 3.994466e+00, PyTorch Sum: 3.994466e+00\n",
      "  Peso Aggiornato b2: CORRISPONDE\n",
      "-------- gw2 --------\n",
      "Numpy gw2[0] (250, 10): [[      0.               0.         4657686.97998538 ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.         4807054.23501514 ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.         4717572.2663423  ...       0.\n",
      "        0.               0.        ]\n",
      " ...\n",
      " [      0.               0.         4621537.1340354  ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.         4688813.37552151 ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.         4607166.42689455 ...       0.\n",
      "        0.               0.        ]]\n",
      "PyTorch gw2[0] (10, 250): [[      0.               0.         4657686.97998538 ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.         4807054.23501514 ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.         4717572.2663423  ...       0.\n",
      "        0.               0.        ]\n",
      " ...\n",
      " [      0.               0.         4621537.1340354  ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.         4688813.37552152 ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.         4607166.42689455 ...       0.\n",
      "        0.               0.        ]]\n",
      "----- Element wise difference gw2 -----\n",
      "gw2 diff: [[ 0.00000000e+00  0.00000000e+00  3.72529030e-09 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  2.79396772e-09 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -9.31322575e-10 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -9.31322575e-10 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  3.72529030e-09 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Assicurati che le tue funzioni load_mnist_images, load_mnist_labels,\n",
    "# im2col_optimized, im2col_gradient_optimized,\n",
    "# ReLU_SoftMax_FullyConnected, ReLU_SoftMax_FC_Backward, crossEntropy\n",
    "# e la classe SimpleCNN siano definite correttamente sopra questo blocco.\n",
    "\n",
    "print(\"--- CONFRONTO DETTAGLIATO NUMPY vs PYTORCH (CON BIAS, SINGOLO STEP + AGGIORNAMENTO PESI) ---\")\n",
    "\n",
    "# 0. Parametri del Test\n",
    "LEARNING_RATE = 0.01 # Usa un LR consistente per entrambi\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(42)\n",
    "\n",
    "\n",
    "# 1. Caricamento Dati e Parametri Iniziali\n",
    "raw_train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "raw_train_labels_int = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "x_single_np = (raw_train_images[0:1] / 255.0).reshape(1, 1, 28, 28).astype(np.float64)\n",
    "y_label_int_single_np = raw_train_labels_int[0:1].astype(np.int64)\n",
    "y_label_one_hot_single_np = np.zeros((1, 10), dtype=np.float64)\n",
    "y_label_one_hot_single_np[0, y_label_int_single_np[0]] = 1.0\n",
    "\n",
    "# Inizializzazione pesi NumPy (casuali ma consistenti)\n",
    "k1_np_initial = np.random.rand(*numpy_weights['k1'].shape).astype(np.float64)\n",
    "bc1_np_initial = np.random.rand(*numpy_weights['b_conv1'].shape).astype(np.float64)\n",
    "k2_np_initial = np.random.rand(*numpy_weights['k2'].shape).astype(np.float64)\n",
    "bc2_np_initial = np.random.rand(*numpy_weights['b_conv2'].shape).astype(np.float64)\n",
    "k3_np_initial = np.random.rand(*numpy_weights['k3'].shape).astype(np.float64)\n",
    "bc3_np_initial = np.random.rand(*numpy_weights['b_conv3'].shape).astype(np.float64)\n",
    "w1_np_initial = np.random.rand(*numpy_weights['w1'].shape).astype(np.float64)\n",
    "b1_np_initial = np.random.rand(*numpy_weights['b1'].shape).astype(np.float64)\n",
    "w2_np_initial = np.random.rand(*numpy_weights['w2'].shape).astype(np.float64)\n",
    "b2_np_initial = np.random.rand(*numpy_weights['b2'].shape).astype(np.float64)\n",
    "\n",
    "# Copia i pesi iniziali per l'aggiornamento NumPy\n",
    "k1_np, bc1_np = np.copy(k1_np_initial), np.copy(bc1_np_initial)\n",
    "k2_np, bc2_np = np.copy(k2_np_initial), np.copy(bc2_np_initial)\n",
    "k3_np, bc3_np = np.copy(k3_np_initial), np.copy(bc3_np_initial)\n",
    "w1_np, b1_np = np.copy(w1_np_initial), np.copy(b1_np_initial)\n",
    "w2_np, b2_np = np.copy(w2_np_initial), np.copy(b2_np_initial)\n",
    "\n",
    "# Inizializzazione modello PyTorch con gli stessi pesi\n",
    "pytorch_model_test = SimpleCNN(num_classes=10).double()\n",
    "with torch.no_grad():\n",
    "    pytorch_model_test.conv1.weight.data = torch.from_numpy(k1_np_initial).double()\n",
    "    pytorch_model_test.conv1.bias.data = torch.from_numpy(bc1_np_initial).double()\n",
    "    pytorch_model_test.conv2.weight.data = torch.from_numpy(k2_np_initial).double()\n",
    "    pytorch_model_test.conv2.bias.data = torch.from_numpy(bc2_np_initial).double()\n",
    "    pytorch_model_test.conv3.weight.data = torch.from_numpy(k3_np_initial).double()\n",
    "    pytorch_model_test.conv3.bias.data = torch.from_numpy(bc3_np_initial).double()\n",
    "    pytorch_model_test.fc1.weight.data = torch.from_numpy(w1_np_initial.T).double()\n",
    "    pytorch_model_test.fc1.bias.data = torch.from_numpy(b1_np_initial.squeeze()).double()\n",
    "    pytorch_model_test.fc2.weight.data = torch.from_numpy(w2_np_initial.T).double()\n",
    "    pytorch_model_test.fc2.bias.data = torch.from_numpy(b2_np_initial.squeeze()).double()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "pytorch_model_test.to(device)\n",
    "x_torch_single = torch.from_numpy(x_single_np).double().to(device)\n",
    "y_torch_label_int_single = torch.from_numpy(y_label_int_single_np).to(device)\n",
    "\n",
    "# --- Dizionari per output e gradienti ---\n",
    "np_outputs_fwd = {} # Rinominato per chiarezza\n",
    "pt_outputs_fwd = {}\n",
    "np_final_grads = {} # Per i gradienti finali dei parametri NumPy\n",
    "\n",
    "# --- 2. Forward Pass (NumPy e PyTorch) ---\n",
    "print(\"\\n--- ESECUZIONE FORWARD PASS ---\")\n",
    "# NumPy\n",
    "np_outputs_fwd['A0'] = x_single_np\n",
    "np_outputs_fwd['conv1_A'], mask1_np = im2col_optimized(np_outputs_fwd['A0'], k1_np, bc1_np, padding=0, stride=2, applyReLU=True)\n",
    "np_outputs_fwd['conv2_A'], mask2_np = im2col_optimized(np_outputs_fwd['conv1_A'], k2_np, bc2_np, padding=1, stride=2, applyReLU=True)\n",
    "np_outputs_fwd['conv3_A'], mask3_np = im2col_optimized(np_outputs_fwd['conv2_A'], k3_np, bc3_np, padding=0, stride=2, applyReLU=True)\n",
    "np_outputs_fwd['fc1_input'] = np_outputs_fwd['conv3_A'].reshape(x_single_np.shape[0], -1)\n",
    "np_outputs_fwd['fc1_Z'], np_outputs_fwd['fc1_A'], np_outputs_fwd['fc2_Z_logits'], np_outputs_fwd['SA_probs'] = \\\n",
    "    ReLU_SoftMax_FullyConnected(np_outputs_fwd['fc1_input'], w1_np, b1_np, w2_np, b2_np)\n",
    "np_loss = crossEntropy(np_outputs_fwd['SA_probs'][0], y_label_one_hot_single_np[0])\n",
    "\n",
    "# PyTorch\n",
    "pytorch_model_test.train()\n",
    "# Usa SGD per un confronto di aggiornamento più semplice\n",
    "optimizer_pytorch = optim.SGD(pytorch_model_test.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_pytorch.zero_grad()\n",
    "\n",
    "logits_pytorch = pytorch_model_test(x_torch_single)\n",
    "pt_outputs_fwd['fc2_Z_logits'] = logits_pytorch.detach().cpu().numpy() # Salva per confronto\n",
    "pt_outputs_fwd['SA_probs'] = torch.softmax(logits_pytorch, dim=-1).detach().cpu().numpy() # Salva per confronto\n",
    "\n",
    "criterion_pytorch = nn.CrossEntropyLoss()\n",
    "pt_loss = criterion_pytorch(logits_pytorch, y_torch_label_int_single)\n",
    "\n",
    "# (Opzionale) Breve confronto forward e loss per assicurarsi che siano allineati prima del backward\n",
    "print(f\"Loss NumPy: {np_loss:.8f}, Loss PyTorch: {pt_loss.item():.8f}\")\n",
    "if not np.allclose(np_loss, pt_loss.item(), atol=1e-5, rtol=1e-4):\n",
    "    print(\"ATTENZIONE: LOSS INIZIALI DIVERSE, IL CONFRONTO DEGLI AGGIORNAMENTI POTREBBE NON ESSERE SIGNIFICATIVO\")\n",
    "    # Potresti voler confrontare gli output intermedi qui se la loss è diversa\n",
    "\n",
    "# --- 3. Backward Pass (NumPy e PyTorch) ---\n",
    "print(\"\\n--- ESECUZIONE BACKWARD PASS ---\")\n",
    "# PyTorch (calcola .grad)\n",
    "pt_loss.backward()\n",
    "\n",
    "# NumPy (calcola gradienti)\n",
    "bs = x_single_np.shape[0]\n",
    "dL_i_mlp, dL_dW1_np, dL_db1_np, dL_dW2_np, dL_db2_np = ReLU_SoftMax_FC_Backward(\n",
    "    bs, np_outputs_fwd['SA_probs'], y_label_one_hot_single_np,\n",
    "    w1_np, w2_np,\n",
    "    np_outputs_fwd['fc1_A'], np_outputs_fwd['fc1_Z'], np_outputs_fwd['fc1_input']\n",
    ")\n",
    "dL_dA3_np = dL_i_mlp.reshape(np_outputs_fwd['conv3_A'].shape)\n",
    "\n",
    "# Assicurati che i gradienti MLP siano normalizzati per batch size\n",
    "# (se la tua ReLU_SoftMax_FC_Backward non lo fa già)\n",
    "dL_dW1_np /= bs\n",
    "dL_db1_np /= bs # Assumendo che dL_db1_np sia la somma, quindi dividi per bs per la media\n",
    "dL_dW2_np /= bs\n",
    "dL_db2_np /= bs\n",
    "\n",
    "np_final_grads['w1'] = dL_dW1_np\n",
    "np_final_grads['b1'] = dL_db1_np\n",
    "np_final_grads['w2'] = dL_dW2_np\n",
    "np_final_grads['b2'] = dL_db2_np\n",
    "\n",
    "dL_dA2_np, dL_dK3_np, dL_dbc3_np = im2col_gradient_optimized(\n",
    "    np_outputs_fwd['conv2_A'], dL_dA3_np, k3_np, mask3_np, padding=0, stride=2\n",
    ")\n",
    "dL_dA1_np, dL_dK2_np, dL_dbc2_np = im2col_gradient_optimized(\n",
    "    np_outputs_fwd['conv1_A'], dL_dA2_np, k2_np, mask2_np, padding=1, stride=2\n",
    ")\n",
    "_, dL_dK1_np, dL_dbc1_np = im2col_gradient_optimized( # dL_dX0_np non ci serve per l'aggiornamento\n",
    "    np_outputs_fwd['A0'], dL_dA1_np, k1_np, mask1_np, padding=0, stride=2\n",
    ")\n",
    "\n",
    "# Assicurati che i gradienti convoluzionali siano normalizzati per batch size\n",
    "# (se la tua im2col_gradient_optimized non lo fa già)\n",
    "dL_dK1_np /= bs; dL_dbc1_np /= bs\n",
    "dL_dK2_np /= bs; dL_dbc2_np /= bs\n",
    "dL_dK3_np /= bs; dL_dbc3_np /= bs\n",
    "\n",
    "np_final_grads['k1'] = dL_dK1_np\n",
    "np_final_grads['bc1'] = dL_dbc1_np\n",
    "np_final_grads['k2'] = dL_dK2_np\n",
    "np_final_grads['bc2'] = dL_dbc2_np\n",
    "np_final_grads['k3'] = dL_dK3_np\n",
    "np_final_grads['bc3'] = dL_dbc3_np\n",
    "\n",
    "# --- 4. Confronto Gradienti dei Parametri (come prima) ---\n",
    "print(\"\\n--- CONFRONTO GRADIENTI DEI PARAMETRI (PRIMA DELL'AGGIORNAMENTO) ---\")\n",
    "atol_grad = 1e-12\n",
    "rtol_grad = 1e-12\n",
    "all_grads_match = True\n",
    "\n",
    "params_to_compare = [\n",
    "    ('k1', pytorch_model_test.conv1.weight.grad.detach().numpy()), ('bc1', pytorch_model_test.conv1.bias.grad.detach().numpy()),\n",
    "    ('k2', pytorch_model_test.conv2.weight.grad.detach().numpy()), ('bc2', pytorch_model_test.conv2.bias.grad.detach().numpy()),\n",
    "    ('k3', pytorch_model_test.conv3.weight.grad.detach().numpy()), ('bc3', pytorch_model_test.conv3.bias.grad.detach().numpy()),\n",
    "    ('w1', pytorch_model_test.fc1.weight.grad.detach().numpy()),   ('b1', pytorch_model_test.fc1.bias.grad.detach().numpy()),\n",
    "    ('w2', pytorch_model_test.fc2.weight.grad.detach().numpy()),   ('b2', pytorch_model_test.fc2.bias.grad.detach().numpy()),\n",
    "]\n",
    "\n",
    "for name, pt_grad_tensor_with_grad_fn in params_to_compare:\n",
    "    np_grad_val = np_final_grads.get(name)\n",
    "    pt_grad_val = pt_grad_tensor_with_grad_fn # Ora è il .grad del tensore del parametro\n",
    "\n",
    "    if name.startswith('w'): pt_grad_val = pt_grad_val.T # PyTorch si aspetta (out, in) per i pesi FC\n",
    "    if name.startswith('b') and name not in ['bc1', 'bc2', 'bc3']:\n",
    "         np_grad_val = np_grad_val.squeeze() # NumPy FC bias era (1,N)\n",
    "\n",
    "    print(f\"\\nConfronto Gradiente per {name}:\")\n",
    "    if np_grad_val is not None and pt_grad_val is not None:\n",
    "        print(f\"  Shape NumPy: {np_grad_val.shape}, Shape PyTorch: {pt_grad_val.shape}\")\n",
    "        print(f\"  NumPy Sum: {np.sum(np_grad_val):.6e}, PyTorch Sum: {np.sum(pt_grad_val):.6e}\")\n",
    "        if np.allclose(np_grad_val, pt_grad_val, atol=atol_grad, rtol=rtol_grad):\n",
    "            print(f\"  Gradiente {name}: CORRISPONDE\")\n",
    "        else:\n",
    "            all_grads_match = False\n",
    "            print(f\"  ATTENZIONE: Gradiente {name} DIVERSO\")\n",
    "            norm_diff = np.linalg.norm(np_grad_val - pt_grad_val)\n",
    "            rel_err = norm_diff / (np.linalg.norm(pt_grad_val) + 1e-9)\n",
    "            print(f\"    Norma Diff: {norm_diff:.2e}, Errore Relativo: {rel_err:.2e}\")\n",
    "    else:\n",
    "        all_grads_match = False\n",
    "        print(f\"  Gradiente {name} non trovato\")\n",
    "\n",
    "if not all_grads_match:\n",
    "    print(\"\\nATTENZIONE: NON TUTTI I GRADIENTI CORRISPONDONO. IL CONFRONTO DEGLI AGGIORNAMENTI POTREBBE NON ESSERE VALIDO.\")\n",
    "\n",
    "# --- 5. Aggiornamento Pesi ---\n",
    "print(\"\\n--- AGGIORNAMENTO PESI ---\")\n",
    "# NumPy (aggiornamento manuale SGD)\n",
    "k1_np -= LEARNING_RATE * np_final_grads['k1']\n",
    "bc1_np -= LEARNING_RATE * np_final_grads['bc1']\n",
    "k2_np -= LEARNING_RATE * np_final_grads['k2']\n",
    "bc2_np -= LEARNING_RATE * np_final_grads['bc2']\n",
    "k3_np -= LEARNING_RATE * np_final_grads['k3']\n",
    "bc3_np -= LEARNING_RATE * np_final_grads['bc3']\n",
    "w1_np -= LEARNING_RATE * np_final_grads['w1']\n",
    "b1_np -= LEARNING_RATE * np_final_grads['b1'].reshape(b1_np_initial.shape) # Assicura forma (1,N)\n",
    "w2_np -= LEARNING_RATE * np_final_grads['w2']\n",
    "b2_np -= LEARNING_RATE * np_final_grads['b2'].reshape(b2_np_initial.shape) # Assicura forma (1,N)\n",
    "\n",
    "\n",
    "# PyTorch (step dell'optimizer SGD)\n",
    "optimizer_pytorch.step()\n",
    "\n",
    "# --- 6. Confronto Pesi Aggiornati ---\n",
    "print(\"\\n--- CONFRONTO PESI AGGIORNATI ---\")\n",
    "atol_weights = 1e-10\n",
    "rtol_weights = 1e-4\n",
    "\n",
    "updated_weights_to_compare = [\n",
    "    ('k1', k1_np, pytorch_model_test.conv1.weight.data.detach().numpy()), ('bc1', bc1_np, pytorch_model_test.conv1.bias.data.detach().numpy()),\n",
    "    ('k2', k2_np, pytorch_model_test.conv2.weight.data.detach().numpy()), ('bc2', bc2_np, pytorch_model_test.conv2.bias.data.detach().numpy()),\n",
    "    ('k3', k3_np, pytorch_model_test.conv3.weight.data.detach().numpy()), ('bc3', bc3_np, pytorch_model_test.conv3.bias.data.detach().numpy()),\n",
    "    ('w1', w1_np, pytorch_model_test.fc1.weight.data.detach().numpy()),   ('b1', b1_np, pytorch_model_test.fc1.bias.data.detach().numpy()),\n",
    "    ('w2', w2_np, pytorch_model_test.fc2.weight.data.detach().numpy()),   ('b2', b2_np, pytorch_model_test.fc2.bias.data.detach().numpy()),\n",
    "]\n",
    "\n",
    "for name, np_w_updated, pt_w_tensor_updated in updated_weights_to_compare:\n",
    "    pt_w_updated = pt_w_tensor_updated\n",
    "    if name.startswith('w'): pt_w_updated = pt_w_updated.T\n",
    "    if name.startswith('b') and name not in ['bc1', 'bc2', 'bc3']:\n",
    "        pt_w_updated = pt_w_updated.reshape(1, -1) # Rendi pt_w_updated (1,N) per coerenza con np\n",
    "\n",
    "    print(f\"\\nConfronto Peso Aggiornato per {name}:\")\n",
    "    print(f\"  Shape NumPy: {np_w_updated.shape}, Shape PyTorch: {pt_w_updated.shape}\")\n",
    "    print(f\"  NumPy Sum: {np.sum(np_w_updated):.6e}, PyTorch Sum: {np.sum(pt_w_updated):.6e}\")\n",
    "    if np.allclose(np_w_updated, pt_w_updated, atol=atol_weights, rtol=rtol_weights):\n",
    "        print(f\"  Peso Aggiornato {name}: CORRISPONDE\")\n",
    "    else:\n",
    "        print(f\"  ATTENZIONE: Peso Aggiornato {name} DIVERSO\")\n",
    "        norm_diff_w = np.linalg.norm(np_w_updated - pt_w_updated)\n",
    "        rel_err_w = norm_diff_w / (np.linalg.norm(pt_w_updated) + 1e-9)\n",
    "        print(f\"    Norma Diff: {norm_diff_w:.2e}, Errore Relativo: {rel_err_w:.2e}\")\n",
    "        # Stampa slice solo per debug se necessario\n",
    "        # if rel_err_w > 1e-3:\n",
    "        #     print(f\"    NumPy Slice (updated): {np_w_updated.ravel()[:min(5, np_w_updated.size)]}\")\n",
    "        #     print(f\"    PyTorch Slice (updated): {pt_w_updated.ravel()[:min(5, pt_w_updated.size)]}\")\n",
    "\n",
    "# print(\"-------- k1 --------\")\n",
    "# print(f\"Numpy k1[0]: {k1_np[:, 0, 0, 0]}\")\n",
    "# print(f\"PyTorch k1[0]: {updated_weights_to_compare[0][2][:, 0, 0, 0]}\")\n",
    "\n",
    "# print(\"-------- gw2 --------\")\n",
    "# print(f\"Numpy gw2[0] {np_final_grads['w2'].shape}: {np_final_grads['w2']}\")\n",
    "# print(f\"PyTorch gw2[0] {pytorch_model_test.fc2.weight.grad.detach().numpy().shape}: {pytorch_model_test.fc2.weight.grad.detach().numpy().T}\")\n",
    "print('----- Element wise difference gw2 -----')\n",
    "print(f\"gw2 diff: {np_final_grads['w2'] - pytorch_model_test.fc2.weight.grad.detach().numpy().T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c2b17",
   "metadata": {},
   "source": [
    "## Gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8bba0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3de3fcf6",
   "metadata": {},
   "source": [
    "#### Comparing optimized versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a0430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TRAINING TIME SUMMARY ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time (Im2Col Optimized): 2.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time (Im2Col Optimized SGEMM): 3.75s\n",
      "\n",
      "========== INFERENCE COMPARISON ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time/sample (Im2Col Optimized): 0.000824s (Accuracy: 14.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time/sample (Im2Col Optimized SGEMM): 0.001469s (Accuracy: 14.00%)\n",
      "\n",
      "Descriptive Statistics of Inference Times per sample (seconds):\n",
      "       Im2Col Optimized  Im2Col Optimized SGEMM\n",
      "count        100.000000              100.000000\n",
      "mean           0.000824                0.001469\n",
      "std            0.002739                0.003941\n",
      "min            0.000000                0.000000\n",
      "25%            0.000000                0.000000\n",
      "50%            0.000000                0.000999\n",
      "75%            0.000936                0.001119\n",
      "max            0.015925                0.033655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "import os\n",
    "import pandas as pd # Per una migliore visualizzazione delle statistiche\n",
    "\n",
    "# --- Funzione di Training Generica per NumPy CNN ---\n",
    "def train_numpy_cnn_generic(train_images_np, train_labels_np,\n",
    "                            conv_forward_fn, conv_backward_fn,\n",
    "                            model_name, initial_np_weights,\n",
    "                            num_epochs=3, batch_size=32, learning_rate=0.001):\n",
    "    # print(f\"\\n--- Training NumPy CNN: {model_name} ---\")\n",
    "\n",
    "    k1, b_conv1 = np.copy(initial_np_weights['k1']), np.copy(initial_np_weights['b_conv1'])\n",
    "    k2, b_conv2 = np.copy(initial_np_weights['k2']), np.copy(initial_np_weights['b_conv2'])\n",
    "    k3, b_conv3 = np.copy(initial_np_weights['k3']), np.copy(initial_np_weights['b_conv3'])\n",
    "    w1_fc, b1_fc = np.copy(initial_np_weights['w1']), np.copy(initial_np_weights['b1'])\n",
    "    w2_fc, b2_fc = np.copy(initial_np_weights['w2']), np.copy(initial_np_weights['b2'])\n",
    "\n",
    "    num_samples = train_images_np.shape[0]\n",
    "    epoch_losses = []\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_loss = 0.0\n",
    "        \n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        shuffled_images = train_images_np[permutation] / 255.0\n",
    "        shuffled_labels = train_labels_np[permutation]\n",
    "\n",
    "        progress_bar = tqdm(range(0, num_samples, batch_size), desc=f\"Epoch {epoch+1}/{num_epochs} ({model_name})\", leave=False)\n",
    "        \n",
    "        num_batches_processed = 0\n",
    "        for i in progress_bar:\n",
    "            num_batches_processed += 1\n",
    "            batch_images = shuffled_images[i:i+batch_size]\n",
    "            batch_labels = shuffled_labels[i:i+batch_size]\n",
    "            \n",
    "            c0 = batch_images.reshape(-1, 1, 28, 28).astype(np.float32)\n",
    "            current_bs = c0.shape[0]\n",
    "\n",
    "            # --- Forward Pass ---\n",
    "            c1, mask1 = conv_forward_fn(c0, k1, b_conv1, padding=0, stride=2, applyReLU=True)\n",
    "            c2, mask2 = conv_forward_fn(c1, k2, b_conv2, padding=1, stride=2, applyReLU=True)\n",
    "            c3, mask3 = conv_forward_fn(c2, k3, b_conv3, padding=0, stride=2, applyReLU=True)\n",
    "            \n",
    "            input_to_mlp = c3.reshape(current_bs, -1)\n",
    "            fl, fa, sl, sa = ReLU_SoftMax_FullyConnected(input_to_mlp, w1_fc, b1_fc, w2_fc, b2_fc)\n",
    "            \n",
    "            batch_loss_val = crossEntropy(sa, batch_labels) / current_bs # Average loss for the batch\n",
    "            current_epoch_loss += batch_loss_val\n",
    "\n",
    "            # --- Backward Pass ---\n",
    "            dL_i_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2 = \\\n",
    "                ReLU_SoftMax_FC_Backward(current_bs, sa, batch_labels, w1_fc, w2_fc, fa, fl, input_to_mlp)\n",
    "            dL_dA3 = dL_i_mlp.reshape(c3.shape) \n",
    "            \n",
    "            gi3, gk3, gb3 = conv_backward_fn(c2, dL_dA3, k3, mask3, padding=0, stride=2)\n",
    "            gi2, gk2, gb2 = conv_backward_fn(c1, gi3, k2, mask2, padding=1, stride=2)\n",
    "            gi1, gk1, gb1 = conv_backward_fn(c0, gi2, k1, mask1, padding=0, stride=2)\n",
    "            \n",
    "            # Normalize gradients by batch size\n",
    "            dL_dw1 /= current_bs; dL_db1 /= current_bs\n",
    "            dL_dw2 /= current_bs; dL_db2 /= current_bs\n",
    "            gk1 /= current_bs; gb1 /= current_bs\n",
    "            gk2 /= current_bs; gb2 /= current_bs\n",
    "            gk3 /= current_bs; gb3 /= current_bs\n",
    "            \n",
    "            # --- Weight Update ---\n",
    "            w1_fc  -= learning_rate * dL_dw1\n",
    "            b1_fc  -= learning_rate * dL_db1.reshape(b1_fc.shape)\n",
    "            w2_fc  -= learning_rate * dL_dw2\n",
    "            b2_fc  -= learning_rate * dL_db2.reshape(b2_fc.shape)\n",
    "            k3  -= learning_rate * gk3\n",
    "            b_conv3 -= learning_rate * gb3.reshape(b_conv3.shape)\n",
    "            k2  -= learning_rate * gk2\n",
    "            b_conv2 -= learning_rate * gb2.reshape(b_conv2.shape)\n",
    "            k1  -= learning_rate * gk1\n",
    "            b_conv1 -= learning_rate * gb1.reshape(b_conv1.shape)\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{batch_loss_val:.4f}\")\n",
    "\n",
    "        avg_epoch_loss = current_epoch_loss / num_batches_processed if num_batches_processed > 0 else 0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs} ({model_name}) - Time: {epoch_time:.2f}s - Avg Training Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "    # print(f\"{model_name} NumPy Training Complete. Total time: {total_training_time:.2f}s\")\n",
    "\n",
    "    # Grafico della loss\n",
    "    # plt.figure(figsize=(6, 4))\n",
    "    # plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "    # plt.title(f'{model_name} NumPy Training Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    trained_weights = {'k1': k1, 'b_conv1': b_conv1, 'k2': k2, 'b_conv2': b_conv2, 'k3': k3, 'b_conv3': b_conv3,\n",
    "                       'w1': w1_fc, 'b1': b1_fc, 'w2': w2_fc, 'b2': b2_fc}\n",
    "\n",
    "    return trained_weights, total_training_time\n",
    "\n",
    "# --- Funzione di Inferenza Generica per NumPy CNN ---\n",
    "def run_numpy_inference_generic(test_images_np, test_labels_np, weights, \n",
    "                                conv_forward_fn, model_name, num_samples_to_test=200):\n",
    "    # print(f\"\\n--- Running Inference for NumPy CNN: {model_name} ---\")\n",
    "    k1, b_conv1 = weights['k1'], weights['b_conv1']\n",
    "    k2, b_conv2 = weights['k2'], weights['b_conv2']\n",
    "    k3, b_conv3 = weights['k3'], weights['b_conv3']\n",
    "    w1_fc, b1_fc = weights['w1'], weights['b1']\n",
    "    w2_fc, b2_fc = weights['w2'], weights['b2']\n",
    "\n",
    "    inference_times = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    # Usa un subset per l'inferenza per velocizzare\n",
    "    test_images_subset = test_images_np[:num_samples_to_test] / 255.0\n",
    "    test_labels_subset = test_labels_np[:num_samples_to_test]\n",
    "\n",
    "    progress_bar = tqdm(range(test_images_subset.shape[0]), desc=f\"Inferring ({model_name})\", leave=False)\n",
    "\n",
    "    for i in progress_bar:\n",
    "        c0 = test_images_subset[i].reshape(1, 1, 28, 28).astype(np.float32)\n",
    "        true_label_idx = np.argmax(test_labels_subset[i])\n",
    "\n",
    "        start_time = time.time()\n",
    "        c1, _ = conv_forward_fn(c0, k1, b_conv1, padding=0, stride=2, applyReLU=True)\n",
    "        c2, _ = conv_forward_fn(c1, k2, b_conv2, padding=1, stride=2, applyReLU=True)\n",
    "        c3, _ = conv_forward_fn(c2, k3, b_conv3, padding=0, stride=2, applyReLU=True)\n",
    "        input_to_mlp = c3.reshape(1, -1)\n",
    "        _, _, _, sa = ReLU_SoftMax_FullyConnected(input_to_mlp, w1_fc, b1_fc, w2_fc, b2_fc)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "        predicted_idx = np.argmax(sa)\n",
    "        if predicted_idx == true_label_idx:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    accuracy = (correct_predictions / test_images_subset.shape[0]) * 100\n",
    "    # print(f\"{model_name} - Avg Inference Time: {avg_inference_time:.6f}s - Accuracy: {accuracy:.2f}% on {test_images_subset.shape[0]} samples\")\n",
    "    return avg_inference_time, inference_times, accuracy\n",
    "\n",
    "# ============================ Main Execution ============================\n",
    "# # --- Caricamento e Preprocessing Dati MNIST ---\n",
    "try:\n",
    "    raw_train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "    raw_train_labels_int = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "    raw_test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "    raw_test_labels_int = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "except FileNotFoundError:\n",
    "    print(\"MNIST data files not found. Please download them and place in 'MNIST/' directory.\")\n",
    "    exit()\n",
    "\n",
    "train_labels_one_hot = np.zeros((raw_train_labels_int.shape[0], 10), dtype=np.float32)\n",
    "for i in range(len(raw_train_labels_int)):\n",
    "    train_labels_one_hot[i][raw_train_labels_int[i]] = 1\n",
    "test_labels_one_hot = np.zeros((raw_test_labels_int.shape[0], 10), dtype=np.float32)\n",
    "for i in range(len(raw_test_labels_int)):\n",
    "    test_labels_one_hot[i][raw_test_labels_int[i]] = 1\n",
    "\n",
    "# --- Subset per test rapido ---\n",
    "NUM_TRAIN_SAMPLES = len(raw_train_images) // 100\n",
    "NUM_TEST_SAMPLES_INFERENCE = len(raw_test_images) // 100\n",
    "NUM_EPOCHS_NP = 3\n",
    "BATCH_SIZE_NP = 64\n",
    "LEARNING_RATE_NP = 0.001\n",
    "\n",
    "train_images_subset = raw_train_images[:NUM_TRAIN_SAMPLES]\n",
    "train_labels_subset = train_labels_one_hot[:NUM_TRAIN_SAMPLES]\n",
    "test_images_subset_inf = raw_test_images[:NUM_TEST_SAMPLES_INFERENCE]\n",
    "test_labels_subset_inf = test_labels_one_hot[:NUM_TEST_SAMPLES_INFERENCE]\n",
    "\n",
    "# --- Ottieni forme dei pesi (da un modello PyTorch temporaneo) ---\n",
    "temp_model = SimpleCNN(num_classes=10)\n",
    "\n",
    "same_weights = initialize_weights()\n",
    "\n",
    "models = {\n",
    "    \"Im2Col Optimized\": {\n",
    "        \"conv_forward_fn\": im2col_optimized,\n",
    "        \"conv_backward_fn\": im2col_gradient_optimized,\n",
    "        \"weights\": same_weights\n",
    "    },\n",
    "    \"Im2Col Optimized SGEMM\": {\n",
    "        \"conv_forward_fn\": im2col_optimized_sgemm,\n",
    "        \"conv_backward_fn\": im2col_gradient_optimized_sgemm,\n",
    "        \"weights\": same_weights\n",
    "    },\n",
    "    # \"Im2Col Optimized Clean\": {\n",
    "    #     \"conv_forward_fn\": im2col_optimized_clean,\n",
    "    #     \"conv_backward_fn\": im2col_gradient_optimized_clean,\n",
    "    #     \"weights\": same_weights\n",
    "    # },\n",
    "    # \"Im2Col Cython\": {\n",
    "    #     \"conv_forward_fn\": im2col_optimized_cython,\n",
    "    #     \"conv_backward_fn\": im2col_gradient_optimized_cython,\n",
    "    # }\n",
    "}\n",
    "\n",
    "print(\"\\n========== TRAINING TIME SUMMARY ==========\")\n",
    "\n",
    "for model_name, model_fns in models.items():\n",
    "    if model_name == \"Im2Col Optimized SGEMM\":\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"14\"\n",
    "\n",
    "    trained_weights, train_time = train_numpy_cnn_generic(\n",
    "        train_images_subset, train_labels_subset,\n",
    "        conv_forward_fn=model_fns[\"conv_forward_fn\"],\n",
    "        conv_backward_fn=model_fns[\"conv_backward_fn\"],\n",
    "        model_name=model_name,\n",
    "        initial_np_weights=model_fns[\"weights\"],\n",
    "        num_epochs=NUM_EPOCHS_NP, batch_size=BATCH_SIZE_NP, learning_rate=LEARNING_RATE_NP\n",
    "    )\n",
    "    print(f\"Total Training Time ({model_name}): {train_time:.2f}s\")\n",
    "\n",
    "    model_fns[\"weights\"] = trained_weights\n",
    "\n",
    "    if model_name == \"Im2Col Optimized SGEMM\":\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "    trained_weights = None\n",
    "    train_time = None\n",
    "\n",
    "# --- Inferenza ---\n",
    "print(\"\\n========== INFERENCE COMPARISON ==========\")\n",
    "\n",
    "df_inference_times = pd.DataFrame()\n",
    "\n",
    "for model_name, model_fns in models.items():\n",
    "    if model_name == \"Im2Col Optimized SGEMM\":\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"14\"\n",
    "\n",
    "    avg_inf_time, all_inf_times, acc = run_numpy_inference_generic(\n",
    "        test_images_subset_inf, test_labels_subset_inf,\n",
    "        weights=model_fns[\"weights\"],\n",
    "        conv_forward_fn=model_fns[\"conv_forward_fn\"],\n",
    "        model_name=model_name,\n",
    "        num_samples_to_test=NUM_TEST_SAMPLES_INFERENCE\n",
    "    )\n",
    "    print(f\"Avg Inference Time/sample ({model_name}): {avg_inf_time:.6f}s (Accuracy: {acc:.2f}%)\")\n",
    "    df_inference_times[model_name] = all_inf_times\n",
    "\n",
    "    if model_name == \"Im2Col Optimized SGEMM\":\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "    avg_inf_time = None\n",
    "    all_inf_times = None\n",
    "    acc = None\n",
    "\n",
    "print(\"\\nDescriptive Statistics of Inference Times per sample (seconds):\")\n",
    "print(df_inference_times.describe())\n",
    "\n",
    "# Boxplot dei tempi di inferenza\n",
    "# plt.figure(figsize=(7, 8))\n",
    "# df_inference_times.boxplot(showfliers=False)\n",
    "# plt.title('Boxplot of Inference Times per Sample')\n",
    "# plt.ylabel('Time (s)')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# Boxplot only for Im2Col Optimized and Im2Col Optimized clean\n",
    "# plt.figure(figsize=(7, 8))\n",
    "# df_inference_times[['Im2Col Optimized', 'Im2Col Optimized Clean']].boxplot(showfliers=False)\n",
    "# plt.title('Boxplot of Inference Times for Selected Models')\n",
    "# plt.ylabel('Time (s)')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# batch_size = \n",
    "# ========== TRAINING TIME SUMMARY ==========\n",
    "                                                                                          \n",
    "# Total Training Time (Im2Col Optimized): 5.58s\n",
    "# Total Training Time (Im2Col Optimized SGEMM): 8.90s\n",
    "# Total Training Time (Im2Col Optimized Clean): 5.53s\n",
    "\n",
    "# ========== INFERENCE COMPARISON ==========\n",
    "                                                                                 \n",
    "# Avg Inference Time/sample (Im2Col Optimized): 0.000615s (Accuracy: 11.00%)\n",
    "# Avg Inference Time/sample (Im2Col Optimized SGEMM): 0.000923s (Accuracy: 17.80%)\n",
    "# Avg Inference Time/sample (Im2Col Optimized Clean): 0.000548s (Accuracy: 4.40%)\n",
    "\n",
    "# Descriptive Statistics of Inference Times per sample (seconds):\n",
    "#        Im2Col Optimized  Im2Col Optimized SGEMM  Im2Col Optimized Clean\n",
    "# count        500.000000              500.000000              500.000000\n",
    "# mean           0.000615                0.000923                0.000548\n",
    "# std            0.002119                0.002832                0.001908\n",
    "# min            0.000000                0.000000                0.000000\n",
    "# 25%            0.000000                0.000000                0.000000\n",
    "# 50%            0.000000                0.000000                0.000000\n",
    "# 75%            0.000988                0.000981                0.000948\n",
    "# max            0.016086                0.021756                0.016135\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c666e2",
   "metadata": {},
   "source": [
    "### JAX implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b0bac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_numpy_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 310\u001b[0m\n\u001b[0;32m    307\u001b[0m shapes_source[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m temp_model\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    308\u001b[0m shapes_source[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m temp_model\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 310\u001b[0m numpy_initial_weights \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_numpy_weights\u001b[49m(shapes_source)\n\u001b[0;32m    311\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    312\u001b[0m jax_initial_params \u001b[38;5;241m=\u001b[39m init_jax_weights(key, shapes_source)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initialize_numpy_weights' is not defined"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import copy\n",
    "import struct\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Queste sono necessarie per il training PyTorch e per CNNDataset\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import torch.optim as optim\n",
    "\n",
    "# --- JAX CNN Definition ---\n",
    "def init_jax_weights(key, shapes_source_dict):\n",
    "    keys = jax.random.split(key, 10)\n",
    "    params = {\n",
    "        'conv1': {'w': jax.random.normal(keys[0], shapes_source_dict['k1'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b_conv1'].shape, dtype=jnp.float32)},\n",
    "        'conv2': {'w': jax.random.normal(keys[1], shapes_source_dict['k2'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b_conv2'].shape, dtype=jnp.float32)},\n",
    "        'conv3': {'w': jax.random.normal(keys[2], shapes_source_dict['k3'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b_conv3'].shape, dtype=jnp.float32)},\n",
    "        'fc1':   {'w': jax.random.normal(keys[3], shapes_source_dict['w1'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b1'].shape[1], dtype=jnp.float32)},\n",
    "        'fc2':   {'w': jax.random.normal(keys[4], shapes_source_dict['w2'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b2'].shape[1], dtype=jnp.float32)}\n",
    "    }\n",
    "    return params\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3, 4, 5, 6)) # stride, applyReLU, dimension_numbers sono statici\n",
    "def jax_conv_layer_core(x, W, b, stride, padding_config, applyReLU, dimension_numbers=('NCHW', 'OIHW', 'NCHW')):\n",
    "    out = jax.lax.conv_general_dilated(\n",
    "        x, W, window_strides=(stride, stride), padding=padding_config,\n",
    "        dimension_numbers=dimension_numbers\n",
    "    )\n",
    "    out = out + jnp.reshape(b, (1, -1, 1, 1))\n",
    "    if applyReLU:\n",
    "        return jax.nn.relu(out)\n",
    "    return out\n",
    "\n",
    "def jax_conv_layer(x, W, b, stride, padding_input, applyReLU=True, dimension_numbers=('NCHW', 'OIHW', 'NCHW')):\n",
    "    padding_config = None\n",
    "    if isinstance(padding_input, str):\n",
    "        padding_input_upper = padding_input.upper()\n",
    "        if padding_input_upper == 'VALID' or padding_input_upper == 'SAME':\n",
    "            padding_config = padding_input_upper\n",
    "        else:\n",
    "            raise ValueError(f\"Stringa padding_input deve essere 'VALID' o 'SAME'. Ricevuto: {padding_input}\")\n",
    "    elif isinstance(padding_input, int):\n",
    "        padding_config = tuple([(padding_input, padding_input), (padding_input, padding_input)]) # Restituisce una tupla\n",
    "    elif isinstance(padding_input, tuple): # Ora ci aspettiamo direttamente una tupla\n",
    "        # Potresti aggiungere controlli sulla struttura della tupla se necessario\n",
    "        padding_config = padding_input\n",
    "    else:\n",
    "        raise ValueError(f\"padding_input deve essere 'VALID', 'SAME', un intero, o una tupla di coppie. Ricevuto: {padding_input}, tipo: {type(padding_input)}\")\n",
    "    \n",
    "    out_conv = jax_conv_layer_core(x, W, b, stride, padding_config, applyReLU, dimension_numbers)\n",
    "    \n",
    "    mask = (out_conv > 0).astype(out_conv.dtype) if applyReLU else jnp.ones_like(out_conv, dtype=out_conv.dtype)\n",
    "    return out_conv, mask\n",
    "\n",
    "def jax_dense_layer(x, W, b):\n",
    "    return jnp.dot(x, W) + b\n",
    "\n",
    "def jax_cnn_forward(params, x_batch):\n",
    "    # Conv1: k=2, s=2, p=0.\n",
    "    conv1_out, _ = jax_conv_layer(x_batch, params['conv1']['w'], params['conv1']['b'], \n",
    "                                  stride=2, padding_input='VALID', applyReLU=True) # 'VALID' è una stringa, hashable\n",
    "    \n",
    "    # Conv2: k=2, s=2, p=1. Passa una TUPLA di TUPLE\n",
    "    conv2_out, _ = jax_conv_layer(conv1_out, params['conv2']['w'], params['conv2']['b'], \n",
    "                                  stride=2, padding_input=((1,1),(1,1)), applyReLU=True) # ((1,1),(1,1)) è hashable\n",
    "    \n",
    "    # Conv3: k=2, s=2, p=0.\n",
    "    conv3_out, _ = jax_conv_layer(conv2_out, params['conv3']['w'], params['conv3']['b'], \n",
    "                                  stride=2, padding_input='VALID', applyReLU=True) # 'VALID' è hashable\n",
    "    \n",
    "    flattened = conv3_out.reshape((conv3_out.shape[0], -1))\n",
    "    fc1_out = jax_dense_layer(flattened, params['fc1']['w'], params['fc1']['b'])\n",
    "    fc1_activated = jax.nn.relu(fc1_out)\n",
    "    logits = jax_dense_layer(fc1_activated, params['fc2']['w'], params['fc2']['b'])\n",
    "    return logits\n",
    "\n",
    "def jax_loss_fn(params_model, x_batch, y_batch_one_hot):\n",
    "    logits = jax_cnn_forward(params_model, x_batch)\n",
    "    loss = optax.softmax_cross_entropy(logits=logits, labels=y_batch_one_hot).mean()\n",
    "    return loss\n",
    "\n",
    "# Funzione di step di training JAX (sarà jittata)\n",
    "def jax_train_step_core(params, x_batch, y_batch_one_hot, opt_state, optimizer):\n",
    "    loss_value, grads = jax.value_and_grad(jax_loss_fn)(params, x_batch, y_batch_one_hot)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    updated_params = optax.apply_updates(params, updates)\n",
    "    return updated_params, opt_state, loss_value\n",
    "\n",
    "# Jitta la funzione di step di training\n",
    "jax_train_step_jitted = jax.jit(jax_train_step_core, static_argnames=['optimizer'])\n",
    "\n",
    "@jax.jit\n",
    "def jax_predict(params, x_batch):\n",
    "    logits = jax_cnn_forward(params, x_batch)\n",
    "    probabilities = jax.nn.softmax(logits, axis=-1)\n",
    "    return probabilities\n",
    "\n",
    "def train_jax_cnn(train_images_np, train_labels_np,\n",
    "                  model_name, initial_jax_params,\n",
    "                  num_epochs=3, batch_size=32, learning_rate=0.001):\n",
    "    print(f\"\\n--- Training JAX CNN: {model_name} ---\")\n",
    "    params = initial_jax_params\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "    num_samples = train_images_np.shape[0]\n",
    "    epoch_losses = []\n",
    "    total_training_time = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_loss = 0.0\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        shuffled_images_np = train_images_np[permutation] / 255.0\n",
    "        shuffled_labels_np = train_labels_np[permutation]\n",
    "        progress_bar = tqdm(range(0, num_samples, batch_size), desc=f\"Epoch {epoch+1}/{num_epochs} (JAX)\", leave=False)\n",
    "        num_batches_processed = 0\n",
    "        for i in progress_bar:\n",
    "            num_batches_processed += 1\n",
    "            batch_images_np = shuffled_images_np[i:i+batch_size]\n",
    "            batch_labels_one_hot_np = shuffled_labels_np[i:i+batch_size]\n",
    "            batch_images_jax = jnp.asarray(batch_images_np.reshape(-1, 1, 28, 28))\n",
    "            batch_labels_jax = jnp.asarray(batch_labels_one_hot_np)\n",
    "            params, opt_state, loss_val = jax_train_step_jitted(\n",
    "                params, batch_images_jax, batch_labels_jax, opt_state, optimizer\n",
    "            )\n",
    "            current_epoch_loss += loss_val.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss_val.item():.4f}\")\n",
    "        avg_epoch_loss = current_epoch_loss / num_batches_processed if num_batches_processed > 0 else 0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "    return params, total_training_time\n",
    "\n",
    "def run_jax_inference(test_images_np, test_labels_np, jax_weights,\n",
    "                      model_name, num_samples_to_test=200):\n",
    "    inference_times = []\n",
    "    correct_predictions = 0\n",
    "    test_images_subset_np = test_images_np[:num_samples_to_test] / 255.0\n",
    "    test_labels_subset_np = test_labels_np[:num_samples_to_test]\n",
    "    progress_bar = tqdm(range(test_images_subset_np.shape[0]), desc=f\"Inferring (JAX {model_name})\", leave=False)\n",
    "    for i in progress_bar:\n",
    "        img_np = test_images_subset_np[i].reshape(1, 1, 28, 28)\n",
    "        true_label_idx = np.argmax(test_labels_subset_np[i])\n",
    "        img_jax = jnp.asarray(img_np)\n",
    "        start_time = time.time()\n",
    "        probabilities = jax_predict(jax_weights, img_jax)\n",
    "        predicted_idx_jax = jnp.argmax(probabilities, axis=-1)\n",
    "        predicted_idx = np.array(predicted_idx_jax)[0]\n",
    "        end_time = time.time()\n",
    "        probabilities.block_until_ready()\n",
    "        inference_times.append(end_time - start_time)\n",
    "        if predicted_idx == true_label_idx:\n",
    "            correct_predictions += 1\n",
    "    avg_inference_time = np.mean(inference_times) if inference_times else 0\n",
    "    accuracy = (correct_predictions / test_images_subset_np.shape[0]) * 100 if test_images_subset_np.shape[0] > 0 else 0\n",
    "    return avg_inference_time, inference_times, accuracy\n",
    "\n",
    "# --- PyTorch Dataset e Training/Inference ---\n",
    "class CNNDataset(Dataset): # Dal tuo notebook, per PyTorch\n",
    "    def __init__(self, digits, labels, transform=None):\n",
    "        assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "        self.digits = digits\n",
    "        self.labels = labels # Per PyTorch CrossEntropyLoss, dovrebbero essere indici di classe, non one-hot\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.digits)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digits[idx]\n",
    "        label = self.labels[idx] # Restituisce l'indice di classe intero\n",
    "        digit = digit.unsqueeze(0) # (H, W) -> (1, H, W)\n",
    "        return digit, label\n",
    "\n",
    "def train_pytorch_cnn(train_images_np, train_labels_int_np, # Ora riceve etichette intere\n",
    "                      model_name,\n",
    "                      num_epochs=3, batch_size=128, learning_rate=0.001):\n",
    "    print(f\"\\n--- Training PyTorch CNN: {model_name} ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tri_tensor = torch.from_numpy(train_images_np).float() / 255.0\n",
    "    trl_tensor = torch.from_numpy(train_labels_int_np).long() # Etichette intere come LongTensor\n",
    "\n",
    "    train_dataset = CNNDataset(tri_tensor, trl_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    pytorch_model = SimpleCNN(num_classes=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_losses = []\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        pytorch_model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (PyTorch)\", leave=False)\n",
    "\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = pytorch_model(inputs)\n",
    "            loss = criterion(outputs, labels) # labels qui sono indici di classe\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "    \n",
    "    return pytorch_model, total_training_time\n",
    "\n",
    "def run_pytorch_inference(pytorch_model, test_images_np, test_labels_one_hot_np, # Riceve one-hot per confronto\n",
    "                          model_name, num_samples_to_test=200):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pytorch_model.to(device)\n",
    "    pytorch_model.eval()\n",
    "\n",
    "    inference_times = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    test_images_subset_np = test_images_np[:num_samples_to_test] / 255.0\n",
    "    test_labels_subset_np = test_labels_one_hot_np[:num_samples_to_test]\n",
    "\n",
    "    progress_bar = tqdm(range(test_images_subset_np.shape[0]), desc=f\"Inferring (PyTorch {model_name})\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in progress_bar:\n",
    "            img_np = test_images_subset_np[i].reshape(1, 1, 28, 28) # NCHW\n",
    "            true_label_idx = np.argmax(test_labels_subset_np[i])\n",
    "            \n",
    "            img_tensor = torch.from_numpy(img_np).float().to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = pytorch_model(img_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=-1)\n",
    "            predicted_idx = torch.argmax(probabilities).item()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            inference_times.append(end_time - start_time)\n",
    "            if predicted_idx == true_label_idx:\n",
    "                correct_predictions += 1\n",
    "                \n",
    "    avg_inference_time = np.mean(inference_times) if inference_times else 0\n",
    "    accuracy = (correct_predictions / test_images_subset_np.shape[0]) * 100 if test_images_subset_np.shape[0] > 0 else 0\n",
    "    return avg_inference_time, inference_times, accuracy\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Caricamento Dati MNIST ---\n",
    "    raw_train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "    raw_train_labels_int = load_mnist_labels('MNIST/train-labels-idx1-ubyte') # Etichette intere\n",
    "    raw_test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "    raw_test_labels_int = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')   # Etichette intere\n",
    "\n",
    "    # One-hot per NumPy e JAX (e per il calcolo dell'accuracy in inferenza PyTorch)\n",
    "    train_labels_one_hot = np.zeros((raw_train_labels_int.shape[0], 10), dtype=np.float32)\n",
    "    for i in range(len(raw_train_labels_int)):\n",
    "        train_labels_one_hot[i][raw_train_labels_int[i]] = 1\n",
    "    test_labels_one_hot = np.zeros((raw_test_labels_int.shape[0], 10), dtype=np.float32)\n",
    "    for i in range(len(raw_test_labels_int)):\n",
    "        test_labels_one_hot[i][raw_test_labels_int[i]] = 1\n",
    "\n",
    "    NUM_TRAIN_SAMPLES = len(raw_train_images) // 10 \n",
    "    NUM_TEST_SAMPLES_INFERENCE = len(raw_test_images) // 10\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE_NP = 32 \n",
    "    BATCH_SIZE_JAX = 128\n",
    "    BATCH_SIZE_PYTORCH = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    train_images_subset = raw_train_images[:NUM_TRAIN_SAMPLES]\n",
    "    train_labels_one_hot_subset = train_labels_one_hot[:NUM_TRAIN_SAMPLES]\n",
    "    train_labels_int_subset = raw_train_labels_int[:NUM_TRAIN_SAMPLES] # Per training PyTorch\n",
    "\n",
    "    test_images_subset_inf = raw_test_images[:NUM_TEST_SAMPLES_INFERENCE]\n",
    "    test_labels_one_hot_subset_inf = test_labels_one_hot[:NUM_TEST_SAMPLES_INFERENCE]\n",
    "\n",
    "    shapes_source = {}\n",
    "    temp_model = SimpleCNN(num_classes=10)\n",
    "    shapes_source['k1'] = temp_model.conv1.weight.data.numpy()\n",
    "    shapes_source['b_conv1'] = temp_model.conv1.bias.data.numpy()\n",
    "    shapes_source['k2'] = temp_model.conv2.weight.data.numpy()\n",
    "    shapes_source['b_conv2'] = temp_model.conv2.bias.data.numpy()\n",
    "    shapes_source['k3'] = temp_model.conv3.weight.data.numpy()\n",
    "    shapes_source['b_conv3'] = temp_model.conv3.bias.data.numpy()\n",
    "    shapes_source['w1'] = temp_model.fc1.weight.data.numpy().T\n",
    "    shapes_source['b1'] = temp_model.fc1.bias.data.numpy().reshape(1, -1)\n",
    "    shapes_source['w2'] = temp_model.fc2.weight.data.numpy().T\n",
    "    shapes_source['b2'] = temp_model.fc2.bias.data.numpy().reshape(1, -1)\n",
    "\n",
    "    numpy_initial_weights = initialize_weights()\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    jax_initial_params = init_jax_weights(key, shapes_source)\n",
    "\n",
    "    models_numpy = {\n",
    "        \"Im2Col Optimized\": {\n",
    "            \"conv_forward_fn\": im2col_optimized,\n",
    "            \"conv_backward_fn\": im2col_gradient_optimized,\n",
    "            \"weights\": copy.deepcopy(numpy_initial_weights)\n",
    "        },\n",
    "        # \"Im2Col Optimized SGEMM\": {\n",
    "        #     \"conv_forward_fn\": im2col_optimized_sgemm,\n",
    "        #     \"conv_backward_fn\": im2col_gradient_optimized_sgemm,\n",
    "        #     \"weights\": copy.deepcopy(numpy_initial_weights)\n",
    "        # },\n",
    "    }\n",
    "    \n",
    "    print(\"\\n========== TRAINING TIME SUMMARY ==========\")\n",
    "    # NumPy models training\n",
    "    for model_name, model_fns in models_numpy.items():\n",
    "        trained_np_weights, train_time_np = train_numpy_cnn_generic(\n",
    "            train_images_subset, train_labels_one_hot_subset, # NumPy usa one-hot\n",
    "            conv_forward_fn=model_fns[\"conv_forward_fn\"],\n",
    "            conv_backward_fn=model_fns[\"conv_backward_fn\"],\n",
    "            model_name=model_name,\n",
    "            initial_np_weights=model_fns[\"weights\"],\n",
    "            num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE_NP, learning_rate=LEARNING_RATE\n",
    "        )\n",
    "        print(f\"Total Training Time ({model_name}): {train_time_np:.2f}s\")\n",
    "        model_fns[\"weights\"] = trained_np_weights\n",
    "\n",
    "    # JAX model training\n",
    "    trained_jax_params, train_time_jax = train_jax_cnn(\n",
    "        train_images_subset, train_labels_one_hot_subset, # JAX usa one-hot\n",
    "        model_name=\"JAX CNN\",\n",
    "        initial_jax_params=jax_initial_params,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE_JAX,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    print(f\"Total Training Time (JAX CNN): {train_time_jax:.2f}s\")\n",
    "\n",
    "    # PyTorch model training\n",
    "    trained_pytorch_model, train_time_pytorch = train_pytorch_cnn(\n",
    "        train_images_subset, train_labels_int_subset, # PyTorch CrossEntropyLoss usa etichette intere\n",
    "        model_name=\"PyTorch CNN\",\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE_PYTORCH,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    print(f\"Total Training Time (PyTorch CNN): {train_time_pytorch:.2f}s\")\n",
    "\n",
    "\n",
    "    print(\"\\n========== INFERENCE COMPARISON ==========\")\n",
    "    df_inference_times = pd.DataFrame()\n",
    "    # NumPy models inference\n",
    "    for model_name, model_fns in models_numpy.items():\n",
    "        avg_inf_time_np, all_inf_times_np, acc_np = run_numpy_inference_generic(\n",
    "            test_images_subset_inf, test_labels_one_hot_subset_inf, # Per accuracy usa one-hot\n",
    "            weights=model_fns[\"weights\"],\n",
    "            conv_forward_fn=model_fns[\"conv_forward_fn\"],\n",
    "            model_name=model_name,\n",
    "            num_samples_to_test=NUM_TEST_SAMPLES_INFERENCE\n",
    "        )\n",
    "        print(f\"Avg Inference Time/sample ({model_name}): {avg_inf_time_np:.6f}s (Accuracy: {acc_np:.2f}%)\")\n",
    "        df_inference_times[model_name] = all_inf_times_np\n",
    "    \n",
    "    # JAX model inference\n",
    "    avg_inf_time_jax, all_inf_times_jax, acc_jax = run_jax_inference(\n",
    "        test_images_subset_inf, test_labels_one_hot_subset_inf, # Per accuracy usa one-hot\n",
    "        jax_weights=trained_jax_params,\n",
    "        model_name=\"JAX CNN\",\n",
    "        num_samples_to_test=NUM_TEST_SAMPLES_INFERENCE\n",
    "    )\n",
    "    print(f\"Avg Inference Time/sample (JAX CNN): {avg_inf_time_jax:.6f}s (Accuracy: {acc_jax:.2f}%)\")\n",
    "    df_inference_times[\"JAX CNN\"] = all_inf_times_jax\n",
    "\n",
    "    # PyTorch model inference\n",
    "    avg_inf_time_pytorch, all_inf_times_pytorch, acc_pytorch = run_pytorch_inference(\n",
    "        trained_pytorch_model,\n",
    "        test_images_subset_inf, test_labels_one_hot_subset_inf, # Per accuracy usa one-hot\n",
    "        model_name=\"PyTorch CNN\",\n",
    "        num_samples_to_test=NUM_TEST_SAMPLES_INFERENCE\n",
    "    )\n",
    "    print(f\"Avg Inference Time/sample (PyTorch CNN): {avg_inf_time_pytorch:.6f}s (Accuracy: {acc_pytorch:.2f}%)\")\n",
    "    df_inference_times[\"PyTorch CNN\"] = all_inf_times_pytorch\n",
    "\n",
    "\n",
    "    print(\"\\nDescriptive Statistics of Inference Times per sample (seconds):\")\n",
    "    print(df_inference_times.describe())\n",
    "\n",
    "    plt.figure(figsize=(7, 5)) # Aumentato per più modelli\n",
    "    df_inference_times.boxplot(showfliers=False)\n",
    "    plt.title('Boxplot of Inference Times per Sample')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259fb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
