{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106183a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05471163",
   "metadata": {},
   "source": [
    "### Dataset Loading and Preprocessing\n",
    "\n",
    "This section handles the loading and initial preparation of the MNIST dataset. MNIST contains 28x28 pixel grayscale images of handwritten digits (0-9).\n",
    "\n",
    "**Key Operations:**\n",
    "\n",
    "1.  **Data Loading (`load_mnist_images`, `load_mnist_labels`):**\n",
    "    *   These functions read the MNIST dataset from its specific binary file format.\n",
    "    *   Image data is reshaped to `(num_images, rows, cols)`.\n",
    "\n",
    "2.  **One-Hot Encoding Labels:**\n",
    "    *   For multi-class classification with a softmax output and categorical cross-entropy loss, integer labels (e.g., digit `5`) are converted into a one-hot vector format (e.g., `[0,0,0,0,0,1,0,0,0,0]` for 10 classes).\n",
    "    *   This represents the true label as a probability distribution where the correct class has a probability of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9edcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "#-------------- Data Extraction ---------------------------\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097cf66",
   "metadata": {},
   "source": [
    "## PyTorch CNN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8be5b",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is defined using PyTorch's `nn.Module` to serve as a reference and source of pre-trained weights.\n",
    "\n",
    "**Architecture (defined as `SimpleCNN` class):**\n",
    "\n",
    "1.  **Conv1 + ReLU1:** `nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 1, 28, 28)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1 = \\lfloor \\frac{(28 - 2 + 0)}{2} \\rfloor + 1 = 14$\n",
    "    *   Output: `(B, 32, 14, 14)`\n",
    "\n",
    "2.  **Conv2 + ReLU2:** `nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)`\n",
    "    *   Input: `(B, 32, 14, 14)`\n",
    "    *   Padded input dimension: $14 + 2*1 = 16$\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(16 - 2 + 0)}{2} \\rfloor + 1 = 8$\n",
    "    *   Output: `(B, 64, 8, 8)`\n",
    "\n",
    "3.  **Conv3 + ReLU3:** `nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 64, 8, 8)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(8 - 2 + 0)}{2} \\rfloor + 1 = 4$\n",
    "    *   Output: `(B, 128, 4, 4)`\n",
    "\n",
    "4.  **Flatten:** `nn.Flatten()`\n",
    "    *   Input: `(B, 128, 4, 4)`\n",
    "    *   Output: `(B, 128 * 4 * 4)` which is `(B, 2048)`\n",
    "\n",
    "5.  **FC1 + ReLU4:** `nn.Linear(in_features=2048, out_features=250)`\n",
    "    *   Input: `(B, 2048)`\n",
    "    *   Operation: $Y = XW^T + b$\n",
    "    *   Output: `(B, 250)`\n",
    "\n",
    "6.  **FC2:** `nn.Linear(in_features=250, out_features=10)` (Output layer)\n",
    "    *   Input: `(B, 250)`\n",
    "    *   Output: `(B, 10)` (logits for 10 classes)\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"cnn.png\", style=\"border-radius:20px;\", height=300>\n",
    "    <figcaption>CNN Architecture (B: Batch size)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dabdea",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff84faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1.------------------ CNN declaration -------------------\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# # 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8a955",
   "metadata": {},
   "source": [
    "### Extracting Pre-trained Weights from PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee8995",
   "metadata": {},
   "source": [
    "This section loads weights from a pre-trained PyTorch model (`simple_cnn_mnist.pth`) and converts them into NumPy arrays. These NumPy weights will be used for our custom CNN implementations to ensure consistency for inference comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e62c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 2, 2), NumPy Shape=(64, 32, 2, 2)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 2, 2), NumPy Shape=(128, 64, 2, 2)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_1 = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pytorch_weights_of_kernels_in_layer_1\n",
    "\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pytorch_weights_of_kernels_in_layer_1.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_2 = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pytorch_weights_of_kernels_in_layer_2\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pytorch_weights_of_kernels_in_layer_2.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_3 = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pytorch_weights_of_kernels_in_layer_3\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pytorch_weights_of_kernels_in_layer_3.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pytorch_fc1_layer_weights = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pytorch_fc1_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pytorch_fc1_layer_biases = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pytorch_fc1_layer_biases.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pytorch_fc1_layer_weights.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pytorch_fc1_layer_biases.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pytorch_fc2_layer_weights = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pytorch_fc2_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pytorch_fc2_layer_biases = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pytorch_fc2_layer_biases.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pytorch_fc2_layer_weights.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pytorch_fc2_layer_biases.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06822b",
   "metadata": {},
   "source": [
    "## CNN - NumPy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c17c8",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ca069",
   "metadata": {},
   "source": [
    "Zero-padding adds a border of zeros around an input image or feature map before convolution. For example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\quad \\xrightarrow{\\textcolor{lightgreen}{\\textnormal{zero padding}}} \\quad\n",
    "\\begin{bmatrix}\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 1 & 2 & 3 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 4 & 5 & 6 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 7 & 8 & 9 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It's important for:\n",
    "\n",
    "1.  **Controlling Output Spatial Dimensions:** Padding can be used to maintain or control the reduction in height/width of feature maps. The output dimension (e.g., height $O_H$) is given by:\n",
    "    $$ O_H = \\left\\lfloor \\frac{I_H - K_H + 2P_H}{S_H} \\right\\rfloor + 1 $$\n",
    "    where $I_H$ is input height, $K_H$ kernel height, $P_H$ padding on one side of height, and $S_H$ stride.\n",
    "2.  **Improving Feature Extraction at Borders:** Allows the kernel to process edge pixels more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70273b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "image_3_by_3 = np.arange(1,37).reshape(2,2,3,3)\n",
    "padded_image_3_by_3 = np.pad(image_3_by_3,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(image_3_by_3)\n",
    "print(padded_image_3_by_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00035f0",
   "metadata": {},
   "source": [
    "### Matrix Dilatation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6aa4a",
   "metadata": {},
   "source": [
    "The `dilateOne` function \"dilates\" an input matrix by inserting a single row and column of zeros between existing rows and columns along its last two spatial dimensions. This means it inserts $S-1$ zeros when the forward convolution stride $S$ was 2.\n",
    "\n",
    "**Relevance in Backpropagation for $\\frac{\\partial L}{\\partial X}$:**\n",
    "\n",
    "This dilation operation is a critical step when computing the gradient of the loss with respect to the input of a convolutional layer ($\\frac{\\partial L}{\\partial X}$), especially if the forward pass utilized a stride $S > 1$. Here is why:\n",
    "* When a forward convolution uses a stride $S > 1$, it effectively downsamples the input, resulting in an output feature map $Z$ with smaller spatial dimensions.\n",
    "* To calculate $\\frac{\\partial L}{\\partial X}$, we need to use the gradient flowing back from the subsequent layer, $\\frac{\\partial L}{\\partial Z}$ (where $Z$ is the output of the strided convolution). **Since the original input $X$ has larger spatial dimensions than $Z$, the gradient $\\frac{\\partial L}{\\partial Z}$ must be \"upsampled\" or \"spread out\" before it can be convolved with the kernel weights to produce a gradient of the correct shape for $X$.**\n",
    "\n",
    "**Dilation Step:** This upsampling is achieved by inserting $S-1$ rows and columns of zeros between the elements of $\\frac{\\partial L}{\\partial Z}$. The `dilateOne` function in this notebook performs this specific operation for a stride $S=2$ (inserting one row/column of zeros).\n",
    "\n",
    "After $\\frac{\\partial L}{\\partial Z}$ is dilated to form $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}$, it is then typically padded (with $K-1$ zeros where $K$ is the kernel dimension, adjusted for any original padding) and subsequently convolved with the 180-degree rotated (or flipped) kernel ($W_{rot180}$). This entire sequence of operations (padding the dilated output gradient and convolving it with the flipped kernel) is what yields $\\frac{\\partial L}{\\partial X}$ and is often referred to as a \"full convolution\" in this context (see \"A guide to convolution arithmetic for deep learning\" by Dumoulin and Visin, or the provided articles by Mayank Kaushik).\n",
    "\n",
    "The image below illustrates the concept of dilating an output gradient tensor. This dilation is a preparatory step before the gradient is used in further convolution operations during backpropagation for layers that had striding in their forward pass.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*luRORFyTmj9mJ7rVhzlbZA.png\" height=250, style=\"border-radius:20px;\">\n",
    "</figure>\n",
    "\n",
    "Illustrative example of dilating an output gradient tensor (like $\\frac{\\partial L}{\\partial Z}$) by inserting $S-1$ (stride minus one) zeros. For `dilateOne`, $S=2$, so one zero is inserted between elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1bd18f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilateOne(matrix):\n",
    "    indexes = np.arange(1,matrix.shape[3])\n",
    "    matrix = np.insert(matrix,indexes,0,3)\n",
    "    indexes = np.arange(-(matrix.shape[-2]-1),0)\n",
    "    matrix = np.insert(matrix,indexes,0,-2)\n",
    "    return matrix\n",
    "\n",
    "# Insert stride-1 central columns and rows of the matrix\n",
    "def dilate(tensor, stride):\n",
    "    if stride == 1:\n",
    "        return tensor\n",
    "\n",
    "    batch_size, num_channels, height, width = tensor.shape\n",
    "    \n",
    "    dilated_height = height + (height - 1) * (stride - 1)\n",
    "    dilated_width = width + (width - 1) * (stride - 1)\n",
    "    \n",
    "    dilated_tensor = np.zeros((batch_size, num_channels, dilated_height, dilated_width))\n",
    "    dilated_tensor[:, :, ::stride, ::stride] = tensor\n",
    "    return dilated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa841de1",
   "metadata": {},
   "source": [
    "### Benchmark network (debug and testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05c783ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class TesterCNN(nn.Module):\n",
    "    def __init__(self, kernels: torch.Tensor, biases: torch.Tensor = None, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernels.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(biases is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernels)\n",
    "            if biases is not None:\n",
    "                self.conv.bias.copy_(biases)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if biases is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817601c0",
   "metadata": {},
   "source": [
    "## Nested-Loops Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4351c2f",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26d187",
   "metadata": {},
   "source": [
    "`Slow_ReLU_Conv` implements a 2D convolution followed by ReLU activation using explicit nested loops. This is computationally highly inefficient.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1.  **Inputs:**\n",
    "    *   `batch_of_images`: size `(N, C_in, H_in, W_in)`.\n",
    "    *   `kernels`: size `(C_out, C_in, K_H, K_W)`.\n",
    "    *   `biases`: Per-filter biases `(C_out,)`.\n",
    "2.  **Padding & Output Size:** Input `batch_of_images` is padded to better extract information from tha borders of the images. Output dimensions $(O_H, O_W)$ are calculated using the formula in the Padding section.\n",
    "3.  **Convolution:** For each output element $(n, f, y_{out}, x_{out})$:\n",
    "    $$ \\text{Output}(n, f, y_{out}, x_{out}) = \\left( \\sum_{c=0}^{C_{in}-1} \\sum_{k_y=0}^{K_H-1} \\sum_{k_x=0}^{K_W-1} \\text{Img}_{pad}(n, c, y_{out}S + k_y, x_{out}S + k_x) \\cdot \\text{Ker}(f, c, k_y, k_x) \\right) + \\text{Bias}(f) $$\n",
    "4.  **ReLU Activation:** If `applyReLU=True`: $\\text{ActivatedOutput} = \\max(0, \\text{Output})$. A binary `mask` (1 where Output > 0, else 0) is also returned for backpropagation.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/refs/heads/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Convolution of a 4x4 image (blue) with a 2x2 kernel (green)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a51071b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------example_image--------\n",
      "[[[[1. 2. 3.]\n",
      "   [4. 5. 6.]\n",
      "   [7. 8. 9.]]]]\n",
      "-------example_kernel-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Nested-Loop approach-------\n",
      "[[[[  5.  19.]\n",
      "   [ 37.  78.]]\n",
      "\n",
      "  [[ 10.  40.]\n",
      "   [ 82. 191.]]]]\n",
      "-------PyTorch model-------\n",
      "tensor([[[[  5.,  19.],\n",
      "          [ 37.,  78.]],\n",
      "\n",
      "         [[ 10.,  40.],\n",
      "          [ 82., 191.]]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nested_loop_convolution(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        output_channels, input_channels, kernel_width, kernel_height = kernels.shape\n",
    "        kernel_channels = output_channels\n",
    "    else: # Backward case\n",
    "        input_channels, output_channels, kernel_width, kernel_height = kernels.shape\n",
    "        kernel_channels = input_channels\n",
    "\n",
    "    # biases has shape output_channels, 1, 1. It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # number of channels taken in input by the kernel 'input_channels' \n",
    "    # must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    batch_size, channels, image_height, image_width  = batch_of_images.shape\n",
    "    output_image_height = int(((image_height - kernel_height) / stride) + 1) # new image height # Padding is already added\n",
    "    output_image_width = int(((image_width - kernel_width) / stride) + 1) # new image width\n",
    "    output = np.zeros((batch_size, output_channels, output_image_height, output_image_width)).astype(np.float32) # new image\n",
    "\n",
    "    if input_channels != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({input_channels}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for single_image in range(batch_size):\n",
    "        for single_kernel_channel in range(kernel_channels):\n",
    "            for output_row_idx in range(output_image_height): # which cycles row by row of the new image\n",
    "                for output_col_idx in range(output_image_width): # which cycles column by column of the new image\n",
    "                    output_cell_accumulator = 0.0  # sum for the current output cell (accumulates the convolution result)\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(input_channels): # channels == input_channels\n",
    "                        for kernel_row_idx in range(kernel_height):\n",
    "                            # Position of the kernel over the input image: row\n",
    "                            input_row_idx = (output_row_idx * stride) + kernel_row_idx\n",
    "                            for kernel_col_idx in range(kernel_width):\n",
    "                                # Position of the kernel over the input image: column\n",
    "                                input_col_idx = (output_col_idx * stride) + kernel_col_idx\n",
    "\n",
    "                                # Check if the position is inside the image\n",
    "                                if 0 <= input_row_idx < image_height and 0 <= input_col_idx < image_width:\n",
    "                                    input_element = batch_of_images[single_image, channel, input_row_idx, input_col_idx]\n",
    "                                    kernel_element = kernels[single_kernel_channel, channel, kernel_row_idx, kernel_col_idx]\n",
    "                                    # Compute the convolution sum (to be done for each element of the sliding kernel over the image)\n",
    "                                    output_cell_accumulator += (input_element * kernel_element).astype(np.float32)\n",
    "\n",
    "                    # Assign the result to the output image\n",
    "                    output[single_image, single_kernel_channel, output_row_idx, output_col_idx] = output_cell_accumulator\n",
    "\n",
    "    if biases.all() != 0:\n",
    "        biases = biases.reshape(biases.shape[0],1,1)\n",
    "        if biases.shape[0] != output_channels:\n",
    "            raise ValueError(f\"biases dimension ({biases.shape[0]}) doesn't match kernel's number of channels ({output_channels})\")\n",
    "        output = output + biases\n",
    "    output = output.astype(np.float32)\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0, output)\n",
    "        mask = output.copy()\n",
    "        mask[mask > 0] = 1\n",
    "        return output,mask\n",
    "    else:\n",
    "        return output\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "example_image = np.arange(1,3*3+1).reshape(1,1,3,3).astype(np.float32)\n",
    "print(\"-------example_image--------\")\n",
    "print(example_image)\n",
    "\n",
    "example_kernel = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------example_kernel-------\")\n",
    "print(example_kernel)\n",
    "\n",
    "example_biases = np.array([1,2]).reshape(2,1,1)\n",
    "outpout, mask = nested_loop_convolution(example_image, example_kernel, biases=example_biases, padding=1, stride=2)\n",
    "print(\"-------Nested-Loop approach-------\")\n",
    "print(outpout)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "example_kernel = torch.from_numpy(example_kernel).float()\n",
    "example_biases = torch.from_numpy(np.array([1,2])).float()\n",
    "testerCNNmodel = TesterCNN(kernels=example_kernel,biases=example_biases, stride=2, padding=1)\n",
    "\n",
    "x = torch.from_numpy(example_image)\n",
    "y = testerCNNmodel(x)\n",
    "print(\"-------PyTorch model-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bad0a3",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff4adf",
   "metadata": {},
   "source": [
    "**Actors:**\n",
    "1. W is the kernel\n",
    "2. $\\delta$ is the gradient\n",
    "3. x is the input to the convolution layer during forward\n",
    "4. b is the bias\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Derive delta**\n",
    "\n",
    "Deriving delta with respect to ReLU activation consists in the hadamard product (element-wise product) of the gradient ($\\delta$) and the mask obtained at the forward step, that is, all the elements in the convolved image greater than zero are put to one, the rest is zero.\n",
    "$$\n",
    "\\delta^{(i)} = \\delta_{\\text{flat reshaped}} \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "- **Gradient with respect to W**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(x^{(i)}, \\delta)\n",
    "$$\n",
    "This convolution creates a matrix for every channel of input image $x^{i}$ and for every channel of output image $\\delta$, thus resulting in the correct number of channels\n",
    "\n",
    "- **Gradient w.r.t. the input \\( x \\)** (To go to the preceding layer):\n",
    "\n",
    "$$\n",
    "\\delta^{(i-1)} = \\text{Full\\_Convolution}(\\delta^{(i)}, W^{(i)})\n",
    "$$\n",
    "\n",
    "- **Gradient w.r.t the bias**\n",
    "\n",
    "Since the bias is added equally across the spatial dimensions of each output channel, the gradient is the sum of all elements in each output channel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{h,w} \\delta^{(i)}_{c,h,w}\n",
    "$$\n",
    "\n",
    "For batched inputs, sum also across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n,h,w} \\delta^{(i)}_{n,c,h,w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a1928d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (1, 1, 7, 7)\n",
      "kernel: (2, 1, 2, 2)\n",
      "d_image: (1, 2, 4, 4)\n",
      "gradient w.r.t. image: (1, 1, 7, 7)\n",
      "gradient w.r.t. kernel: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "def nested_loop_gradient(batch_of_images, d_image, kernels, mask, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution.\n",
    "    The mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image and the gradient of the kernel,\n",
    "    to backpropagate the error.\n",
    "    The gradient of the bias is also returned.\n",
    "    \"\"\" \n",
    "    ############### Gradient of Input Image ###############\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image\n",
    "    # dilated (zeros between matrix elements) by stride-1 and padded by kernel-1 in height and width.\n",
    "    # The kernel is flipped by 180 degrees and the stride is set to 1.\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, image_channels, image_height, iamge_width = batch_of_images.shape\n",
    "\n",
    "    # Backward ReLU\n",
    "    d_image = np.multiply(d_image, mask)\n",
    "\n",
    "    # Dilate the gradient of the output\n",
    "    d_image = dilate(d_image, stride)\n",
    "\n",
    "    d_image = np.pad(d_image,\n",
    "                     (\n",
    "                      (0,0),(0,0),\n",
    "                      (kernel_height-1-padding, kernel_height-1-padding),\n",
    "                      (kernel_width-1-padding, kernel_width-1-padding)\n",
    "                     ))\n",
    "    \n",
    "    batch_size, kernels_number, d_image_height, d_iamge_width = d_image.shape\n",
    "    \n",
    "    # Flip the kernel\n",
    "    flipped_kernel = np.rot90(kernels,2,(-2,-1))\n",
    "\n",
    "    # Computation\n",
    "    gradient_wrt_image = np.zeros_like(batch_of_images)\n",
    "    current_sum = 0.0\n",
    "    for single_image in range(batch_size):\n",
    "        for image_row_idx in range(image_height):\n",
    "            for image_col_idx in range(iamge_width):\n",
    "                for single_kernel in range(kernels_number):\n",
    "                    for input_channel in range(kernel_channels):\n",
    "                        for kernel_row_idx in range(kernel_height):\n",
    "                            y = image_row_idx + kernel_row_idx\n",
    "\n",
    "                            for kernel_col_idx in range(kernel_width):\n",
    "                                x = image_row_idx + kernel_col_idx\n",
    "\n",
    "                                if 0 <= y < d_image.shape[-2] and 0 <= x < d_image.shape[-1]:\n",
    "                                    input_element = d_image[single_image,single_kernel,y,x]\n",
    "                                    kernel_element = flipped_kernel[single_kernel,input_channel,kernel_row_idx,kernel_col_idx] \n",
    "                                else:\n",
    "                                    break\n",
    "\n",
    "                                current_sum += input_element*kernel_element\n",
    "\n",
    "                    gradient_wrt_image[single_image,input_channel,image_row_idx,image_col_idx] = current_sum\n",
    "                    current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the dilated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    gradient_wrt_kernel = np.zeros_like(kernels)\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    current_sum = 0.0\n",
    "\n",
    "    for single_image in range(batch_size):\n",
    "        for kernel_row_idx in range(kernel_height):\n",
    "            for kernel_col_idx in range(kernel_width):\n",
    "                for input_channel in range(kernel_channels):\n",
    "                    # The number of kernels determines the number of channels of the output tensor\n",
    "                    for outpot_channel in range(kernels_number):\n",
    "                        for d_image_row_idx in range(d_image_height):\n",
    "                            y = kernel_row_idx + d_image_row_idx\n",
    "\n",
    "                            for d_image_col_idx in range(d_iamge_width):\n",
    "                                x = kernel_row_idx + d_image_col_idx\n",
    "\n",
    "                                if 0 <= y < image_height and 0 <= x < iamge_width:\n",
    "                                    input_element = batch_of_images[single_image,input_channel,y,x]\n",
    "                                    kernel_element = d_image[single_image,outpot_channel,d_image_row_idx,d_image_col_idx] \n",
    "                                    current_sum += input_element*kernel_element\n",
    "                                else:\n",
    "                                    break\n",
    "\n",
    "                        gradient_wrt_kernel[outpot_channel,input_channel,kernel_row_idx,kernel_col_idx] = current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gradient_wrt_bias = d_image.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gradient_wrt_image, gradient_wrt_kernel, gradient_wrt_bias\n",
    "\n",
    "kernel_channels = 1\n",
    "kernels_number = 2\n",
    "image_side_len = 7\n",
    "kernel_side_len = 2\n",
    "example_stride = 2\n",
    "example_padding = 1\n",
    "\n",
    "example_image = np.arange(1, kernel_channels * image_side_len * image_side_len + 1)\n",
    "example_image = example_image.reshape(1, kernel_channels, image_side_len, image_side_len)\n",
    "\n",
    "example_kernel = np.arange(1, kernels_number * kernel_channels * (kernel_side_len**2) + 1)\n",
    "example_kernel = example_kernel.reshape(kernels_number, kernel_channels, kernel_side_len, kernel_side_len)\n",
    "\n",
    "example_d_image, mask = nested_loop_convolution(example_image, example_kernel, stride=example_stride, padding=example_padding) \n",
    "example_d_image = example_d_image / np.mean(example_d_image)\n",
    "\n",
    "image_grad, kernel_grad, bias_grad = nested_loop_gradient(example_image, example_d_image, example_kernel, mask, stride=example_stride, padding=example_padding)\n",
    "\n",
    "print(f\"image: {example_image.shape}\")\n",
    "print(f\"kernel: {example_kernel.shape}\")\n",
    "print(f\"d_image: {example_d_image.shape}\")\n",
    "print(f\"gradient w.r.t. image: {image_grad.shape}\")\n",
    "print(f\"gradient w.r.t. kernel: {kernel_grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2fbc0",
   "metadata": {},
   "source": [
    "## Im2Col Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae10455",
   "metadata": {},
   "source": [
    "### Im2Col approach: Convolutional Layer - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c44f",
   "metadata": {},
   "source": [
    "`im2col_convolution` implements 2D convolution more efficiently using an Im2Col approach using`sliding_window_view` and optimized matrix multiplication.\n",
    "\n",
    "**Im2Col Core Idea:**\n",
    "\n",
    "* **Input Patch Extraction:**\n",
    "    *   `window_m = ... .reshape((-1,(kw*kh*nc)))`: Flattens each extracted patch into a row vector of size `kw*kh*nc`. `window_m` (our $X_{col}$) thus has shape `(N_patches, patch_size)`.\n",
    "    *   `kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)`: Flattens each filter `(kc, ac, kw, kh)` -> `(kc, ac*kw*kh)`. This is $W_{col}$, shape `(patch_size, C_out)`.\n",
    "    Following the example in the animation below, we have a 4x4 RGB image (so **3x4x4**) that has to be convoluted by a **2x2** kernel (represented as the red outlines sliding over the image). The kernel will slide **9** times over the image and, for each slide, the number of multiplications and values to be summed is 4 (elements of the kernel) times 3 (channels of both the image and the kernel) that equals **12**. Hence, as we can see in the animation below, the flattened patches are vectors of **12** elements, and in total they are **9**, as the slides performed by the kernel.\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/1.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Each patch is extracted and flattened into a vector</figcaption>\n",
    "</figure>\n",
    "\n",
    "* Kernel Reshaping:\n",
    "    To perform the convolution as a vetor-matrix multilication (or matrix-matrix multiplication) we then have to flatten the kernel(s) into row vectors\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/10.png\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Kernels are flattened into a vector</figcaption>\n",
    "</figure>\n",
    "\n",
    "* **Convolution as Matrix Multiplication:**\n",
    "    At this point, the convolution boils down to a simple matrix multiplication, with enormous gains in efficiency and simplicity\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/11.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Each element of the output is given by a simple and optimized dot product</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56c9deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Conv Slow-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast Matrix-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n"
     ]
    }
   ],
   "source": [
    "def im2col_convolution(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "\n",
    "    # im2col: Window creation\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    sliding_windows = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,input_channels,kernel_height,kernel_width))[:,:,::stride,::stride]\n",
    "    sliding_windows = sliding_windows.reshape((-1,(kernel_height * kernel_width * input_channels)))\n",
    "\n",
    "    # Convolution\n",
    "    kernels = kernels.reshape((-1,(kernel_height*kernel_width*input_channels))).transpose(1,0)\n",
    "    image_dot_kernel = (sliding_windows @ kernels).astype(np.float32) # convolved image matrix\n",
    "\n",
    "    # ReLU activation\n",
    "    output_width = int(((image_width-kernel_width) / stride) + 1) # Padding was already added\n",
    "    output_height = int(((image_height-kernel_height) / stride) + 1)\n",
    "\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output = image_dot_kernel.reshape(batch_size, output_width, output_height, kernels_number)\n",
    "\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    output = output.transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "    if biases.any() != 0:\n",
    "        output = (output + biases.reshape(1,-1,1,1))\n",
    "\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0,output)\n",
    "\n",
    "    mask = np.copy(output)\n",
    "    mask[mask > 0] = 1\n",
    "\n",
    "    return output, mask\n",
    "\n",
    "def im2col_convolutional_matrix(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    # Padding\n",
    "    if padding > 0:\n",
    "        batch_of_images = np.pad(batch_of_images, ((0, 0), (0, 0), (padding, padding), (padding, padding)))\n",
    "\n",
    "    kn, kc, kh, kw = kernels.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    bs, nc, ih, iw = batch_of_images.shape # batch size, number of channels, image height and image width\n",
    "\n",
    "\n",
    "    # Unroll del kernel e dell'input per la moltiplicazione matriciale   \n",
    "    oh_idx_calc = ih - kh + 1 # Output height se stride fosse 1\n",
    "    ow_idx_calc = iw - kw + 1 # Output width se stride fosse 1\n",
    "\n",
    "    # Preparazione della convolution_matrix (kernel unrolled)\n",
    "    convolution_matrix = np.zeros((nc, ih, iw, kn), dtype=kernels.dtype)\n",
    "    convolution_matrix[:, :kh, :kw, :] = kernels.transpose(1, 2, 3, 0)\n",
    "    convolution_matrix = convolution_matrix.reshape(nc, -1, kn) # (nc, ih * iw, kn)\n",
    "\n",
    "    row_idx = np.arange(oh_idx_calc * ow_idx_calc) // ow_idx_calc * (kw - 1)\n",
    "    y_idx, x_idx = np.ogrid[:ih * iw, :oh_idx_calc * ow_idx_calc]\n",
    "    idx = (y_idx - (x_idx + row_idx)) % (ih * iw)\n",
    "    # idx.shape = (ih * iw, oh_idx_calc * ow_idx_calc)\n",
    "\n",
    "    convolution_matrix = convolution_matrix[:, idx, :]\n",
    "\n",
    "    batch_of_images = batch_of_images.reshape(bs, nc, -1) # (bs, nc, ih*iw)\n",
    "\n",
    "    # Convoluzione (stride 1)\n",
    "    # batch_of_images: (bs, nc, ih*iw)\n",
    "    # convolution_matrix: (nc, ih*iw, oh_idx_calc*ow_idx_calc, kn)\n",
    "    # Assi per somma: (nc, ih*iw) per entrambi\n",
    "    output = np.tensordot(\n",
    "        batch_of_images,\n",
    "        convolution_matrix,\n",
    "        axes=((1, 2), (0, 1)) # Somma su nc e sulla dimensione spaziale appiattita dell'input\n",
    "    )\n",
    "    # output.shape: (bs, oh_idx_calc*ow_idx_calc, kn)\n",
    "\n",
    "    # Reshape nella forma 4D (output di convoluzione stride 1)\n",
    "    output = output.transpose(0, 2, 1).reshape(bs, kn, oh_idx_calc, ow_idx_calc)\n",
    "\n",
    "    # Applicazione dello stride all'output della convoluzione stride 1\n",
    "    if stride > 1:\n",
    "        output = output[:, :, ::stride, ::stride]\n",
    "\n",
    "    # Bias\n",
    "    if biases is not None and biases.any() != 0:\n",
    "        output = output + biases.reshape(1, -1, 1, 1)\n",
    "\n",
    "    mask = None\n",
    "\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0, output)\n",
    "        \n",
    "    mask = np.copy(output)\n",
    "    mask[mask > 0] = 1\n",
    "        \n",
    "    return output, mask\n",
    "\n",
    "img = np.arange(1,2*3*3+1).reshape(1,2,3,3).astype(np.float32)\n",
    "# print(\"-------img-------\")\n",
    "# print(img)\n",
    "ker = np.arange(1,16+1).reshape(2,2,2,2)\n",
    "# print(\"-------ker-------\")\n",
    "# print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "\n",
    "res,mask = nested_loop_convolution(img, ker, bias,padding=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = im2col_convolution(img, ker, bias,padding=0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)\n",
    "X_c, mask = im2col_convolutional_matrix(img, ker, bias,padding=0,stride=1)\n",
    "print(\"-------Conv Fast Matrix-------\")\n",
    "print(X_c)\n",
    "# res,mask = nested_loop_convolution(res, ker, bias,padding=0,stride=1)\n",
    "# print(\"-------Conv Slow-------\")\n",
    "# print(res)\n",
    "# X_c,mask = im2col_convolution(X_c, ker, bias,padding=0,stride=1)\n",
    "# print(\"-------Conv Fast-------\")\n",
    "# print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c414872",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e1403",
   "metadata": {},
   "source": [
    "### \"Fast\" Convolution Backward Pass\n",
    "\n",
    "This revised `Fast_ReLU_Gradient` computes the following gradients using again the im2col approach:\n",
    "* $\\frac{\\partial L}{\\partial X}$ (`gi`): gradient w.r.t. the input images;\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial W}$ (`gk`): gradient w.r.t. the kernels;\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial b}$ (`gb`): gradient w.r.t. the biases.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Backward ReLU:** $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\text{mask}$<br>\n",
    "    \"$\\text{mask}$\" is a matrix (tensor) whose elements are 1 if the corrisponding elements in the output of a given layer were > 0. It enables the flowing of the gradient only through elements that contributed to form the output in the first place.\n",
    "\n",
    "2.  **Gradient w.r.t. Input (`gi`):**\n",
    "    $\\frac{\\partial L}{\\partial X} = \\text{FullConv}(\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}, W_{rot180})$<br>\n",
    "    A full convolution is needed, since we must obtain a result with the same size as the input image. As you remember, the output size is given by this formula: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1$, so we must \"invert\" it. In particular, the output tensor must be:\n",
    "    * dilated by $stride - 1$ and\n",
    "    * padded by:\n",
    "    $(kernel\\_height-1-input\\_padding{\\textnormal{, }}kernel\\_width-1-input\\_padding)$\n",
    "\n",
    "2.  **Gradient w.r.t. Kernel (`gk`):**\n",
    "    This is $\\frac{\\partial L}{\\partial W} = \\text{Conv}(X_{padded}, \\frac{\\partial L}{\\partial Z})$.\n",
    "   \n",
    "\n",
    "3.  **Gradient w.r.t. Bias (`gb`):** $\\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} (\\frac{\\partial L}{\\partial Z})_{n,f,h,w}$. (`gb = d_img.sum((-1,-2))`).\n",
    "\n",
    "**What if the stride was greater than 1 in the forward phase?**<br>\n",
    "To give a simpler explanation we will now consider the case in which stride is $S= 2$. The output gradient tensor needs to be \"dilated\" to be convoluted with the kernel gradient, in order to obtain a tensor with the same size as the input image, effectively useful to backpropgate the loss. A visualization of why dilation enables this fundamental result is in the gif below:\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KOHfsOHX5ujcMfr6Xjy6zQ.png\" height=\"250\", style=\"border-radius:20px 0 0 20px;\"/>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QvTW-pNwJAlbfj1LrZe8JA.gif\" height=\"250\", style=\"border-radius:0 20px 20px 0;\"/>\n",
    "    <figcaption>Te Backpropagation operation is identical to a stride = 1 Convolution of a padded, dilated version of the output gradient tensor with a flipped version of the filter.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c5583e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimAge: (1, 4, 2, 2)\n",
      "imAge: (1, 3, 5, 5)\n",
      "kerNel: (4, 3, 3, 3)\n",
      "dimAge: (1, 4, 2, 2)\n",
      "ggi: (1, 3, 5, 5)\n",
      "ggradient_wrt_kernels: (4, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "def im2col_gradient(batch_of_images, d_image, kernels, mask, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\"\n",
    "    ############ Gradient of Input Image ############\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image dilated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel is rotated by 180 degrees (flipped vertically and horizontally)\n",
    "    # FullConvolution(d_imageDilated, Rotated180Deg(kernel)) with stride 1\n",
    "    output_channels, input_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_image = np.multiply(d_image, mask)\n",
    "\n",
    "    # Delating the gradient of the output\n",
    "    d_image = dilate(d_image, stride)\n",
    "  \n",
    "    # Padding the gradient of the output\n",
    "    d_image_padded = np.pad(d_image,((0,0),(0,0),(kernel_height-1-padding,kernel_height-1-padding),(kernel_width-1-padding,kernel_width-1-padding)))\n",
    "\n",
    "    batch_size, output_channels, d_image_height, d_image_width = d_image.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    kernels = np.rot90(kernels,2,(-2,-1))\n",
    "\n",
    "    sliding_windows_d_image = np.lib.stride_tricks.sliding_window_view(d_image_padded,(1,output_channels,kernel_width,kernel_height))\n",
    "    sliding_windows_d_image = sliding_windows_d_image.reshape(-1,(kernel_width*kernel_height*output_channels)) # window matrix\n",
    "    \n",
    "    # Convolution\n",
    "    kernels = kernels.reshape((-1,(kernel_width*kernel_height*output_channels))).transpose(1,0)\n",
    "\n",
    "    gradient_wrt_images = (sliding_windows_d_image @ kernels).astype(np.float32).transpose(1,0).reshape(batch_of_images.shape)\n",
    "    \n",
    "    ############## Gradient of Kernel ##############\n",
    "    # The computation consists in a convolution between the original image and the dilated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    sliding_windows_batch_of_images = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,1,d_image_height,d_image_width)).reshape(-1,d_image_height*d_image_width)\n",
    "    \n",
    "    d_image = d_image.reshape(-1, d_image_height * d_image_width).transpose(1,0)\n",
    "    \n",
    "    gradient_wrt_kernels = (sliding_windows_batch_of_images @ d_image).astype(np.float32).transpose(1,0).reshape(output_channels,input_channels,kernel_height,kernel_width)\n",
    "    \n",
    "    ############### Gradient of Bias ###############\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    grdient_wrt_biases = d_image.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gradient_wrt_images, gradient_wrt_kernels, grdient_wrt_biases\n",
    "\n",
    "input_channels = 3\n",
    "output_channels = 4\n",
    "idim = 5\n",
    "kdim = 3\n",
    "s = 2\n",
    "p = 0\n",
    "\n",
    "imAge = np.arange(1,input_channels*idim*idim+1).reshape(1,input_channels,idim,idim)\n",
    "kerNel = np.arange(1,output_channels*input_channels*(kdim**2)+1).reshape(output_channels,input_channels,kdim,kdim)\n",
    "dimAge,mask = im2col_convolution(imAge,kerNel,stride=s,padding=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "\n",
    "ggi,ggradient_wrt_kernels,ggrdient_wrt_biases = im2col_gradient(imAge,dimAge,kerNel,mask,stride=s,padding=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggradient_wrt_kernels: {ggradient_wrt_kernels.shape}\")\n",
    "\n",
    "# input_channels = 1\n",
    "# output_channels = 2\n",
    "# idim = 7\n",
    "# kdim = 2\n",
    "# s = 1\n",
    "# p = 0\n",
    "\n",
    "# imAge: (1, 1, 7, 7)\n",
    "# kerNel: (2, 1, 2, 2)\n",
    "# dimAge: (1, 2, 6, 6)\n",
    "# ggi: (1, 1, 7, 7)\n",
    "# ggradient_wrt_kernels: (2, 1, 2, 2)\n",
    "\n",
    "# input_channels = 1\n",
    "# output_channels = 2\n",
    "# idim = 7\n",
    "# kdim = 2\n",
    "# s = 2\n",
    "# p = 1\n",
    "\n",
    "# imAge: (1, 1, 7, 7)\n",
    "# kerNel: (2, 1, 2, 2)\n",
    "# dimAge: (1, 2, 4, 4)\n",
    "# ggi: (1, 1, 7, 7)\n",
    "# ggradient_wrt_kernels: (2, 1, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "daa1dccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimAge: (1, 2, 6, 6)\n",
      "gradient_wrt_input: (1, 1, 7, 7)\n",
      "gradient_wrt_kernels: (2, 1, 2, 2)\n",
      "gradient_wrt_biases: ()\n",
      "gradient_wrt_input_matrix: (1, 1, 7, 7)\n",
      "gradient_wrt_kernels_matrix: (2, 1, 2, 2)\n",
      "gradient_wrt_biases_matrix: (2,)\n"
     ]
    }
   ],
   "source": [
    "def im2col_gradient_convolutional_matrix(original_forward_input, d_image, kernels, mask, padding, stride):\n",
    "    # 0. Backward ReLU: dL/dZ = dL/dA * mask\n",
    "    # (dL/dZ è il gradiente rispetto all'output del layer PRIMA della ReLU)\n",
    "\n",
    "    gradient_pre_activation = np.multiply(d_image, mask)\n",
    "\n",
    "    # --- Parametri dimensionali dalla Forward Pass ---\n",
    "    kernels_number, input_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, _, input_height, input_width = original_forward_input.shape\n",
    "    \n",
    "    # --- 1. Gradiente rispetto all'Input (gradient_wrt_input = dL/dX) ---\n",
    "    # Calcolato come: ConvoluzioneTrasposta(gradient_pre_activation, kernels_rot180)\n",
    "    # La Convoluzione Trasposta si implementa con:\n",
    "    #   a. Dilatazione di `gradient_pre_activation` (se stride_fwd > 1)\n",
    "    #   b. Convoluzione standard (stride 1) con kernel ruotati e padding specifico.\n",
    "\n",
    "    # a. Dilatazione di gradient_pre_activation\n",
    "    dilated_grad_pre_activation = dilate(gradient_pre_activation, stride)\n",
    "    _, _, dilated_grad_h, dilated_grad_w = dilated_grad_pre_activation.shape\n",
    "\n",
    "    # b. Kernel per dL/dX: ruotati di 180° e canali scambiati\n",
    "    flipped_kernels = np.rot90(kernels, 2, axes=(2, 3)).transpose(1, 0, 2, 3)\n",
    "\n",
    "    # c. Padding per la convoluzione interna di dL/dX\n",
    "    #    L'output di questa convoluzione (prima di un eventuale crop finale) deve essere\n",
    "    #    tale che, dopo il crop, si ottengano le dimensioni dell'input originale.\n",
    "    #    Padding_convoluzione_interna = KernelDim - 1 - Padding_Forward\n",
    "    padding_for_dX_conv = kernel_height - 1 - padding\n",
    "\n",
    "    # d. Esegui la convoluzione per dL/dX (stride interno SEMPRE 1)\n",
    "    gradient_wrt_input_raw, _ = im2col_convolutional_matrix(\n",
    "        dilated_grad_pre_activation,\n",
    "        flipped_kernels,\n",
    "        biases=None, # No bias nel calcolo del gradiente\n",
    "        padding=padding_for_dX_conv,\n",
    "        stride=1,\n",
    "        applyReLU=False\n",
    "    )\n",
    "\n",
    "    # e. Crop/slice finale per assicurare le dimensioni corrette di dL/dX\n",
    "    #    L'output `gradient_wrt_input_raw` dovrebbe avere dimensioni vicine a\n",
    "    #    (batch_size, input_channels, input_height, input_width)\n",
    "    #    Se il padding è stato calcolato correttamente, potrebbe già corrispondere o\n",
    "    #    essere leggermente più grande.\n",
    "    gradient_wrt_input = gradient_wrt_input_raw[:, :, :input_height, :input_width]\n",
    "\n",
    "\n",
    "    # --- 2. Gradiente rispetto ai Pesi (gradient_wrt_kernels = dL/dW) ---\n",
    "    # Calcolato come: Somma_batch[ Convoluzione(original_forward_input_padded, gradient_pre_activation) ]\n",
    "    # dove la convoluzione ha lo STESSO padding e stride della forward pass,\n",
    "    # e l'output di questa convoluzione ha le dimensioni spaziali dei filtri (kernel_height, kernel_width).\n",
    "    # L'input a questa convoluzione è `original_forward_input`.\n",
    "    # Il \"kernel\" per questa convoluzione è `gradient_pre_activation`.\n",
    "\n",
    "    gradient_wrt_kernels_sum = np.zeros_like(kernels)\n",
    "\n",
    "    # Dimensioni spaziali di gradient_pre_activation (che agisce come kernel)\n",
    "    _, _, grad_h_as_kernel, grad_w_as_kernel = gradient_pre_activation.shape\n",
    "\n",
    "    for b_idx in range(batch_size):\n",
    "        for k_fwd_idx in range(kernels_number):         # Per ogni filtro di output della forward (k)\n",
    "            for c_fwd_idx in range(input_channels): # Per ogni canale di input della forward (c)\n",
    "                \n",
    "                # Input per calcolare dW[k,c]: original_forward_input[b, c]\n",
    "                # Kernel per calcolare dW[k,c]: gradient_pre_activation[b, k]\n",
    "                \n",
    "                input_slice_for_dW = original_forward_input[b_idx:b_idx+1, c_fwd_idx:c_fwd_idx+1, :, :]\n",
    "                # -> (1, 1, input_height, input_width)\n",
    "                \n",
    "                grad_slice_as_kernel = gradient_pre_activation[b_idx:b_idx+1, k_fwd_idx:k_fwd_idx+1, :, :]\n",
    "                # -> (1, 1, grad_h_as_kernel, grad_w_as_kernel)\n",
    "                # Questo è il \"filtro\" per la nostra funzione di convoluzione.\n",
    "                \n",
    "                # Esegui la convoluzione. L'output deve avere dimensioni kernel_height x kernel_width.\n",
    "                # Per ottenere ciò, il padding e lo stride devono essere quelli della forward originale.\n",
    "                # La tua `cnn_forward_pass_matrix_unroll` calcola le dimensioni dell'output.\n",
    "                # Dobbiamo assicurarci che queste dimensioni corrispondano a kernel_height, kernel_width.\n",
    "                # (H_in_padded - K_h_eff) // S + 1 = H_target\n",
    "                # (input_height + 2*padding - grad_h_as_kernel) // stride + 1 == kernel_height\n",
    "                # Questo è vero per definizione se la forward pass era standard.\n",
    "                \n",
    "                component_grad_W, _ = im2col_convolutional_matrix(\n",
    "                    input_slice_for_dW,\n",
    "                    grad_slice_as_kernel,\n",
    "                    biases=None, # No bias nel calcolo del gradiente\n",
    "                    padding=padding,\n",
    "                    stride=stride,\n",
    "                    applyReLU=False\n",
    "                )\n",
    "                \n",
    "                # L'output component_grad_W dovrebbe essere (1, 1, kernel_height, kernel_width)\n",
    "                if component_grad_W.shape == (1, 1, kernel_height, kernel_width):\n",
    "                    gradient_wrt_kernels_sum[k_fwd_idx, c_fwd_idx, :, :] += component_grad_W[0, 0, :, :]\n",
    "                else:\n",
    "                    # Questo indica un problema dimensionale nel setup per dL/dW\n",
    "                    print(f\"Attenzione: Shape errata per component_grad_W: {component_grad_W.shape}\")\n",
    "\n",
    "    # --- 3. Gradiente rispetto ai Bias (gradient_wrt_biases = dL/db) ---\n",
    "    # Somma `gradient_pre_activation` (dL/dZ) lungo le dimensioni batch, altezza, larghezza.\n",
    "    # gradient_pre_activation ha shape (batch_size, kernels_number, oh_grad, ow_grad)\n",
    "    # gradient_wrt_biases deve avere shape (kernels_number,)\n",
    "    gradient_wrt_biases = np.sum(gradient_pre_activation, axis=(0, 2, 3))\n",
    "\n",
    "    return gradient_wrt_input, gradient_wrt_kernels_sum, gradient_wrt_biases\n",
    "\n",
    "# Test the backward pass gradients\n",
    "# 1. Define the forward pass parameters\n",
    "input_channels = 1\n",
    "output_channels = 2\n",
    "idim = 7\n",
    "kdim = 2\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "s = 1\n",
    "p = 0\n",
    "imAge = np.arange(1,input_channels*idim*idim+1).reshape(1,input_channels,idim,idim)\n",
    "kerNel = np.arange(1,output_channels*input_channels*(kdim**2)+1).reshape(output_channels,input_channels,kdim,kdim)\n",
    "# 2. Perform the forward pass\n",
    "dimAge, mask = im2col_convolution(imAge, kerNel, bias, stride=s, padding=p)\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "# 3. Perform the backward pass\n",
    "gradient_wrt_input, gradient_wrt_kernels, gradient_wrt_biases = im2col_gradient(imAge, dimAge, kerNel, mask, stride=s, padding=p)\n",
    "print(f\"gradient_wrt_input: {gradient_wrt_input.shape}\")\n",
    "print(f\"gradient_wrt_kernels: {gradient_wrt_kernels.shape}\")\n",
    "print(f\"gradient_wrt_biases: {gradient_wrt_biases.shape}\")\n",
    "# 4. Perform the backward pass using the convolution matrix approach\n",
    "dimAge, mask = im2col_convolutional_matrix(imAge, kerNel, bias, stride=s, padding=p)\n",
    "gradient_wrt_input_matrix, gradient_wrt_kernels_matrix, gradient_wrt_biases_matrix = im2col_gradient_convolutional_matrix(imAge, dimAge, kerNel, mask, padding=p, stride=s)\n",
    "print(f\"gradient_wrt_input_matrix: {gradient_wrt_input_matrix.shape}\")\n",
    "print(f\"gradient_wrt_kernels_matrix: {gradient_wrt_kernels_matrix.shape}\")\n",
    "print(f\"gradient_wrt_biases_matrix: {gradient_wrt_biases_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249b2ef",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3f21f",
   "metadata": {},
   "source": [
    "### MLP Forward Pass\n",
    "\n",
    "`ReLU_SoftMax_FullyConnected` executes the forward pass of a two-layer Multi-Layer Perceptron (one hidden layer, one output layer), typically used for classification after feature extraction by convolutional layers.\n",
    "\n",
    "**Operations:**\n",
    "\n",
    "1.  **Input:** `input_array` ($X_{mlp}$), the flattened output from conv layers.\n",
    "2.  **Hidden Layer (fc1):**\n",
    "    *   Linear: $Z_1 = X_{mlp} W_1 + b_1$ (output `fl`)\n",
    "    *   ReLU Activation: $A_1 = \\max(0, Z_1)$ (output `fa`)\n",
    "3.  **Output Layer (fc2):**\n",
    "    *   Linear: $Z_2 = A_1 W_2 + b_2$ (output `sl`, logits)\n",
    "    *   Softmax Activation: $P = \\text{Softmax}(Z_2)$ (output `sa`, probabilities)\n",
    "        $$ \\text{Softmax}(z)_j = \\frac{e^{z_j - \\max(z)}}{\\sum_{k} e^{z_k - \\max(z)}} $$\n",
    "        (The subtraction of $\\max(z)$ aids numerical stability).\n",
    "\n",
    "Returns `fl, fa, sl, sa` (pre-activations, activations, logits, probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68206d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa\n",
    "\n",
    "#print(softmax([1,2,3,100000]))\n",
    "#print(softmax_no_NS([1,2,3,1000]))\n",
    "#r = np.array(np.array([1,2,777,2]))\n",
    "#print(softmax(r))\n",
    "#r = np.array((np.array([1,2,777,2]),np.array([1,2,777,2]),np.array([1,2,777,2])))\n",
    "#print(softmax(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3eaeb8",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7b0a1",
   "metadata": {},
   "source": [
    "### MLP Backward Pass\n",
    "\n",
    "`ReLU_SoftMax_FC_Backward` computes gradients for the MLP. Inputs: batch size `bs`, predictions `pred` ($P$), true `labels` ($Y$), weights $W_1, W_2$, hidden activation `fa` ($A_1$), hidden pre-activation `fl` ($Z_1$), MLP input `i_mlp` ($X_{mlp}$).\n",
    "\n",
    "**Gradients (from output layer backwards):**\n",
    "\n",
    "1.  $\\frac{\\partial L}{\\partial Z_2} = P - Y$ (`dL_dz2`)\n",
    "2.  $\\frac{\\partial L}{\\partial W_2} = A_1^T \\frac{\\partial L}{\\partial Z_2}$ (`dL_dw2`)\n",
    "3.  $\\frac{\\partial L}{\\partial b_2} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_2}$ (`dL_db2`)\n",
    "4.  $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} W_2^T$ (`dL_dfa`)\n",
    "5.  $\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\text{ReLU}'(Z_1)$ (`dL_dfl`, where $\\text{ReLU}'(Z_1)$ is 1 if $Z_1 > 0$, else 0)\n",
    "6.  $\\frac{\\partial L}{\\partial W_1} = X_{mlp}^T \\frac{\\partial L}{\\partial Z_1}$ (`dL_dw1`)\n",
    "7.  $\\frac{\\partial L}{\\partial b_1} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_1}$ (`dL_db1`)\n",
    "8.  $\\frac{\\partial L}{\\partial X_{mlp}} = \\frac{\\partial L}{\\partial Z_1} W_1^T$ (`dL_i_mlp`) (gradient to pass to conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10736bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels[0:bs]\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd7105",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf96a8c",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy\n",
    "\n",
    "`crossEntropy` calculates the Categorical Cross-Entropy loss for a single sample.\n",
    "\n",
    "**Formula:** Given predicted probabilities $P=(p_1, ..., p_K)$ and one-hot true label $Y=(y_1, ..., y_K)$:\n",
    "$$ L(P, Y) = - \\sum_{k=1}^{K} y_k \\log(p_k) $$\n",
    "If class $c$ is the true class ($y_c=1$), $L = - \\log(p_c)$.\n",
    "A small epsilon (`1/100000`) is added to $p$ to prevent $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1291609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(p,t):\n",
    "    # p stands for prediction and t stands for true label\n",
    "    # p = [0,0,1] and t = [1,0,0]\n",
    "    p = p+(1/100000) # for numerical stability\n",
    "    return -np.dot(t,np.log(p).T)\n",
    "\n",
    "#c = [1,1000000000000000,1,1]\n",
    "#c = softmax(c)\n",
    "#print(c)\n",
    "#c = crossEntropy(c,[0,1,0,0])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49079",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e3c9f",
   "metadata": {},
   "source": [
    "## Inference: Comparing Implementations\n",
    "\n",
    "This section compares the inference (prediction) performance and correctness of three CNN implementations: PyTorch, \"Slow\" NumPy (loop-based), and \"Fast\" NumPy (Im2Col-based). All use identical pre-trained weights.\n",
    "\n",
    "**Objectives:**\n",
    "1.  **Correctness:** Verify that all three models yield the same predictions.\n",
    "2.  **Speed:** Compare average inference time per image.\n",
    "\n",
    "The loop iterates through test images, runs each model, records predictions and times. This demonstrates the efficiency gains from optimized libraries (PyTorch) and vectorized NumPy (Fast) over naive loops (Slow). The `pad` and `stride` parameters in the NumPy calls (`Slow_ReLU_Conv`, `im2col_convolution`) are set to match the PyTorch model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 100/100 [02:02<00:00,  1.23s/it, average_times=t: 0.0024 s, s: 1.2202 s, f: 0.0015 s, correct_predictions=100.0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward execution time in seconds: \n",
      "PyTorch: 0.0024 s, \n",
      "Slow: 1.2202 s, \n",
      "Fast: 0.0015 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "\n",
    "#length = test_labels.shape[0]\n",
    "length = 100\n",
    "correct = 0\n",
    "skip = True\n",
    "loop = tqdm(range(length),desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "    ############### CNN Slow Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted2 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "    ############### CNN Fast Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = im2col_convolution(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2f,mask2f = im2col_convolution(c1f.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3f,mask3f = im2col_convolution(c2f.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted3 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    t = int(predicted1[0])\n",
    "    s = int(predicted2[0])\n",
    "    f = int(predicted3[0])\n",
    "    if t == s and t == f:\n",
    "        correct+=1\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    tat = round(sum(dict_times['ctorch'])/(i+1),4)\n",
    "    sat = round(sum(dict_times['cslow'])/(i+1),4)\n",
    "    fat = round(sum(dict_times['cfast'])/(i+1),4)\n",
    "    loop.set_postfix(average_times =f\"t: {tat} s, s: {sat} s, f: {fat} s\" , correct_predictions=f\"{100*correct/(i+1)}%\")\n",
    "tat = round(sum(dict_times['ctorch'])/length,4)\n",
    "sat = round(sum(dict_times['cslow'])/length,4)\n",
    "fat = round(sum(dict_times['cfast'])/length,4)\n",
    "print(f\"Average forward execution time in seconds: \\nPyTorch: {tat} s, \\nSlow: {sat} s, \\nFast: {fat} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abe784",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3b44b",
   "metadata": {},
   "source": [
    "### Test for Slow approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f8f80",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d1c78",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a476a",
   "metadata": {},
   "source": [
    "### NumPy Model Training: Weights Initialization\n",
    "\n",
    "For training our NumPy CNNs from scratch, weights and biases are initialized randomly.\n",
    "The shapes are taken from `numpy_weights` (derived from the PyTorch model) to maintain architectural consistency. `np.random.rand()` provides initial values (uniform in [0,1)). While more advanced initializers exist, this suffices for observing basic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "733935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa5546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382d816",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b10d0",
   "metadata": {},
   "source": [
    "### Training the \"Slow\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This tests training the loop-based `Slow_ReLU_Conv` and `Slow_ReLU_Gradient` implementations on a single image.\n",
    "\n",
    "**Per-Epoch Steps:**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> Slow_ReLU_Conv (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> Slow_ReLU_Conv (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> Slow_ReLU_Conv (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa` (probabilities)\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** Gradients are computed using `ReLU_SoftMax_FC_Backward` for MLP, then `Slow_ReLU_Gradient` is called sequentially for conv layers, propagating gradients backward.\n",
    "4.  **Weight Update:** Parameters updated via $W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W_{old}}$.\n",
    "\n",
    "The loss is plotted to observe learning. The padding and stride parameters in `Slow_ReLU_Conv` calls are set to match the PyTorch model architecture, ensuring the flattened features `imlps` have the correct dimension (2048) for the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40e5bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:30<00:00,  4.54s/it, avgBackward=3.3668 s, avgForward=1.168 s, pendence=[-0.02896908]] \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToBeTrained = True\n",
    "if ToBeTrained:\n",
    "    avg_loss = []\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    numEpochs = 20\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    loop = tqdm(range(numEpochs))\n",
    "    for i in loop:\n",
    "        c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "        \n",
    "        # Forward\n",
    "        sfts = time.time() # slow forward time start\n",
    "        c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "        c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "        c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "\n",
    "        imlps = c3s.reshape(1,-1)\n",
    "        fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "        sfte = time.time() # slow forward time end\n",
    "        sft = sfte - sfts\n",
    "        forward_time.append(sft)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa,train_labels[0])\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        sbts = time.time() # slow backward time start\n",
    "        dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "        dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "        gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "\n",
    "        gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "        gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "        sbte = time.time() # slow backward time end\n",
    "        sbt = sbte - sbts\n",
    "        backward_time.append(sbt)\n",
    "\n",
    "        # Weights update\n",
    "        w1 -= lr*dL_dw1\n",
    "        b1 -= lr*dL_db1\n",
    "        w2 -= lr*dL_dw2\n",
    "        b2 -= lr*dL_db2\n",
    "        k3 -= lr*gk3\n",
    "        k2 -= lr*gk2\n",
    "        k1 -= lr*gk1\n",
    "        bc3 -= lr*gb3.reshape(-1)\n",
    "        bc2 -= lr*gb2.reshape(-1)\n",
    "        bc1 -= lr*gb1.reshape(-1)\n",
    "        \n",
    "        if len(avg_loss) >= 2:\n",
    "            loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "    plt.plot(avg_loss)\n",
    "    plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef545289",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 3.6265 s\n",
    "- average backward time : 9.8262 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Slow Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea8a5",
   "metadata": {},
   "source": [
    "### Test for Fast approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1f5ac",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a336a9",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "756e2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffd8c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d6af1",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fff279",
   "metadata": {},
   "source": [
    "### Training the \"Fast\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This tests training using the Im2Col-based `im2col_convolution` and the revised `Fast_ReLU_Gradient` (from cell `c808bdb6`) on a single image.\n",
    "\n",
    "**Per-Epoch Steps (differences from \"Slow\" are conv/grad functions):**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> im2col_convolution (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> im2col_convolution (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> im2col_convolution (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa`\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** `ReLU_SoftMax_FC_Backward` for MLP, then `Fast_ReLU_Gradient` (using `sliding_window_view` for `gi` and `gk`) for conv layers.\n",
    "4.  **Weight Update:** Standard gradient descent.\n",
    "\n",
    "The loss is plotted. Consistent padding/stride ensures correct feature dimensions for the MLP. This setup tests the learning capability and performance of the more optimized NumPy convolution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 112.21it/s, avgBackward=0.0025 s, avgForward=0.0008 s, pendence=[-0.01787574]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = im2col_convolution(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "    c2s,mask2s = im2col_convolution(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = im2col_convolution(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = Fast_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "    gi2,gk2,gb2 = Fast_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "    gi1,gk1,gb1 = Fast_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd84d2",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 0.0022 s\n",
    "- average backward time : 0.0097 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Fast Approach.png\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
