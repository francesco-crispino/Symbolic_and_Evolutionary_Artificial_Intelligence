{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42490d20",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82b0d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "#-------------- Data Extraction ---------------------------\n",
    "\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcf859",
   "metadata": {},
   "source": [
    "## CNN - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c11b9",
   "metadata": {},
   "source": [
    "### PyTorch CNN Model Architecture\n",
    "\n",
    "This section defines our Convolutional Neural Network (CNN) using PyTorch's `nn.Module`. This PyTorch model will serve as a reference, particularly for obtaining trained weights that we'll later use in our NumPy implementations.\n",
    "\n",
    "**Layers and Their Parameters:**\n",
    "\n",
    "The `SimpleCNN` class has the following architecture:\n",
    "\n",
    "1.  **`conv1`**: 2D Convolutional Layer (`nn.Conv2d`)\n",
    "    *   `in_channels=1`: Accepts 1 input channel (grayscale image).\n",
    "    *   `out_channels=32`: Produces 32 output feature maps (using 32 filters).\n",
    "    *   `kernel_size=2`: Uses 2x2 filters.\n",
    "    *   `stride=2`: Moves the filter 2 pixels at a time.\n",
    "    *   `padding=0`: No zero-padding around the input.\n",
    "    *   **Output size calculation for a dimension:** $O = \\lfloor \\frac{I - K + 2P}{S} \\rfloor + 1$\n",
    "        *   For an input of 28x28: $O_H = O_W = \\lfloor \\frac{28 - 2 + 2 \\cdot 0}{2} \\rfloor + 1 = \\lfloor \\frac{26}{2} \\rfloor + 1 = 13 + 1 = 14$.\n",
    "        *   Output shape (Batch, 32, 14, 14).\n",
    "    *   Followed by `relu1`: ReLU activation function ($f(x) = \\max(0, x)$).\n",
    "\n",
    "2.  **`conv2`**: 2D Convolutional Layer\n",
    "    *   `in_channels=32`: Accepts 32 input channels from `conv1`.\n",
    "    *   `out_channels=64`: Produces 64 output feature maps.\n",
    "    *   `kernel_size=2`: Uses 2x2 filters.\n",
    "    *   `stride=2`: Moves the filter 2 pixels at a time.\n",
    "    *   `padding=1`: Adds 1 layer of zero-padding around the input.\n",
    "        *   Input size from `conv1` is 14x14. Padded input: $14 + 2 \\cdot 1 = 16$.\n",
    "        *   $O_H = O_W = \\lfloor \\frac{16 - 2 + 2 \\cdot 0}{2} \\rfloor + 1 = \\lfloor \\frac{14}{2} \\rfloor + 1 = 7 + 1 = 8$. (Note: PyTorch padding is applied before stride, effectively $P$ in the formula refers to the padding in `nn.Conv2d`, so the formula applies to the padded input size, or adjust $2P$ term if $I$ is unpadded size). Simpler: $(I_{padded} - K)/S + 1 = (14+2*1 - 2)/2 + 1 = (16-2)/2 + 1 = 8$.\n",
    "        *   Output shape (Batch, 64, 8, 8).\n",
    "    *   Followed by `relu2`: ReLU activation.\n",
    "\n",
    "3.  **`conv3`**: 2D Convolutional Layer\n",
    "    *   `in_channels=64`: Accepts 64 input channels from `conv2`.\n",
    "    *   `out_channels=128`: Produces 128 output feature maps.\n",
    "    *   `kernel_size=2`: Uses 2x2 filters.\n",
    "    *   `stride=2`: Moves the filter 2 pixels at a time.\n",
    "    *   `padding=0`: No zero-padding.\n",
    "        *   Input size from `conv2` is 8x8.\n",
    "        *   $O_H = O_W = \\lfloor \\frac{8 - 2 + 2 \\cdot 0}{2} \\rfloor + 1 = \\lfloor \\frac{6}{2} \\rfloor + 1 = 3 + 1 = 4$.\n",
    "        *   Output shape (Batch, 128, 4, 4).\n",
    "    *   Followed by `relu3`: ReLU activation.\n",
    "\n",
    "4.  **`flatten`**: (`nn.Flatten`)\n",
    "    *   Reshapes the 3D output of `conv3` (128 channels, 4x4 spatial) into a 1D vector.\n",
    "    *   Output size: $128 \\times 4 \\times 4 = 2048$ features.\n",
    "\n",
    "5.  **`fc1`**: Fully Connected (Linear) Layer (`nn.Linear`)\n",
    "    *   `in_features=2048`: Accepts the flattened vector.\n",
    "    *   `out_features=250`: Transforms it into a 250-dimensional vector.\n",
    "    *   This is a hidden layer in the MLP part.\n",
    "    *   Operation: $Y = XW^T + b$.\n",
    "    *   Followed by `relu4`: ReLU activation.\n",
    "\n",
    "6.  **`fc2`**: Fully Connected (Linear) Layer (Output Layer)\n",
    "    *   `in_features=250`: Accepts the output from `fc1`.\n",
    "    *   `out_features=10`: Produces 10 output values (logits), one for each digit class.\n",
    "    *   No activation here, as loss functions like `nn.CrossEntropyLoss` typically expect raw logits.\n",
    "\n",
    "The `forward` method defines the data flow through these layers. The commented-out sections below the model would typically handle dataset preparation (`CNNDataset`, `DataLoader`), training setup (loss, optimizer), and the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68a1d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 1.------------------ CNN declaration -------------------\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# # 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9541aa5",
   "metadata": {},
   "source": [
    "### Extracting Pre-trained Weights\n",
    "\n",
    "To ensure our NumPy implementations of the CNN behave identically to a known, working model (at least for inference), we first load weights from a pre-trained PyTorch model (`simple_cnn_mnist.pth`). These weights are then converted to NumPy arrays.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1.  **Model Initialization and Loading State:**\n",
    "    *   An instance of `SimpleCNN` is created.\n",
    "    *   `model.load_state_dict(torch.load(...))`: This populates the model's layers with the learned parameters (weights and biases) from the saved file. `map_location=torch.device('cpu')` ensures the model is loaded onto the CPU, regardless of where it was trained.\n",
    "2.  **Evaluation Mode:**\n",
    "    *   `model.eval()`: This sets the model to evaluation mode. While our current model doesn't have layers like Dropout or BatchNorm that behave differently during training and inference, it's a good practice.\n",
    "3.  **Parameter Conversion:**\n",
    "    *   For each layer, weights (`.weight`) and biases (`.bias`) are accessed.\n",
    "    *   `.data.detach().numpy()`:\n",
    "        *   `.data`: Accesses the underlying tensor.\n",
    "        *   `.detach()`: Creates a new tensor that shares the same data but is detached from the computation graph (i.e., `requires_grad=False`). This is important when you don't need to track gradients.\n",
    "        *   `.numpy()`: Converts the PyTorch tensor to a NumPy array.\n",
    "\n",
    "**Shape Conventions and Transpositions:**\n",
    "\n",
    "*   **Convolutional Kernels (`k1`, `k2`, `k3`):**\n",
    "    *   PyTorch stores Conv2D weights as `(out_channels, in_channels, kernel_height, kernel_width)`.\n",
    "    *   Our NumPy implementation will use this same convention: `(num_filters, depth_input_channels, filter_height, filter_width)`. No transposition is needed here.\n",
    "    *   Biases (`b_conv1`, etc.) are `(out_channels,)`, which NumPy can broadcast correctly.\n",
    "\n",
    "*   **Fully Connected Weights (`w1`, `w2`):**\n",
    "    *   PyTorch `nn.Linear` layers store weights as `(out_features, in_features)`.\n",
    "    *   For a standard matrix multiplication $Y = XW + b$ in NumPy, if $X$ is `(batch_size, in_features)`, then $W$ needs to be `(in_features, out_features)`.\n",
    "    *   Therefore, the PyTorch weights are transposed: `numpy_weights['w1'] = pyt_w1.T`.\n",
    "    *   Biases (`b1`, `b2`) are reshaped from `(out_features,)` to `(1, out_features)` to allow direct broadcasting during addition in NumPy: `output_of_matmul + bias_reshaped`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5906ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 2, 2), NumPy Shape=(64, 32, 2, 2)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 2, 2), NumPy Shape=(128, 64, 2, 2)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pyt_k1_w = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pyt_k1_w\n",
    "\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pyt_k1_w.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pyt_k2_w = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pyt_k2_w\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pyt_k2_w.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pyt_k3_w = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pyt_k3_w\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pyt_k3_w.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pyt_w1 = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pyt_w1.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pyt_b1 = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pyt_b1.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pyt_w1.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pyt_b1.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pyt_w2 = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pyt_w2.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pyt_b2 = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pyt_b2.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pyt_w2.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pyt_b2.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']\n",
    "\n",
    "\n",
    "\n",
    "# [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]]\n",
    "\n",
    "\n",
    "#  [[[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]]\n",
    "\n",
    "\n",
    "\n",
    "#    [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]\n",
    "\n",
    "#   [[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba32882",
   "metadata": {},
   "source": [
    "## CNN - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1185e05",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b49fb",
   "metadata": {},
   "source": [
    "### Zero-Padding in Convolutions\n",
    "\n",
    "Zero-padding is the process of adding zeros around the border of an input image or feature map before applying a convolution. It serves several important purposes:\n",
    "\n",
    "1.  **Controlling Output Spatial Dimensions:**\n",
    "    Without padding, each convolution operation typically shrinks the output's height and width. Padding allows us to influence or maintain these dimensions. For example, \"same\" padding aims to make the output spatial dimensions equal to the input spatial dimensions (for stride 1).\n",
    "    The formula for an output dimension (e.g., height) is:\n",
    "    $$ O_H = \\left\\lfloor \\frac{I_H - K_H + 2P_H}{S_H} \\right\\rfloor + 1 $$\n",
    "    where:\n",
    "    *   $I_H$: Input height\n",
    "    *   $K_H$: Kernel height\n",
    "    *   $P_H$: Padding applied to height (on one side, so $2P_H$ is total padding)\n",
    "    *   $S_H$: Stride along height\n",
    "    *   $O_H$: Output height\n",
    "\n",
    "2.  **Improving Performance at Borders:**\n",
    "    Pixels at the edges and corners of an image are touched by the kernel fewer times than pixels in the center. Padding allows the kernel to be centered on these edge pixels, giving them more influence and potentially leading to better feature extraction at the boundaries.\n",
    "\n",
    "**`np.pad()` in NumPy:**\n",
    "The `np.pad()` function is used for padding. For a 4D tensor representing a batch of images (`BATCH, CHANNELS, HEIGHT, WIDTH`), the `pad_width` argument is a tuple of tuples, one for each dimension:\n",
    "`((pad_batch_before, pad_batch_after), (pad_channels_before, pad_channels_after), (pad_height_before, pad_height_after), (pad_width_before, pad_width_after))`\n",
    "To pad only the spatial dimensions (Height, Width) with `p` zeros on each side:\n",
    "`np.pad(image_batch, ((0,0), (0,0), (p,p), (p,p)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0599825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "img9 = np.arange(1,37).reshape(2,2,3,3)\n",
    "pad_img9 = np.pad(img9,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(img9)\n",
    "print(pad_img9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b7c53",
   "metadata": {},
   "source": [
    "### Delating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce9344",
   "metadata": {},
   "source": [
    "### Dilation (Interleaving Zeros)\n",
    "\n",
    "The `delateOne` function (conceptually, \"dilation\" or \"upsampling by inserting zeros\") modifies a matrix by inserting one row/column of zeros between existing rows/columns in its last two spatial dimensions (height and width).\n",
    "\n",
    "**Purpose in CNN Backpropagation:**\n",
    "\n",
    "This operation is particularly important when calculating the gradient with respect to the input of a **strided convolution** (a convolution with `stride > 1`).\n",
    "*   In the **forward pass**, a stride $S$ effectively downsamples the feature map.\n",
    "*   In the **backward pass**, to compute $\\frac{\\partial L}{\\partial X}$ (gradient of loss $L$ w.r.t. input $X$ of the strided convolution), we often convolve the gradient from the next layer $\\frac{\\partial L}{\\partial Z}$ (where $Z$ is the output of the strided convolution) with the rotated kernel $W_{rot180}$.\n",
    "*   However, since $Z$ was downsampled, $\\frac{\\partial L}{\\partial Z}$ has smaller spatial dimensions than $X$. To make the convolution dimensions work out correctly to produce $\\frac{\\partial L}{\\partial X}$ with the same shape as $X$, we need to \"upsample\" $\\frac{\\partial L}{\\partial Z}$ by inserting $S-1$ zeros between its elements along the strided dimensions.\n",
    "*   The `delateOne` function handles the case for $S=2$ by inserting $2-1=1$ zero.\n",
    "\n",
    "This process is a core component of what is often called a **transposed convolution** (or sometimes, misleadingly, deconvolution).\n",
    "\n",
    "**Example (1D):**\n",
    "If forward stride was 2, and output gradient is `[g1, g2, g3]`.\n",
    "Dilated gradient becomes `[g1, 0, g2, 0, g3]`. (Assuming `delateOne` doesn't add a trailing zero if input dim is odd for this example's simplicity).\n",
    "This dilated gradient is then convolved (with appropriate padding and kernel) to get the input gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ee6997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delateOne(matrix):\n",
    "    indix = np.arange(1,matrix.shape[3])\n",
    "    matrix = np.insert(matrix,indix,0,3)\n",
    "    indix = np.arange(-(matrix.shape[-2]-1),0)\n",
    "    matrix = np.insert(matrix,indix,0,-2)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d1319",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cbee5",
   "metadata": {},
   "source": [
    "### \"Slow\" Convolution Forward Pass (Explicit Loops)\n",
    "\n",
    "The `Slow_ReLU_Conv` function implements the forward pass of a 2D convolution layer followed by a ReLU activation using explicit, nested Python loops. This approach is:\n",
    "*   **Intuitive:** Clearly shows the mechanics of how a filter slides over an input.\n",
    "*   **Inefficient:** Python loops are slow for numerical computations compared to vectorized operations or optimized library functions.\n",
    "\n",
    "**Mathematical Process:**\n",
    "\n",
    "1.  **Input and Kernel:**\n",
    "    *   Input `img`: A batch of images, typically 4D `(N, C_in, H_in, W_in)`.\n",
    "    *   Kernel `ker`: A set of filters, 4D `(C_out, C_in, K_H, K_W)`.\n",
    "        *   `C_out`: Number of output channels (number of filters).\n",
    "        *   `C_in`: Number of input channels (must match `img`).\n",
    "        *   `K_H, K_W`: Kernel height and width.\n",
    "    *   Bias `bias`: A 1D array of size `C_out`, one bias term per output filter.\n",
    "\n",
    "2.  **Padding:** The input `img` is padded based on the `pad` parameter.\n",
    "\n",
    "3.  **Output Dimensions:** The spatial dimensions of the output feature map are calculated:\n",
    "    $$ O_H = \\left\\lfloor \\frac{H_{in} - K_H + 2P}{S} \\right\\rfloor + 1 $$\n",
    "    $$ O_W = \\left\\lfloor \\frac{W_{in} - K_W + 2P}{S} \\right\\rfloor + 1 $$\n",
    "    Where $P$ is the padding and $S$ is the stride.\n",
    "\n",
    "4.  **Convolution Operation (per output pixel):**\n",
    "    For each image $n$ in the batch, each output filter $f$, and each output spatial location $(y_{out}, x_{out})$:\n",
    "    $$ \\text{Output}(n, f, y_{out}, x_{out}) = \\left( \\sum_{c=0}^{C_{in}-1} \\sum_{k_y=0}^{K_H-1} \\sum_{k_x=0}^{K_W-1} \\text{Input}_{padded}(n, c, y_{start} + k_y, x_{start} + k_x) \\cdot \\text{Kernel}(f, c, k_y, k_x) \\right) + \\text{Bias}(f) $$\n",
    "    Where:\n",
    "    *   $y_{start} = y_{out} \\cdot S$\n",
    "    *   $x_{start} = x_{out} \\cdot S$\n",
    "    The loops iterate through `n_images`, `nk_channel` (output filters), `i_nih` ($y_{out}$), `i_niw` ($x_{out}$), `channel` ($c$), `i_kh` ($k_y$), and `i_kw` ($k_x$).\n",
    "\n",
    "5.  **ReLU Activation:**\n",
    "    If `applyReLU` is true, the Rectified Linear Unit is applied element-wise to the result of the convolution plus bias:\n",
    "    $$ \\text{Activated\\_Output} = \\max(0, \\text{Output}) $$\n",
    "    A `mask` is also generated: `mask = 1` if `Output > 0`, and `0` otherwise. This mask is crucial for the backward pass of the ReLU.\n",
    "\n",
    "---\n",
    "**Suggested Visual Aid:**\n",
    "A GIF animating a 2D convolution is highly recommended here. It should show a kernel sliding over an input, the element-wise multiplications, and the summation to produce one output pixel. Different colors for input channels and kernel slices can illustrate multi-channel convolution.\n",
    "\n",
    "*(Example: Search \"2D convolution animation gif\")*\n",
    "<img src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides.gif\" alt=\"Convolution Animation\" width=\"300\"/>\n",
    "*(This GIF shows stride 1. An ideal one would also illustrate stride and padding effects.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6c26aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[ 1.  2.  3.  4.]\n",
      "   [ 5.  6.  7.  8.]\n",
      "   [ 9. 10. 11. 12.]]]]\n",
      "-------ker-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[  5.  19.  13.]\n",
      "   [ 47.  95.  45.]]\n",
      "\n",
      "  [[ 10.  40.  30.]\n",
      "   [104. 232. 126.]]]]\n",
      "-------Conv PyTorch-------\n",
      "tensor([[[[  5.,  19.,  13.],\n",
      "          [ 47.,  95.,  45.]],\n",
      "\n",
      "         [[ 10.,  40.,  30.],\n",
      "          [104., 232., 126.]]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This is a PyTorch Convolution example to be used to check if the convolution implemented in both slow and fast approaches are correct\n",
    "\n",
    "class CustomConv(nn.Module):\n",
    "    def __init__(self, kernel: torch.Tensor, bias: torch.Tensor = None, \n",
    "                 stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernel.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(bias is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernel)\n",
    "            if bias is not None:\n",
    "                self.conv.bias.copy_(bias)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if bias is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "def Slow_ReLU_Conv(img,ker,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        out_ch, in_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = out_ch\n",
    "    else: # Backward case\n",
    "        in_ch, out_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = in_ch\n",
    "\n",
    "    # bias has shape out_ch, 1, 1. It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # number of channels taken in input by the kernel 'in_ch' \n",
    "    # must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    n_images, channels, i_height, i_width  = img.shape\n",
    "    ni_height = int(((i_height - k_height) / stride) + 1) # new image height # Padding is already added\n",
    "    ni_width = int(((i_width - k_width) / stride) + 1) # new image width\n",
    "    ni = np.zeros((n_images, out_ch, ni_height, ni_width)).astype(np.float32) # new image\n",
    "\n",
    "    if in_ch != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({in_ch}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for one_img in range(n_images):\n",
    "        for one_k_channel in range(nk_channel):\n",
    "            for i_nih in range(ni_height): # which cycles row by row of the new image\n",
    "                for i_niw in range(ni_width): # which cycles column by column of the new image\n",
    "                    current_sum = 0.0 # convolution sum for the specific output cell\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(channels): # channels == in_ch\n",
    "                        for i_kh in range(k_height):\n",
    "                            input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "                            for i_kw in range(k_width):\n",
    "                                input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "                                    input_val = img[one_img, channel, input_y, input_x]\n",
    "                                    kernel_val = ker[one_k_channel, channel, i_kh, i_kw]\n",
    "                                    current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "                    ni[one_img, one_k_channel, i_nih, i_niw] = current_sum\n",
    "    if bias.all() != 0:\n",
    "        bias = bias.reshape(bias.shape[0],1,1)\n",
    "        if bias.shape[0] != out_ch:\n",
    "            raise ValueError(f\"bias dimension ({bias.shape[0]}) doesn't match kernel's number of channels ({out_ch})\")\n",
    "        ni = ni + bias\n",
    "    ni = ni.astype(np.float32)\n",
    "    if applyReLU:\n",
    "        ni = np.maximum(0, ni)\n",
    "        mask = ni.copy()\n",
    "        mask[mask > 0] = 1\n",
    "        return ni,mask\n",
    "    else:\n",
    "        return ni\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "img = np.arange(1,3*4+1).reshape(1,1,3,4).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=1,stride=2)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "\n",
    "my_kernel = torch.from_numpy(ker).float()\n",
    "\n",
    "my_bias = torch.from_numpy(np.array([1,2])).float()\n",
    "\n",
    "modelC = CustomConv(kernel=my_kernel,bias=my_bias, stride=2, padding=1)\n",
    "\n",
    "# input di prova (batch=1, canali=1, H=5, W=5)\n",
    "x = torch.from_numpy(img)\n",
    "y = modelC(x)\n",
    "print(\"-------Conv PyTorch-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec0a01",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805b797",
   "metadata": {},
   "source": [
    "**Actors:**\n",
    "1. W is the kernel\n",
    "2. $\\delta$ is the gradient\n",
    "3. x is the input to the convolution layer during forward\n",
    "4. b is the bias\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Derive delta**\n",
    "\n",
    "Deriving delta with respect to ReLU activation consists in the hadamard product (element-wise product) of the gradient ($\\delta$) and the mask obtained at the forward step, that is, all the elements in the convolved image greater than zero are put to one, the rest is zero.\n",
    "$$\n",
    "\\delta^{(i)} = \\delta_{\\text{flat reshaped}} \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "- **Gradient with respect to W**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(x^{(i)}, \\delta)\n",
    "$$\n",
    "This convolution creates a matrix for every channel of input image $x^{i}$ and for every channel of output image $\\delta$, thus resulting in the correct number of channels\n",
    "\n",
    "- **Gradient w.r.t. the input \\( x \\)** (To go to the preceding layer):\n",
    "\n",
    "$$\n",
    "\\delta^{(i-1)} = \\text{Full\\_Convolution}(\\delta^{(i)}, W^{(i)})\n",
    "$$\n",
    "\n",
    "- **Gradient w.r.t the bias**\n",
    "\n",
    "Since the bias is added equally across the spatial dimensions of each output channel, the gradient is the sum of all elements in each output channel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{h,w} \\delta^{(i)}_{c,h,w}\n",
    "$$\n",
    "\n",
    "For batched inputs, sum also across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n,h,w} \\delta^{(i)}_{n,c,h,w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342693d0",
   "metadata": {},
   "source": [
    "### \"Slow\" Convolution Backward Pass (Explicit Loops)\n",
    "\n",
    "The `Slow_ReLU_Gradient` function computes the gradients for the \"slow\" convolutional layer (and its preceding ReLU activation) with respect to its inputs ($X$), weights ($W$, the kernel), and biases ($b$). These gradients are essential for updating the model parameters during training via backpropagation.\n",
    "\n",
    "Let $L$ be the loss function. We are given $\\frac{\\partial L}{\\partial A}$ (the gradient of the loss with respect to the output $A$ of the ReLU activation, denoted `d_img` in the code). We need to compute:\n",
    "*   $\\frac{\\partial L}{\\partial X}$: Gradient to pass to the previous layer.\n",
    "*   $\\frac{\\partial L}{\\partial W}$: Gradient to update the kernel weights.\n",
    "*   $\\frac{\\partial L}{\\partial b}$: Gradient to update the biases.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Backward ReLU:**\n",
    "    The gradient flows back through the ReLU activation first. If $A = \\text{ReLU}(Z)$ where $Z = X*W+b$, then:\n",
    "    $$ \\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\frac{dA}{dZ} $$\n",
    "    The derivative $\\frac{dA}{dZ}$ is 1 if $Z > 0$ and 0 otherwise. This is exactly what the `mask` (computed during the forward pass) represents.\n",
    "    In code: `d_img_activated_grad = np.multiply(d_img, mask)` (where `d_img` is $\\frac{\\partial L}{\\partial A}$). Let's call this `dL_dZ`.\n",
    "\n",
    "2.  **Gradient with respect to Bias ($\\frac{\\partial L}{\\partial b}$), denoted `gb`:**\n",
    "    The bias $b_f$ for filter $f$ is added to all spatial locations of the $f$-th output channel of $Z$. Thus, its gradient is the sum of the gradients $\\frac{\\partial L}{\\partial Z}$ across all spatial locations (height, width) and batch elements for that channel:\n",
    "    $$ \\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} \\left(\\frac{\\partial L}{\\partial Z}\\right)_{n,f,h,w} $$\n",
    "    In code: `gb = d_img_activated_grad.sum(axis=(0, 2, 3))`\n",
    "\n",
    "3.  **Gradient with respect to Weights/Kernel ($\\frac{\\partial L}{\\partial W}$), denoted `gk`:**\n",
    "    The gradient for a specific weight $W_{f,c,ky,kx}$ (filter $f$, input channel $c$, kernel row $ky$, kernel col $kx$) is found by convolving the original padded input $X_{padded}$ with the (potentially dilated) gradient $\\frac{\\partial L}{\\partial Z}$.\n",
    "    $$ \\frac{\\partial L}{\\partial W_{f,c,ky,kx}} = \\sum_{n, y_{out}, x_{out}} X_{padded}(n, c, y_{out}S + ky, x_{out}S + kx) \\cdot \\left(\\frac{\\partial L}{\\partial Z}\\right)_{n,f,y_{out},x_{out}} $$\n",
    "    (If forward stride $S > 1$, $\\frac{\\partial L}{\\partial Z}$ should be used directly without dilation here, as the sum iterates over the output coordinates of $\\frac{\\partial L}{\\partial Z}$. The dilation of $\\frac{\\partial L}{\\partial Z}$ is for $\\frac{\\partial L}{\\partial X}$.)\n",
    "    The code iterates to fill each element of `gk` using nested loops, effectively performing this convolution. The `pad` argument for `img_padded_fwd` is the original forward pass padding.\n",
    "\n",
    "4.  **Gradient with respect to Input ($\\frac{\\partial L}{\\partial X}$), denoted `gi`:**\n",
    "    This involves a \"full\" convolution (often implemented as a regular convolution with specific padding and a 180-degree rotated kernel) of the gradient $\\frac{\\partial L}{\\partial Z}$ with the kernel $W$.\n",
    "    *   **Dilation of $\\frac{\\partial L}{\\partial Z}$**: If the forward pass used `stride > 1` (e.g., $S=2$), $\\frac{\\partial L}{\\partial Z}$ must be dilated by inserting $S-1$ zeros between its elements (handled by `delateOne`). Let this be $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}$.\n",
    "    *   **Kernel Rotation**: The kernel $W$ is rotated by 180 degrees ($W_{rot180}$).\n",
    "    *   **Padding $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}$**: To ensure `gi` has the same shape as the original $X$ (before forward padding), $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}$ needs specific padding. A common formula is $P_{bwd} = K_H - 1 - P_{fwd}$.\n",
    "    The convolution is then:\n",
    "    $$ \\frac{\\partial L}{\\partial X} = \\text{Conv}\\left(\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated\\_padded}, W_{rot180}, \\text{stride}=1\\right) $$\n",
    "    The code implements this with nested loops, convolving each channel of $W_{rot180}$ (where input channels of $W$ become output channels for this operation, and output channels of $W$ become input channels) with the corresponding channels of $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated\\_padded}$.\n",
    "\n",
    "The \"NEW APPROACH\" comment in the code refers to the direct loop-based computation of these convolution operations for the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5de649af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imAge: (1, 1, 7, 7)\n",
      "kerNel: (2, 1, 2, 2)\n",
      "dimAge: (1, 2, 4, 4)\n",
      "ggi: (1, 1, 7, 7)\n",
      "ggk: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "def Slow_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "    ############################################# Gradient of Input Image ####################################\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image delated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel 180 degrees rotation (flipped vertically and then horizontally)\n",
    "    # FullConvolution(d_imgDelated, Rotated180Deg(kernel)) with stride 1\n",
    "    out_ch, in_ch, k_height, k_width = ker.shape\n",
    "    batch_s, in_ch, img_height, img_width = img.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "\n",
    "    # Delating the gradient of output\n",
    "    if stride == 2:\n",
    "        d_img = delateOne(d_img)\n",
    "    elif stride > 2:\n",
    "        raise ValueError(f\"Stride greater than 2 is not acceptable\")\n",
    "    d_imgPadded = np.pad(d_img,((0,0),(0,0),(k_height-1-pad,k_height-1-pad),(k_width-1-pad,k_width-1-pad)))\n",
    "    batch_s, out_ch, dimg_height, dimg_width = d_img.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    ker180 = np.rot90(ker,2,(-2,-1))\n",
    "\n",
    "    # Computation\n",
    "    gi = np.zeros_like(img)\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(img_height):\n",
    "            for i_giw in range(img_width):\n",
    "                for i_outch in range(out_ch):\n",
    "                    for i_inch in range(in_ch):\n",
    "                        for i_kh in range(k_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(k_width):\n",
    "                                x = i_gih + i_kw\n",
    "\n",
    "                                if 0 <= y < d_imgPadded.shape[-2] and 0 <= x < d_imgPadded.shape[-1]:\n",
    "                                    input_val = d_imgPadded[bs,i_outch,y,x]\n",
    "                                    ker_val = ker180[i_outch,i_inch,i_kh,i_kw] \n",
    "                                else:\n",
    "                                    break\n",
    "                                current_sum += input_val*ker_val\n",
    "                    gi[bs,i_inch,i_gih,i_giw] = current_sum\n",
    "                    current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the delated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    gk = np.zeros_like(ker)\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(k_height):\n",
    "            for i_giw in range(k_width):\n",
    "                for i_inch in range(in_ch):\n",
    "                    for i_outch in range(out_ch):\n",
    "                        for i_kh in range(dimg_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(dimg_width):\n",
    "                                x = i_gih + i_kw\n",
    "                                if 0 <= y < img_height and 0 <= x < img_width:\n",
    "                                    input_val = img[bs,i_inch,y,x]\n",
    "                                    ker_val = d_img[bs,i_outch,i_kh,i_kw] \n",
    "                                    current_sum += input_val*ker_val\n",
    "                                else:\n",
    "                                    break\n",
    "                        gk[i_outch,i_inch,i_gih,i_giw] = current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gb = d_img.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "in_ch = 1\n",
    "out_ch = 2\n",
    "idim = 7\n",
    "kdim = 2\n",
    "s = 2\n",
    "p = 1\n",
    "imAge = np.arange(1,1*in_ch*idim*idim+1).reshape(1,in_ch,idim,idim)\n",
    "kerNel = np.arange(1,out_ch*in_ch*(kdim**2)+1).reshape(out_ch,in_ch,kdim,kdim)\n",
    "dimAge,mask = Slow_ReLU_Conv(imAge,kerNel,stride=s,pad=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "ggi,ggk,ggb = Slow_ReLU_Gradient(imAge,dimAge,kerNel,mask,stride=s,pad=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggk: {ggk.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02f87035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# OLD APPROACH ######################\n",
    "# def Slow_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "#     \"\"\"\n",
    "#     Performs the backward pass of the convolution layer. It takes the original image, \n",
    "#     the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "#     It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "#     \"\"\" \n",
    "\n",
    "#     out_ch, in_ch, k_height, k_width  = ker.shape \n",
    "#                                             # Example #\n",
    "#     # Convolving an RGB image with 32 2x2 kernels will give a shape of (32, 3, 2, 2) to the kernel. #\n",
    "    \n",
    "#     n_images, channels, i_height, i_width  = img.shape\n",
    "#     n_images, dch, di_height, di_width  = d_img.shape\n",
    "    \n",
    "#     ni_height = (i_height-1)*stride-(2*pad)+k_height # new image height\n",
    "#     ni_width =  (i_width-1)*stride-(2*pad)+k_width # new image width\n",
    "#     height_to_pad = ni_height-i_height\n",
    "#     width_to_pad = ni_width-i_width\n",
    "#     d_img = np.multiply(d_img,mask)\n",
    "#     d_imgP = np.pad(d_img,((0,0),(0,0),(height_to_pad,height_to_pad),(width_to_pad,width_to_pad)))\n",
    "#     gi = np.zeros_like(img).astype(np.float32) # gradient of original image\n",
    "#     gk = np.zeros_like(ker).astype(np.float32) # gradient of kernel\n",
    "\n",
    "# ############################## Computing the gradient of the original image ######################################\n",
    "#     current_sum = 0.0 # convolution sum for the specific output cell\n",
    "#     for one_img in range(n_images):\n",
    "#         for channel in range(channels):\n",
    "#             for i_nih in range(i_height): # which cycles row by row of the new image\n",
    "#                 for i_niw in range(i_width): # which cycles column by column of the new image\n",
    "#                     # Convolution cycles\n",
    "#                     for one_k_channel in range(out_ch): # channels == out_ch\n",
    "#                         for i_kh in range(k_height):\n",
    "#                             input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "#                             for i_kw in range(k_width):\n",
    "#                                 input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "#                                 # check that everything stays in the measures\n",
    "#                                 if 0 <= input_y < d_imgP.shape[2] and 0 <= input_x < d_imgP.shape[3]:\n",
    "#                                     input_val = d_imgP[one_img, one_k_channel, input_y, input_x]\n",
    "#                                     kernel_val = ker[one_k_channel,channel, i_kh, i_kw]\n",
    "#                                     current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "#                     gi[one_img, channel, i_nih, i_niw] = current_sum\n",
    "#                     current_sum = 0.0\n",
    "    \n",
    "# ############################## Computing the gradient of the kernel ##############################################\n",
    "# # Need to convolve the gradient of the image with the original image, using the gradient of the image as the kernel and \n",
    "# # keeping the same stride and padding (Otherwise the kernel won't work)\n",
    "#     current_sum = 0.0\n",
    "#     for one_img in range(n_images):\n",
    "#         for in_k_ch in range(in_ch): # which in the example is 3\n",
    "#             for out_k_ch in range(out_ch): # which in the example is 32\n",
    "#                 for k_gh in range(k_height):\n",
    "#                     for k_gw in range(k_width):\n",
    "#                     # gk[out_k_ch,in_k_ch,k_gh,k_gw] = something\n",
    "#                         for i_dh in range(di_height):\n",
    "#                             input_y = (k_gh * stride) + i_dh # get the y location, the height\n",
    "#                             for i_dw in range(di_width):\n",
    "#                                 input_x = (k_gw * stride) + i_dw\n",
    "#                                 # check that everything stays in the measures\n",
    "#                                 if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "#                                     input_val = img[one_img, in_k_ch, input_y, input_x]\n",
    "#                                     kernel_val = d_img[one_img, out_k_ch, i_dh, i_dw]\n",
    "#                                     current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "#                         gk[out_k_ch, in_k_ch, k_gh, k_gw] += current_sum\n",
    "\n",
    "# ############################## Computing the gradient of the bias ##############################################\n",
    "#     gb = d_img.sum((0,-1,-2)) # sum over batch, height and width\n",
    "# ################################################### Return Results ###############################################\n",
    "#     return gi,gk,gb\n",
    "\n",
    "# img = np.arange(1,17).reshape(1,1,4,4)\n",
    "# ker = np.arange(1,9).reshape(2,1,2,2)\n",
    "# bias = np.array([1,1])\n",
    "# d_img,mask=Slow_ReLU_Conv(img,ker,bias)\n",
    "# print(\"-------------d_img--------------\")\n",
    "# d_img = d_img - 2\n",
    "# print(d_img)\n",
    "# print(d_img.shape)\n",
    "# print(\"--------------------------------\")\n",
    "# a,b,c = Slow_ReLU_Gradient(img,d_img,ker,mask)\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876559ef",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50be7c5",
   "metadata": {},
   "source": [
    "### \"Fast\" Convolution Forward Pass (Im2Col Approach)\n",
    "\n",
    "The `Fast_ReLU_Conv` function aims to perform convolution more efficiently than explicit loops by leveraging matrix multiplication. This is typically achieved using a technique called **Im2Col** (Image to Columns).\n",
    "\n",
    "**The Im2Col Concept:**\n",
    "\n",
    "1.  **Input Transformation ($X \\rightarrow X_{col}$):**\n",
    "    Relevant patches (receptive fields) are extracted from the input image(s) `batch_of_images`. Each patch that a filter would cover is flattened into a column vector. These column vectors are stacked side-by-side to form a large matrix $X_{col}$.\n",
    "    *   If input is `(N, C_in, H_in, W_in)` and kernel is `(K_H, K_W)`, each patch is `(C_in, K_H, K_W)`, flattened to a vector of size $C_{in}K_H K_W$.\n",
    "    *   $X_{col}$ has shape `(C_in * K_H * K_W, N * O_H * O_W)`, where $O_H, O_W$ are output height/width. (Note: conventions for $X_{col}$ rows/columns can vary; another is `(N*O_H*O_W, C_in*K_H*K_W)`).\n",
    "\n",
    "2.  **Kernel Transformation ($W \\rightarrow W_{row}$ or $W_{col}$):**\n",
    "    The filters (kernels) are also unrolled. Each filter `(C_in, K_H, K_W)` is flattened into a row vector (or column). These are stacked to form a matrix $W_{row}$.\n",
    "    *   $W_{row}$ has shape `(C_out, C_in * K_H * K_W)`.\n",
    "\n",
    "3.  **Convolution as Matrix Multiplication:**\n",
    "    The convolution operation over all patches and all filters can now be performed as a single matrix multiplication:\n",
    "    $$ Z_{flat} = W_{row} \\cdot X_{col} $$\n",
    "    The result $Z_{flat}$ is a matrix of shape `(C_out, N * O_H * O_W)`.\n",
    "    (If $X_{col}$ is `(N*O_H*O_W, C_in*K_H*K_W)` and $W_{col}$ is `(C_in*K_H*K_W, C_out)`, then $Z_{flat} = X_{col} \\cdot W_{col}$, shape `(N*O_H*O_W, C_out)`).\n",
    "\n",
    "4.  **Output Reshaping (Col2Im concept):**\n",
    "    The flattened output $Z_{flat}$ is then reshaped back to the desired 4D output tensor: `(N, C_out, O_H, O_W)`.\n",
    "\n",
    "**Implementation Details in `Fast_ReLU_Conv`:**\n",
    "\n",
    "*   The code uses `np.lib.stride_tricks.as_strided` (or `sliding_window_view` in the original version which is similar but might have different internal mechanics for creating views) to efficiently create the views of input patches without explicit copying, which forms the basis of $X_{col}$.\n",
    "*   `X_col = ... .reshape(bs * nih * niw, ac * kh * kw)`: This creates $X_{col}$ with shape `(num_total_patches, patch_vector_size)`.\n",
    "*   `kernel_reshaped = kernel.reshape(kc, ac * kh * kw).T`: This creates $W_{col}$ with shape `(patch_vector_size, num_output_channels)`.\n",
    "*   `c_m = (X_col @ kernel_reshaped)`: Performs the matrix multiplication.\n",
    "*   The subsequent `reshape` and `transpose` operations effectively perform the \"Col2Im\" step.\n",
    "*   Bias addition and ReLU activation (with mask generation) are applied as in the slow version.\n",
    "\n",
    "This approach is \"fast\" because matrix multiplication is highly optimized in libraries like NumPy (which often use underlying BLAS/LAPACK routines).\n",
    "\n",
    "---\n",
    "**Suggested Visual Aid:**\n",
    "A GIF or diagram clearly showing:\n",
    "1. Input feature map.\n",
    "2. Kernel.\n",
    "3. Extraction of input patches and their flattening into columns ($X_{col}$).\n",
    "4. Flattening of kernels into rows/columns ($W_{row}$ or $W_{col}$).\n",
    "5. The matrix multiplication.\n",
    "6. Reshaping the result back into an output feature map.\n",
    "\n",
    "*(Example: Search \"im2col convolution gif\")*\n",
    "<img src=\"https://leonardoaraujosantos.gitbook.io/assets/ConvolutionArithmetic/im2col.gif\" alt=\"Im2Col Animation\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3469f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Conv Slow-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[  85.  170.  192.   94.]\n",
      "   [ 183.  357.  393.  187.]\n",
      "   [ 243.  465.  501.  235.]\n",
      "   [ 111.  206.  220.  100.]]\n",
      "\n",
      "  [[ 174.  363.  417.  215.]\n",
      "   [ 408.  838.  938.  476.]\n",
      "   [ 564. 1138. 1238.  620.]\n",
      "   [ 296.  591.  637.  317.]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[15011. 20885. 16057.]\n",
      "   [23607. 32231. 24383.]\n",
      "   [18779. 25317. 18937.]]\n",
      "\n",
      "  [[35636. 50230. 39354.]\n",
      "   [57176. 79176. 61088.]\n",
      "   [47692. 65286. 49882.]]]]\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Conv(batch_of_images,kernel,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    kc, ac, kw, kh = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    # im2col: Window creation\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    window_m = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))[:,:,::stride,::stride].reshape((-1,(kw*kh*nc))) # window matrix\n",
    "    # Convolution\n",
    "    kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)\n",
    "    c_m = (window_m @ kernel).astype(np.float32) # convolved image matrix\n",
    "    # ReLU activation\n",
    "    nih = int(((ih-kh) / stride) + 1) # new image height # Padding is already added\n",
    "    niw = int(((iw-kw) / stride) + 1) # new image width\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output_temp = c_m.reshape(bs, nih, niw, kc)\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    reshaped_correct_order = output_temp.transpose(0,3,1,2).astype(np.float32)\n",
    "    if bias.any() != 0:\n",
    "        reshaped_correct_order = (reshaped_correct_order + bias.reshape(1,-1,1,1))\n",
    "    if applyReLU:\n",
    "        reshaped_correct_order = np.maximum(0,reshaped_correct_order)\n",
    "    mask = np.copy(reshaped_correct_order)\n",
    "    mask[mask>0]=1\n",
    "    return reshaped_correct_order,mask\n",
    "\n",
    "\n",
    "\n",
    "img = np.arange(1,2*3*3+1).reshape(1,2,3,3).astype(np.float32)\n",
    "# print(\"-------img-------\")\n",
    "# print(img)\n",
    "ker = np.arange(1,16+1).reshape(2,2,2,2)\n",
    "# print(\"-------ker-------\")\n",
    "# print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(img,ker,bias,pad = 1,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)\n",
    "res,mask = Slow_ReLU_Conv(res,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(X_c,ker,bias,pad = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91cb44",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac3b4b",
   "metadata": {},
   "source": [
    "### \"Fast\" Convolution Backward Pass (Im2Col-based Gradients)\n",
    "\n",
    "The `Fast_ReLU_Gradient` function computes gradients for the \"fast\" convolutional layer, leveraging properties of the Im2Col transformation used in its forward pass. As with the forward pass, this relies on efficient matrix operations.\n",
    "\n",
    "Let $X_{col}$ be the Im2Col version of the input $X$, and $W_{col}$ be the reshaped kernel. The forward output (before reshaping and activation) is $Z_{col} = X_{col} W_{col}$ (assuming this convention). We are given $\\frac{\\partial L}{\\partial A}$ (`d_image_incoming`).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Backward ReLU:** Same as before.\n",
    "    $$ \\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\text{mask} $$\n",
    "    Let `dL_dZ` be this result. We'll need its \"column\" form, $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{col}$, which is `dL_dZ` reshaped to `(bs * dh_dL * dw_dL, out_ch_W)`.\n",
    "\n",
    "2.  **Gradient with respect to Bias ($\\frac{\\partial L}{\\partial b}$), denoted `gb`:**\n",
    "    Identical to the slow version: sum `dL_dZ` over batch, height, and width for each output channel.\n",
    "    $$ \\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} \\left(\\frac{\\partial L}{\\partial Z}\\right)_{n,f,h,w} $$\n",
    "\n",
    "3.  **Gradient with respect to Weights/Kernel ($\\frac{\\partial L}{\\partial W}$), denoted `gk`:**\n",
    "    Using the Im2Col formulation $Z_{col} = X_{col} W_{col}$:\n",
    "    $$ \\frac{\\partial L}{\\partial W_{col}} = X_{col}^T \\left(\\frac{\\partial L}{\\partial Z}\\right)_{col} $$\n",
    "    The result $\\frac{\\partial L}{\\partial W_{col}}$ (shape `(in_ch*kh*kw, out_ch)`) is then reshaped back to the original kernel shape `(out_ch, in_ch, kh, kw)`.\n",
    "    The provided code attempts to calculate `gk` using explicit loops similar to `Slow_ReLU_Gradient` (summing contributions from each batch item). This is a valid way to compute it, but a fully \"fast\" Im2Col backward pass would use the matrix multiplication above. *This highlights a difference in the \"fast\" backward implementation for `gk` compared to a pure Im2Col strategy.*\n",
    "\n",
    "4.  **Gradient with respect to Input ($\\frac{\\partial L}{\\partial X}$), denoted `gi`:**\n",
    "    Again, from $Z_{col} = X_{col} W_{col}$:\n",
    "    $$ \\frac{\\partial L}{\\partial X_{col}} = \\left(\\frac{\\partial L}{\\partial Z}\\right)_{col} W_{col}^T $$\n",
    "    The result $\\frac{\\partial L}{\\partial X_{col}}$ (shape `(bs*dh*dw, in_ch*kh*kw)`) is a matrix where each row is the gradient for a flattened input patch. This needs to be transformed back to the original input image shape `(bs, in_ch, i_height, i_width)` using a **Col2Im** operation. Col2Im involves placing these patch gradients back into their original locations in the input image and summing contributions where patches overlapped.\n",
    "\n",
    "    The provided code cleverly reuses the `Fast_ReLU_Conv` function to compute `gi`. This is a common technique, as the computation for $\\frac{\\partial L}{\\partial X}$ is mathematically equivalent to a forward convolution (transposed convolution) of $\\frac{\\partial L}{\\partial Z}$ (appropriately padded and dilated if forward stride > 1) with the kernel $W$ rotated/transposed appropriately.\n",
    "    *   `dL_dZ_dilated_padded`: The gradient $\\frac{\\partial L}{\\partial Z}$ is dilated (if original stride > 1) and padded.\n",
    "    *   `W_for_gi = W_rot180.transpose(1,0,2,3)`: The original kernel $W$ is rotated and its input/output channel dimensions are swapped to serve as the kernel for this backward convolution.\n",
    "    *   `gi, _ = Fast_ReLU_Conv(dL_dZ_dilated_padded, W_for_gi, ...)`: This performs the transposed convolution.\n",
    "    *   Cropping `gi`: The \"full\" nature of transposed convolution might result in `gi` being larger than the original input `X`. If so, it's cropped to the correct size.\n",
    "\n",
    "This mixed approach for `Fast_ReLU_Gradient` (Im2Col-like for `gi` via `Fast_ReLU_Conv`, loop-based for `gk`) demonstrates different strategies for implementing gradient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b7be149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------img--------------\n",
      "(1, 3, 28, 28)\n",
      "-------------ker--------------\n",
      "(32, 3, 3, 3)\n",
      "################################\n",
      "-------------d_img--------------\n",
      "(1, 32, 14, 14)\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Gradient(batch_of_images,d_image,kernel,mask,pad=0,stride=1):\n",
    "    out_ch, in_ch, kh, kw = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    bs, nc, i_height,i_width = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "\n",
    "    batchSize, out_ch, dh, dw = d_image.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    ni_height = int(((i_height-1)*stride)+kh) # new image height\n",
    "    ni_width =  int(((i_width-1)*stride)+kw) # new image width\n",
    "    height_to_pad = (ni_height-dh)\n",
    "    width_to_pad = (ni_width-dw)\n",
    "\n",
    "    half_htp = height_to_pad//2\n",
    "    half_wtp = width_to_pad//2\n",
    "\n",
    "    d_image = np.multiply(d_image,mask)\n",
    "    d_imgP = np.pad(d_image,((0,0),(0,0),(half_htp,half_htp),(half_wtp,half_wtp)))\n",
    "\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    \n",
    "    ############################## Computing the gradient of the bias ##############################################\n",
    "    gb = d_image.sum((0,-1,-2)) # sum over batch, height and width\n",
    "\n",
    "    ########################################## Gradient of Kernel ###################################################\n",
    "    window_boi = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,1,dh,dw))[:,:,::stride,::stride].reshape((-1,(dw*dh*1))) # window matrix\n",
    "    d_image = d_image.reshape((-1,(dw*dh*1))).transpose(1,0)\n",
    "    gk = (window_boi @ d_image).transpose(1,0).reshape(out_ch, in_ch, kh, kw,).astype(np.float32) # convolved image matrix\n",
    "\n",
    "    ########################################## Gradient of Image ###################################################\n",
    "    gi,_ = Fast_ReLU_Conv(d_imgP,kernel.transpose(1,0,2,3),stride = stride,pad=pad,applyReLU=False)\n",
    "    # window_dboi = np.lib.stride_tricks.sliding_window_view(d_imgP,(1,out_ch,kh,kw))[:,:,::stride,::stride].reshape((-1,(kw*kh*out_ch))) # window matrix\n",
    "    # kernel = kernel.reshape((-1,(kw*kh*out_ch))).transpose(1,0)\n",
    "    # gi = (window_dboi @ kernel).reshape(bs, i_height, i_width, nc).transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "s = 2\n",
    "p = 1\n",
    "in_ch = 3\n",
    "out_ch = 32\n",
    "i_dim = 28\n",
    "k_dim = 3\n",
    "img = np.arange(1,i_dim*in_ch*i_dim+1).reshape(1,in_ch,i_dim,i_dim)\n",
    "ker = np.arange(1,out_ch*k_dim*in_ch*k_dim+1).reshape(out_ch,in_ch,k_dim,k_dim)\n",
    "bias = np.ones(out_ch)\n",
    "d_img,mask = Fast_ReLU_Conv(img,ker,bias,stride=s,pad=p)\n",
    "\n",
    "print(\"-------------img--------------\")\n",
    "#print(img)\n",
    "print(img.shape)\n",
    "print(\"-------------ker--------------\")\n",
    "#print(ker)\n",
    "print(ker.shape)\n",
    "print(\"################################\")\n",
    "print(\"-------------d_img--------------\")\n",
    "#print(d_img-2)\n",
    "print(d_img.shape)\n",
    "\n",
    "print(\"************************************\")\n",
    "# a,b,c = Fast_ReLU_Gradient(img,d_img-2,ker,mask,stride=s,pad=p)\n",
    "# print(\"-------------gi-----------------\")\n",
    "# print(a.shape)\n",
    "# print(\"--------------gk-----------------\")\n",
    "# print(b.shape)\n",
    "# print(\"--------------gb-----------------\")\n",
    "# print(c.shape)\n",
    "\n",
    "####################### Expected result ##########################\n",
    "# [[[[ 1  2  3  4]\n",
    "#    [ 5  6  7  8]\n",
    "#    [ 9 10 11 12]\n",
    "#    [13 14 15 16]]]]\n",
    "# -------------d_img--------------\n",
    "# [[[[ 43.  53.  63.]\n",
    "#    [ 83.  93. 103.]\n",
    "#    [123. 133. 143.]]\n",
    "\n",
    "#   [[ 99. 125. 151.]\n",
    "#    [203. 229. 255.]\n",
    "#    [307. 333. 359.]]]]\n",
    "# (1, 2, 3, 3)\n",
    "# --------------------------------\n",
    "# [[[[ 964. 2034. 2494. 1246.]\n",
    "#    [2636. 5268. 6044. 2912.]\n",
    "#    [4332. 8372. 9148. 4320.]\n",
    "#    [2088. 3922. 4238. 1938.]]]]\n",
    "# [[[[ 6042.  6879.]\n",
    "#    [ 9390. 10227.]]]\n",
    "\n",
    "\n",
    "#  [[[15018. 17079.]\n",
    "#    [23262. 25323.]]]]\n",
    "# [ 837. 2061.]\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc8707",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e553e97",
   "metadata": {},
   "source": [
    "### MLP Forward Pass\n",
    "\n",
    "The `ReLU_SoftMax_FullyConnected` function defines the forward pass for a Multi-Layer Perceptron (MLP) with one hidden layer. This MLP typically follows the convolutional feature extractor.\n",
    "\n",
    "**Layers and Operations:**\n",
    "\n",
    "1.  **Input:** `input_array` (denoted $X_{mlp}$) is the flattened output from the final convolutional layer.\n",
    "\n",
    "2.  **First Hidden Layer (fc1):**\n",
    "    *   Linear Transformation: $Z_1 = X_{mlp} W_1 + b_1$\n",
    "        *   `w1` ($W_1$): Weights of the first fully connected layer.\n",
    "        *   `b1` ($B_1$): Biases of the first fully connected layer.\n",
    "        *   The result is `fl` ($Z_1$).\n",
    "    *   ReLU Activation: $A_1 = \\max(0, Z_1)$\n",
    "        *   The result is `fa` ($A_1$).\n",
    "\n",
    "3.  **Output Layer (fc2):**\n",
    "    *   Linear Transformation: $Z_2 = A_1 W_2 + b_2$\n",
    "        *   `w2` ($W_2$): Weights of the output layer.\n",
    "        *   `b2` ($B_2$): Biases of the output layer.\n",
    "        *   The result is `sl` ($Z_2$), often called \"logits\" or raw scores.\n",
    "    *   Softmax Activation: $P = \\text{Softmax}(Z_2)$\n",
    "        The softmax function converts the logits $Z_2$ into class probabilities $P$. For a vector $z=(z_1, ..., z_K)$:\n",
    "        $$ \\text{Softmax}(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}} $$\n",
    "        The implementation includes a numerical stability trick by subtracting the maximum value from $z$ before exponentiation: $e^{z_j - \\max(z)}$. This prevents overflow with large $z_j$ values without changing the output probabilities.\n",
    "        *   The result is `sa` ($P$).\n",
    "\n",
    "The function returns intermediate values (`fl`, `fa`, `sl`) as they are needed for the backward pass, along with the final class probabilities `sa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70cbf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa\n",
    "\n",
    "#print(softmax([1,2,3,100000]))\n",
    "#print(softmax_no_NS([1,2,3,1000]))\n",
    "#r = np.array(np.array([1,2,777,2]))\n",
    "#print(softmax(r))\n",
    "#r = np.array((np.array([1,2,777,2]),np.array([1,2,777,2]),np.array([1,2,777,2])))\n",
    "#print(softmax(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108f244",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ee3cb",
   "metadata": {},
   "source": [
    "### MLP Backward Pass\n",
    "\n",
    "The `ReLU_SoftMax_FC_Backward` function computes the gradients for the MLP layers. It takes the batch size `bs`, predicted probabilities `pred` ($P$), true labels `labels` ($Y$), weights $W_1, W_2$, hidden layer activation `fa` ($A_1$), hidden layer pre-activation `fl` ($Z_1$), and the input to the MLP `i_mlp` ($X_{mlp}$).\n",
    "\n",
    "**Gradients (from output backwards):**\n",
    "\n",
    "Let $L$ be the Categorical Cross-Entropy loss.\n",
    "\n",
    "1.  **Gradient of Loss w.r.t. $Z_2$ (input to Softmax):**\n",
    "    For Softmax activation followed by Categorical Cross-Entropy loss, this gradient has a simple form:\n",
    "    $$ \\frac{\\partial L}{\\partial Z_2} = P - Y $$\n",
    "    Where $P$ are the predicted probabilities and $Y$ are the one-hot encoded true labels.\n",
    "    In code: `dL_dz2 = pred - labels[0:bs]`\n",
    "\n",
    "2.  **Gradient w.r.t. $W_2$ (weights of output layer):**\n",
    "    $$ \\frac{\\partial L}{\\partial W_2} = A_1^T \\frac{\\partial L}{\\partial Z_2} $$\n",
    "    In code: `dL_dw2 = fa.T @ dL_dz2`\n",
    "\n",
    "3.  **Gradient w.r.t. $b_2$ (bias of output layer):**\n",
    "    $$ \\frac{\\partial L}{\\partial b_2} = \\sum_{batch} \\frac{\\partial L}{\\partial Z_2} \\quad (\\text{summing across the batch dimension}) $$\n",
    "    In code: `dL_db2 = np.sum(dL_dz2, axis=0)`\n",
    "\n",
    "4.  **Gradient w.r.t. $A_1$ (activation of hidden layer):**\n",
    "    $$ \\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} W_2^T $$\n",
    "    In code: `dL_dfa = dL_dz2 @ w2.T`\n",
    "\n",
    "5.  **Gradient w.r.t. $Z_1$ (pre-activation of hidden layer - backward ReLU):**\n",
    "    $$ \\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\frac{dA_1}{dZ_1} $$\n",
    "    Where $\\frac{dA_1}{dZ_1}$ is the derivative of ReLU, which is 1 if $Z_1 > 0$ and 0 otherwise.\n",
    "    In code: `dReLU = (fl > 0).astype(float)`, then `dL_dfl = dL_dfa * dReLU`\n",
    "\n",
    "6.  **Gradient w.r.t. $W_1$ (weights of hidden layer):**\n",
    "    $$ \\frac{\\partial L}{\\partial W_1} = X_{mlp}^T \\frac{\\partial L}{\\partial Z_1} $$\n",
    "    In code: `dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl`\n",
    "\n",
    "7.  **Gradient w.r.t. $b_1$ (bias of hidden layer):**\n",
    "    $$ \\frac{\\partial L}{\\partial b_1} = \\sum_{batch} \\frac{\\partial L}{\\partial Z_1} $$\n",
    "    In code: `dL_db1 = np.sum(dL_dfl, axis=0)`\n",
    "\n",
    "8.  **Gradient w.r.t. $X_{mlp}$ (input to MLP):**\n",
    "    This is the gradient that will be passed back to the convolutional layers (after reshaping).\n",
    "    $$ \\frac{\\partial L}{\\partial X_{mlp}} = \\frac{\\partial L}{\\partial Z_1} W_1^T $$\n",
    "    In code: `dL_i_mlp = dL_dfl @ w1.T`\n",
    "\n",
    "These gradients are then used to update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7942596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels[0:bs]\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d3b94",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9e7e6",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy\n",
    "\n",
    "The `crossEntropy` function calculates the Categorical Cross-Entropy loss between the predicted probabilities and the true (one-hot encoded) labels for a single sample.\n",
    "\n",
    "**Formula:**\n",
    "For a single sample, if $P = (p_1, p_2, ..., p_K)$ is the vector of predicted probabilities for $K$ classes, and $Y = (y_1, y_2, ..., y_K)$ is the one-hot encoded true label vector:\n",
    "$$ L(P, Y) = - \\sum_{k=1}^{K} y_k \\log(p_k) $$\n",
    "Since $Y$ is one-hot encoded, only one $y_k$ is 1 (say for class $c$), and all others are 0. So the sum simplifies to:\n",
    "$$ L(P, Y) = - \\log(p_c) $$\n",
    "where $p_c$ is the predicted probability for the true class $c$.\n",
    "\n",
    "**Numerical Stability:**\n",
    "The term `p = p + (1/100000)` is added to prevent $\\log(0)$, which is undefined. If the model predicts a probability of exactly 0 for the true class, this would lead to an infinite loss. Adding a small epsilon ensures that $p_k$ is always slightly greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb425b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(p,t):\n",
    "    # p stands for prediction and t stands for true label\n",
    "    # p = [0,0,1] and t = [1,0,0]\n",
    "    p = p+(1/100000) # for numerical stability\n",
    "    return -np.dot(t,np.log(p).T)\n",
    "\n",
    "#c = [1,1000000000000000,1,1]\n",
    "#c = softmax(c)\n",
    "#print(c)\n",
    "#c = crossEntropy(c,[0,1,0,0])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fd4f5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad189b98",
   "metadata": {},
   "source": [
    "## Inference: Comparing Implementations\n",
    "\n",
    "This section performs inference using the trained PyTorch model and our two NumPy-based CNN implementations (\"Slow\" and \"Fast\"). The key objectives are:\n",
    "\n",
    "1.  **Correctness Check:** Since all three implementations use the *exact same weights* (extracted from the pre-trained PyTorch model), they should produce identical predictions for the same input image. The `correct` counter tracks if the predicted class from all three models matches.\n",
    "2.  **Performance Comparison:** The primary difference expected is in execution speed. We measure and compare the average inference time per image for:\n",
    "    *   PyTorch (highly optimized C++/CUDA backend).\n",
    "    *   \"Slow\" NumPy (explicit Python loops for convolution).\n",
    "    *   \"Fast\" NumPy (intended to use Im2Col for convolution, leveraging optimized matrix multiplication).\n",
    "\n",
    "The loop iterates through a subset of the test images. For each image:\n",
    "*   It's fed through the PyTorch `model`.\n",
    "*   It's fed through the \"Slow\" NumPy CNN (sequence of `Slow_ReLU_Conv` and MLP calls).\n",
    "*   It's fed through the \"Fast\" NumPy CNN (sequence of `Fast_ReLU_Conv` and MLP calls).\n",
    "\n",
    "Predictions are compared, and timings are recorded and averaged. This will highlight the significant performance advantage of optimized libraries (PyTorch) and vectorized approaches (like Im2Col in \"Fast\" NumPy) over naive loop-based implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...:  10%|█         | 10/100 [00:35<06:10,  4.12s/it, average_times=t: 0.0034 s, s: 3.5649 s, f: 0.0028 s, correct_predictions=100.0%]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "\n",
    "#length = test_labels.shape[0]\n",
    "length = 100\n",
    "correct = 0\n",
    "skip = True\n",
    "loop = tqdm(range(length),desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "    ############### CNN Slow Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted2 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "    ############### CNN Fast Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = Fast_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2f,mask2f = Fast_ReLU_Conv(c1f.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3f,mask3f = Fast_ReLU_Conv(c2f.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted3 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    t = int(predicted1[0])\n",
    "    s = int(predicted2[0])\n",
    "    f = int(predicted3[0])\n",
    "    if t == s and t == f:\n",
    "        correct+=1\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    tat = round(sum(dict_times['ctorch'])/(i+1),4)\n",
    "    sat = round(sum(dict_times['cslow'])/(i+1),4)\n",
    "    fat = round(sum(dict_times['cfast'])/(i+1),4)\n",
    "    loop.set_postfix(average_times =f\"t: {tat} s, s: {sat} s, f: {fat} s\" , correct_predictions=f\"{100*correct/(i+1)}%\")\n",
    "tat = round(sum(dict_times['ctorch'])/length,4)\n",
    "sat = round(sum(dict_times['cslow'])/length,4)\n",
    "fat = round(sum(dict_times['cfast'])/length,4)\n",
    "print(f\"Average forward execution time in seconds: \\nPyTorch: {tat} s, \\nSlow: {sat} s, \\nFast: {fat} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae1ea1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97018854",
   "metadata": {},
   "source": [
    "### Test for Slow approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b559791",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf57c6",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d687e",
   "metadata": {},
   "source": [
    "### NumPy Model Training: Weights Initialization\n",
    "\n",
    "Before attempting to train our NumPy-based CNN implementations from scratch, we need to initialize their weights and biases. Unlike the inference section where we used pre-trained PyTorch weights, here we start with random values.\n",
    "\n",
    "**Initialization Strategy:**\n",
    "\n",
    "*   The shapes of the kernels (`k1, k2, k3`), convolutional biases (`bc1, bc2, bc3`), fully connected weights (`w1, w2`), and FC biases (`b1, b2`) are determined by the shapes of the corresponding parameters in our `numpy_weights` dictionary (which came from the PyTorch model). This ensures our NumPy model has the same architecture.\n",
    "*   `np.random.rand(...)` is used to fill these arrays with random numbers uniformly distributed between 0 and 1.\n",
    "    *   More sophisticated initialization schemes (e.g., Xavier/Glorot, He initialization) are often used in practice to help with training dynamics, but for this demonstration, simple random initialization is sufficient to observe if learning occurs.\n",
    "\n",
    "These randomly initialized weights will be updated during the training process described in the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f3b00564",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c8760193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88167606",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8346885",
   "metadata": {},
   "source": [
    "### Training the \"Slow\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This section tests the training process for our \"Slow\" NumPy CNN implementation, which uses explicit loops for convolutions. To keep the test manageable and to clearly observe if the loss decreases, training is performed on a single image from the training set for several epochs.\n",
    "\n",
    "**Training Loop Steps:**\n",
    "\n",
    "For each epoch:\n",
    "1.  **Forward Pass:**\n",
    "    *   The input image `c0` is passed through the convolutional layers:\n",
    "        *   `c1s, mask1s = Slow_ReLU_Conv(c0, k1, bc1, ...)`\n",
    "        *   `c2s, mask2s = Slow_ReLU_Conv(c1s, k2, bc2, ...)`\n",
    "        *   `c3s, mask3s = Slow_ReLU_Conv(c2s, k3, bc3, ...)`\n",
    "        (Padding and stride values should match the PyTorch model for consistent architecture if aiming for similar behavior).\n",
    "    *   The output `c3s` is flattened (`imlps`).\n",
    "    *   The flattened features are passed through the MLP:\n",
    "        *   `fl, fa, sl, sa = ReLU_SoftMax_FullyConnected(imlps, w1, b1, w2, b2)`\n",
    "        The output `sa` contains the predicted class probabilities.\n",
    "\n",
    "2.  **Loss Calculation:**\n",
    "    *   The Categorical Cross-Entropy loss is computed between the predicted probabilities `sa` and the true one-hot encoded label for the image:\n",
    "        *   `loss = crossEntropy(sa, train_labels[0])`\n",
    "\n",
    "3.  **Backward Pass (Gradient Computation):**\n",
    "    *   Gradients for the MLP layers are computed:\n",
    "        *   `dL_i_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2 = ReLU_SoftMax_FC_Backward(...)`\n",
    "    *   The gradient `dL_i_mlp` (gradient w.r.t. the input of the MLP) is reshaped to match the output shape of the last convolutional layer (`c3s.shape`). This is $\\frac{\\partial L}{\\partial A_3}$ for the last conv layer.\n",
    "    *   Gradients for the convolutional layers are computed by backpropagating `dL_i_mlp` (and subsequent input gradients `gi3`, `gi2`):\n",
    "        *   `gi3, gk3, gb3 = Slow_ReLU_Gradient(c2s, dL_i_mlp, k3, mask3s, ...)` ($\\frac{\\partial L}{\\partial X_2}, \\frac{\\partial L}{\\partial W_3}, \\frac{\\partial L}{\\partial b_3}$)\n",
    "        *   `gi2, gk2, gb2 = Slow_ReLU_Gradient(c1s, gi3, k2, mask2s, ...)` ($\\frac{\\partial L}{\\partial X_1}, \\frac{\\partial L}{\\partial W_2}, \\frac{\\partial L}{\\partial b_2}$)\n",
    "        *   `gi1, gk1, gb1 = Slow_ReLU_Gradient(c0, gi2, k1, mask1s, ...)` ($\\frac{\\partial L}{\\partial X_0}, \\frac{\\partial L}{\\partial W_1}, \\frac{\\partial L}{\\partial b_1}$)\n",
    "\n",
    "4.  **Weights Update (Gradient Descent):**\n",
    "    All weights and biases are updated using basic gradient descent:\n",
    "    $$ W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W_{old}} $$\n",
    "    $$ b_{new} = b_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial b_{old}} $$\n",
    "    Where $\\eta$ is the learning rate (`lr`).\n",
    "\n",
    "The `avg_loss` is plotted to visualize if the model is learning (loss should decrease over epochs). The average forward and backward times are also tracked.\n",
    "**Note:** The `ValueError` in your output for this cell indicates a shape mismatch when `ReLU_SoftMax_FullyConnected` is called. The flattened output of `c3s` (`imlps`) does not have the expected number of features (2048) for `w1`. This is likely due to the `Slow_ReLU_Conv` calls not perfectly replicating the output dimensions of the PyTorch model due to different padding/stride choices in the training loop compared to the PyTorch model's definition. For example, PyTorch `conv1` used `padding=0`, but the `Slow_ReLU_Conv` in the training loop used `pad=0` for `c1s`, `pad=1` for `c2s`, `pad=0` for `c3s`. To match PyTorch: `conv1(p=0,s=2)`, `conv2(p=1,s=2)`, `conv3(p=0,s=2)`.\n",
    "Corrected params in loop:\n",
    "`c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)`\n",
    "`c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)`\n",
    "`c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)`\n",
    "This should yield `c3s` as `(1, 128, 4, 4)`, flattening to 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe14aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "(1, 32, 14, 14)\n",
      "(1, 64, 6, 6)\n",
      "(1, 128, 2, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1152 is different from 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[288], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(c3s\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     23\u001b[0m imlps \u001b[38;5;241m=\u001b[39m c3s\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m fl,fa,sl,sa \u001b[38;5;241m=\u001b[39m \u001b[43mReLU_SoftMax_FullyConnected\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimlps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m sfte \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# slow forward time end\u001b[39;00m\n\u001b[0;32m     26\u001b[0m sft \u001b[38;5;241m=\u001b[39m sfte \u001b[38;5;241m-\u001b[39m sfts\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mReLU_SoftMax_FullyConnected\u001b[1;34m(input_array, w1, b1, w2, b2)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReLU_SoftMax_FullyConnected\u001b[39m(input_array,w1,b1,w2,b2):\n\u001b[1;32m----> 6\u001b[0m     fl \u001b[38;5;241m=\u001b[39m (\u001b[43minput_array\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m)\u001b[38;5;241m+\u001b[39mb1 \u001b[38;5;66;03m# first layer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     fa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m,fl) \u001b[38;5;66;03m# first activation: ReLU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     sl \u001b[38;5;241m=\u001b[39m (fa \u001b[38;5;241m@\u001b[39m w2)\u001b[38;5;241m+\u001b[39mb2 \u001b[38;5;66;03m# second layer\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1152 is different from 512)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToBeTrained = True\n",
    "if ToBeTrained:\n",
    "    avg_loss = []\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    numEpochs = 20\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    loop = tqdm(range(numEpochs))\n",
    "    for i in loop:\n",
    "        c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "        \n",
    "        # Forward\n",
    "        sfts = time.time() # slow forward time start\n",
    "        c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "        c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "        c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "        print(c0.shape)\n",
    "        print(c1s.shape)\n",
    "        print(c2s.shape)\n",
    "        print(c3s.shape)\n",
    "        imlps = c3s.reshape(1,-1)\n",
    "        fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "        sfte = time.time() # slow forward time end\n",
    "        sft = sfte - sfts\n",
    "        forward_time.append(sft)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa,train_labels[0])\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        sbts = time.time() # slow backward time start\n",
    "        dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "        dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "        gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "\n",
    "        gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "        gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "        sbte = time.time() # slow backward time end\n",
    "        sbt = sbte - sbts\n",
    "        backward_time.append(sbt)\n",
    "\n",
    "        # Weights update\n",
    "        w1 -= lr*dL_dw1\n",
    "        b1 -= lr*dL_db1\n",
    "        w2 -= lr*dL_dw2\n",
    "        b2 -= lr*dL_db2\n",
    "        k3 -= lr*gk3\n",
    "        k2 -= lr*gk2\n",
    "        k1 -= lr*gk1\n",
    "        bc3 -= lr*gb3.reshape(-1)\n",
    "        bc2 -= lr*gb2.reshape(-1)\n",
    "        bc1 -= lr*gb1.reshape(-1)\n",
    "        \n",
    "        if len(avg_loss) >= 2:\n",
    "            loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "    plt.plot(avg_loss)\n",
    "    plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdde0d",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 4.9017 s\n",
    "- average backward time : 22.5251 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Slow Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e8aa7",
   "metadata": {},
   "source": [
    "### Test for Fast approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84440e1b",
   "metadata": {},
   "source": [
    "### Training the \"Fast\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This section mirrors the training test for the \"Slow\" CNN, but now using the \"Fast\" (Im2Col-based) implementations for the convolutional forward and backward passes. The goal is to verify if this more optimized approach also learns correctly.\n",
    "\n",
    "**Training Loop Steps (Differences from \"Slow\" highlighted):**\n",
    "\n",
    "For each epoch:\n",
    "1.  **Forward Pass:**\n",
    "    *   Convolutional layers use `Fast_ReLU_Conv`:\n",
    "        *   `c1s, mask1s = Fast_ReLU_Conv(c0, k1, bc1, ...)`\n",
    "        *   `c2s, mask2s = Fast_ReLU_Conv(c1s, k2, bc2, ...)`\n",
    "        *   `c3s, mask3s = Fast_ReLU_Conv(c2s, k3, bc3, ...)`\n",
    "        (Padding and stride values should ideally match the PyTorch model for architectural consistency.)\n",
    "    *   MLP part (`ReLU_SoftMax_FullyConnected`) is the same.\n",
    "\n",
    "2.  **Loss Calculation:** Same (`crossEntropy`).\n",
    "\n",
    "3.  **Backward Pass (Gradient Computation):**\n",
    "    *   MLP gradients (`ReLU_SoftMax_FC_Backward`) are computed the same way.\n",
    "    *   Convolutional layer gradients use `Fast_ReLU_Gradient`:\n",
    "        *   `gi3, gk3, gb3 = Fast_ReLU_Gradient(c2s, dL_i_mlp, k3, mask3s, ...)`\n",
    "        *   `gi2, gk2, gb2 = Fast_ReLU_Gradient(c1s, gi3, k2, mask2s, ...)`\n",
    "        *   `gi1, gk1, gb1 = Fast_ReLU_Gradient(c0, gi2, k1, mask1s, ...)`\n",
    "\n",
    "4.  **Weights Update (Gradient Descent):** Same update rule.\n",
    "\n",
    "The expectation is that the loss will decrease, similar to the \"Slow\" version, but potentially with different forward/backward pass timings due to the change in convolution implementation.\n",
    "**Note on the `ValueError`:** Similar to the \"Slow\" training test, the `ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0 ... (size 1152 is different from 2048)` in your output for this cell indicates that the flattened output of the `Fast_ReLU_Conv` sequence (`c3s`) does not match the expected input dimension (2048) for the first fully connected layer (`w1`). This arises because the `pad` parameters used in the `Fast_ReLU_Conv` calls in this training loop (`pad=1` for all three) cause the final output `c3s` to have a different spatial dimension than the PyTorch reference model (which used `p=0` for `conv1` and `conv3`, and `p=1` for `conv2`). To fix this, the `pad` arguments in the `Fast_ReLU_Conv` calls should be: `pad=0` for the first, `pad=1` for the second, and `pad=0` for the third convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f98a2",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d54b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ea750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac1aa3",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0b604f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1152 is different from 2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[284], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m c3s,mask3s \u001b[38;5;241m=\u001b[39m Fast_ReLU_Conv(c2s\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),k3,bc3,pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m imlps \u001b[38;5;241m=\u001b[39m c3s\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m fl,fa,sl,sa \u001b[38;5;241m=\u001b[39m \u001b[43mReLU_SoftMax_FullyConnected\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimlps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m sfte \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# slow forward time end\u001b[39;00m\n\u001b[0;32m     20\u001b[0m sft \u001b[38;5;241m=\u001b[39m sfte \u001b[38;5;241m-\u001b[39m sfts\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mReLU_SoftMax_FullyConnected\u001b[1;34m(input_array, w1, b1, w2, b2)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReLU_SoftMax_FullyConnected\u001b[39m(input_array,w1,b1,w2,b2):\n\u001b[1;32m----> 6\u001b[0m     fl \u001b[38;5;241m=\u001b[39m (\u001b[43minput_array\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m)\u001b[38;5;241m+\u001b[39mb1 \u001b[38;5;66;03m# first layer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     fa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m,fl) \u001b[38;5;66;03m# first activation: ReLU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     sl \u001b[38;5;241m=\u001b[39m (fa \u001b[38;5;241m@\u001b[39m w2)\u001b[38;5;241m+\u001b[39mb2 \u001b[38;5;66;03m# second layer\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1152 is different from 2048)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = Fast_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=1,stride=2)\n",
    "    c2s,mask2s = Fast_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = Fast_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=1,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = Fast_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=1,stride=2)\n",
    "    print(c3s.shape)\n",
    "    print(gi3.shape)\n",
    "    print(c2s.shape)\n",
    "    print(gk3.shape)\n",
    "    print(gb3.shape)\n",
    "    print(bc3.shape)\n",
    "    gi2,gk2,gb2 = Fast_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "    gi1,gk1,gb1 = Fast_ReLU_Gradient(c0,gi2,k1,mask1s,pad=1,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdb31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51200"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "51200/32/64/5/5\n",
    "800*64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
