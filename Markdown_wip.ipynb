{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106183a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05471163",
   "metadata": {},
   "source": [
    "### Dataset Loading and Preprocessing\n",
    "\n",
    "This section handles the loading and initial preparation of the MNIST dataset. MNIST contains 28x28 pixel grayscale images of handwritten digits (0-9).\n",
    "\n",
    "**Key Operations:**\n",
    "\n",
    "1.  **Data Loading (`load_mnist_images`, `load_mnist_labels`):**\n",
    "    *   These functions read the MNIST dataset from its specific binary file format. `struct.unpack` is used to parse metadata (like image dimensions and count) from the file headers.\n",
    "    *   Image data is read as a flat byte array and then reshaped to `(num_images, rows, cols)`.\n",
    "\n",
    "2.  **One-Hot Encoding Labels:**\n",
    "    *   For multi-class classification with a softmax output and categorical cross-entropy loss, integer labels (e.g., digit `5`) are converted into a one-hot vector format (e.g., `[0,0,0,0,0,1,0,0,0,0]` for 10 classes).\n",
    "    *   This represents the true label as a probability distribution where the correct class has a probability of 1.\n",
    "    *   **Formula:** For a label $y_i$ and $K$ classes, the one-hot vector $Y_i$ has elements $Y_{i,j}$:\n",
    "        $$ Y_{i,j} = \\begin{cases} 1 & \\text{if } j = y_i \\\\ 0 & \\text{otherwise} \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9edcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "#-------------- Data Extraction ---------------------------\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097cf66",
   "metadata": {},
   "source": [
    "## PyTorch CNN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8be5b",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is defined using PyTorch's `nn.Module` to serve as a reference and source of pre-trained weights.\n",
    "\n",
    "**Architecture (defined as `SimpleCNN` class):**\n",
    "\n",
    "1.  **Conv1 + ReLU1:** `nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 1, 28, 28)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1 = \\lfloor \\frac{(28 - 2 + 0)}{2} \\rfloor + 1 = 14$\n",
    "    *   Output: `(B, 32, 14, 14)`\n",
    "\n",
    "2.  **Conv2 + ReLU2:** `nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)`\n",
    "    *   Input: `(B, 32, 14, 14)`\n",
    "    *   Padded input dim: $14 + 2*1 = 16$\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(16 - 2 + 0)}{2} \\rfloor + 1 = 8$\n",
    "    *   Output: `(B, 64, 8, 8)`\n",
    "\n",
    "3.  **Conv3 + ReLU3:** `nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 64, 8, 8)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(8 - 2 + 0)}{2} \\rfloor + 1 = 4$\n",
    "    *   Output: `(B, 128, 4, 4)`\n",
    "\n",
    "4.  **Flatten:** `nn.Flatten()`\n",
    "    *   Input: `(B, 128, 4, 4)`\n",
    "    *   Output: `(B, 128 * 4 * 4)` which is `(B, 2048)`\n",
    "\n",
    "5.  **FC1 + ReLU4:** `nn.Linear(in_features=2048, out_features=250)`\n",
    "    *   Input: `(B, 2048)`\n",
    "    *   Operation: $Y = XW^T + b$\n",
    "    *   Output: `(B, 250)`\n",
    "\n",
    "6.  **FC2:** `nn.Linear(in_features=250, out_features=10)` (Output layer)\n",
    "    *   Input: `(B, 250)`\n",
    "    *   Output: `(B, 10)` (logits for 10 classes)\n",
    "\n",
    "The commented-out code sections would typically handle dataset loading into PyTorch `Dataset` and `DataLoader` objects, and the training loop (loss computation, backpropagation, optimizer steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dabdea",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff84faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 1.------------------ CNN declaration -------------------\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# # 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8a955",
   "metadata": {},
   "source": [
    "### Weights extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee8995",
   "metadata": {},
   "source": [
    "### Extracting Pre-trained Weights from PyTorch Model\n",
    "\n",
    "This section loads weights from a pre-trained PyTorch model (`simple_cnn_mnist.pth`) and converts them into NumPy arrays. These NumPy weights will be used for our custom CNN implementations to ensure consistency for inference comparisons.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1.  **Load State Dictionary:** `model.load_state_dict(torch.load(...))` populates the instantiated `SimpleCNN` model with saved parameters. `map_location=torch.device('cpu')` ensures CPU loading.\n",
    "2.  **Evaluation Mode:** `model.eval()` sets the model to evaluation mode, which is good practice (disables layers like dropout if present).\n",
    "3.  **NumPy Conversion:** For each layer, weights (`.weight`) and biases (`.bias`) are extracted:\n",
    "    *   `.data.detach().numpy()`: Converts PyTorch tensors to NumPy arrays, detaching them from the computation graph.\n",
    "\n",
    "**Shape Conventions and Transpositions:**\n",
    "\n",
    "*   **Convolutional Kernels (`k1`, `k2`, `k3`):**\n",
    "    *   PyTorch: `(out_channels, in_channels, kernel_height, kernel_width)`.\n",
    "    *   Stored directly in `numpy_weights` with this shape, as our NumPy convolution functions expect this.\n",
    "*   **Convolutional Biases (`b_conv1`, etc.):**\n",
    "    *   PyTorch: `(out_channels,)`. Stored directly.\n",
    "*   **Fully Connected Weights (`w1`, `w2`):**\n",
    "    *   PyTorch: `(out_features, in_features)`.\n",
    "    *   For NumPy $XW+b$ where $X$ is `(batch, in_features)`, $W$ must be `(in_features, out_features)`. Thus, PyTorch weights are transposed (`.T`).\n",
    "*   **Fully Connected Biases (`b1`, `b2`):**\n",
    "    *   PyTorch: `(out_features,)`. Reshaped to `(1, out_features)` for NumPy broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 2, 2), NumPy Shape=(64, 32, 2, 2)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 2, 2), NumPy Shape=(128, 64, 2, 2)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pyt_k1_w = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pyt_k1_w\n",
    "\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pyt_k1_w.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pyt_k2_w = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pyt_k2_w\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pyt_k2_w.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pyt_k3_w = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pyt_k3_w\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pyt_k3_w.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pyt_w1 = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pyt_w1.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pyt_b1 = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pyt_b1.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pyt_w1.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pyt_b1.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pyt_w2 = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pyt_w2.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pyt_b2 = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pyt_b2.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pyt_w2.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pyt_b2.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']\n",
    "\n",
    "\n",
    "\n",
    "# [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]]\n",
    "\n",
    "\n",
    "#  [[[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]]\n",
    "\n",
    "\n",
    "\n",
    "#    [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]\n",
    "\n",
    "#   [[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06822b",
   "metadata": {},
   "source": [
    "## CNN - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c17c8",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ca069",
   "metadata": {},
   "source": [
    "### Zero-Padding in Convolutions\n",
    "\n",
    "Zero-padding adds a border of zeros around an input image or feature map before convolution. For example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\quad \\xrightarrow{\\textcolor{lightgreen}{\\textnormal{zero padding}}} \\quad\n",
    "\\begin{bmatrix}\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 1 & 2 & 3 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 4 & 5 & 6 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 7 & 8 & 9 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It's crucial for:\n",
    "\n",
    "1.  **Controlling Output Spatial Dimensions:** Padding can be used to maintain or control the reduction in height/width of feature maps. The output dimension (e.g., height $O_H$) is given by:\n",
    "    $$ O_H = \\left\\lfloor \\frac{I_H - K_H + 2P_H}{S_H} \\right\\rfloor + 1 $$\n",
    "    where $I_H$ is input height, $K_H$ kernel height, $P_H$ padding on one side of height, and $S_H$ stride.\n",
    "2.  **Improving Feature Extraction at Borders:** Allows the kernel to process edge pixels more effectively.\n",
    "\n",
    "**`np.pad()` Usage:**\n",
    "For a 4D tensor (`BATCH, CHANNELS, HEIGHT, WIDTH`), `np.pad(array, ((0,0),(0,0),(p,p),(p,p)))` adds `p` zeros to the top/bottom of HEIGHT and left/right of WIDTH, leaving BATCH and CHANNELS unpadded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70273b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "img9 = np.arange(1,37).reshape(2,2,3,3)\n",
    "pad_img9 = np.pad(img9,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(img9)\n",
    "print(pad_img9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00035f0",
   "metadata": {},
   "source": [
    "### Delating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6aa4a",
   "metadata": {},
   "source": [
    "### Dilation for Transposed Convolutions\n",
    "\n",
    "The `delateOne` function \"dilates\" an input matrix by inserting a single row/column of zeros between existing rows/columns along its last two spatial dimensions.\n",
    "\n",
    "**Relevance in Backpropagation:**\n",
    "This operation is a key component in implementing the **transposed convolution** (often called deconvolution), which is required when calculating the gradient of a convolutional layer's loss with respect to its input ($\\frac{\\partial L}{\\partial X}$).\n",
    "*   If a forward convolution used a `stride S > 1`, its output feature map is downsampled.\n",
    "*   To compute $\\frac{\\partial L}{\\partial X}$, the gradient from the subsequent layer, $\\frac{\\partial L}{\\partial Z}$ (where $Z$ is the output of the strided convolution), must be \"upsampled\" before convolving with the rotated kernel.\n",
    "*   This upsampling involves inserting $S-1$ zeros between elements of $\\frac{\\partial L}{\\partial Z}$. `delateOne` performs this for $S=2$.\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*luRORFyTmj9mJ7rVhzlbZA.png\" height=250, style=\"border-radius:20px;\">\n",
    "    <figcaption>Dilating the output gradient pixels with stride_R — 1 zeroes vertically, and stride_S — 1 zeros horizontally.</figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd18f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delateOne(matrix):\n",
    "    indix = np.arange(1,matrix.shape[3])\n",
    "    matrix = np.insert(matrix,indix,0,3)\n",
    "    indix = np.arange(-(matrix.shape[-2]-1),0)\n",
    "    matrix = np.insert(matrix,indix,0,-2)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4351c2f",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26d187",
   "metadata": {},
   "source": [
    "### \"Slow\" Convolution Forward Pass (Explicit Loops)\n",
    "\n",
    "`Slow_ReLU_Conv` implements a 2D convolution followed by ReLU activation using explicit nested Python loops. This is educationally valuable for clarity but computationally inefficient.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1.  **Inputs:**\n",
    "    *   `img`: Batch of images `(N, C_in, H_in, W_in)`.\n",
    "    *   `ker`: Filters `(C_out, C_in, K_H, K_W)`.\n",
    "    *   `bias`: Per-filter biases `(C_out,)`.\n",
    "2.  **Padding & Output Size:** Input `img` is padded. Output dimensions $(O_H, O_W)$ are calculated using the standard formula (see Padding section).\n",
    "3.  **Convolution:** For each output element $(n, f, y_{out}, x_{out})$:\n",
    "    $$ \\text{Output}(n, f, y_{out}, x_{out}) = \\left( \\sum_{c=0}^{C_{in}-1} \\sum_{k_y=0}^{K_H-1} \\sum_{k_x=0}^{K_W-1} \\text{Img}_{pad}(n, c, y_{out}S + k_y, x_{out}S + k_x) \\cdot \\text{Ker}(f, c, k_y, k_x) \\right) + \\text{Bias}(f) $$\n",
    "4.  **ReLU Activation:** If `applyReLU=True`: $\\text{ActivatedOutput} = \\max(0, \\text{Output})$. A binary `mask` (1 where Output > 0, else 0) is also returned for backpropagation.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/refs/heads/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Convolution of a 4x4 image (blue) with a 2x2 kernel (green)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51071b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[ 1.  2.  3.  4.]\n",
      "   [ 5.  6.  7.  8.]\n",
      "   [ 9. 10. 11. 12.]]]]\n",
      "-------ker-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[  5.  19.  13.]\n",
      "   [ 47.  95.  45.]]\n",
      "\n",
      "  [[ 10.  40.  30.]\n",
      "   [104. 232. 126.]]]]\n",
      "-------Conv PyTorch-------\n",
      "tensor([[[[  5.,  19.,  13.],\n",
      "          [ 47.,  95.,  45.]],\n",
      "\n",
      "         [[ 10.,  40.,  30.],\n",
      "          [104., 232., 126.]]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This is a PyTorch Convolution example to be used to check if the convolution implemented in both slow and fast approaches are correct\n",
    "\n",
    "class CustomConv(nn.Module):\n",
    "    def __init__(self, kernel: torch.Tensor, bias: torch.Tensor = None, \n",
    "                 stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernel.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(bias is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernel)\n",
    "            if bias is not None:\n",
    "                self.conv.bias.copy_(bias)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if bias is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "def Slow_ReLU_Conv(img,ker,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        out_ch, in_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = out_ch\n",
    "    else: # Backward case\n",
    "        in_ch, out_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = in_ch\n",
    "\n",
    "    # bias has shape out_ch, 1, 1. It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # number of channels taken in input by the kernel 'in_ch' \n",
    "    # must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    n_images, channels, i_height, i_width  = img.shape\n",
    "    ni_height = int(((i_height - k_height) / stride) + 1) # new image height # Padding is already added\n",
    "    ni_width = int(((i_width - k_width) / stride) + 1) # new image width\n",
    "    ni = np.zeros((n_images, out_ch, ni_height, ni_width)).astype(np.float32) # new image\n",
    "\n",
    "    if in_ch != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({in_ch}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for one_img in range(n_images):\n",
    "        for one_k_channel in range(nk_channel):\n",
    "            for i_nih in range(ni_height): # which cycles row by row of the new image\n",
    "                for i_niw in range(ni_width): # which cycles column by column of the new image\n",
    "                    current_sum = 0.0 # convolution sum for the specific output cell\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(channels): # channels == in_ch\n",
    "                        for i_kh in range(k_height):\n",
    "                            input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "                            for i_kw in range(k_width):\n",
    "                                input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "                                    input_val = img[one_img, channel, input_y, input_x]\n",
    "                                    kernel_val = ker[one_k_channel, channel, i_kh, i_kw]\n",
    "                                    current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "                    ni[one_img, one_k_channel, i_nih, i_niw] = current_sum\n",
    "    if bias.all() != 0:\n",
    "        bias = bias.reshape(bias.shape[0],1,1)\n",
    "        if bias.shape[0] != out_ch:\n",
    "            raise ValueError(f\"bias dimension ({bias.shape[0]}) doesn't match kernel's number of channels ({out_ch})\")\n",
    "        ni = ni + bias\n",
    "    ni = ni.astype(np.float32)\n",
    "    if applyReLU:\n",
    "        ni = np.maximum(0, ni)\n",
    "        mask = ni.copy()\n",
    "        mask[mask > 0] = 1\n",
    "        return ni,mask\n",
    "    else:\n",
    "        return ni\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "img = np.arange(1,3*4+1).reshape(1,1,3,4).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=1,stride=2)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "\n",
    "my_kernel = torch.from_numpy(ker).float()\n",
    "\n",
    "my_bias = torch.from_numpy(np.array([1,2])).float()\n",
    "\n",
    "modelC = CustomConv(kernel=my_kernel,bias=my_bias, stride=2, padding=1)\n",
    "\n",
    "# input di prova (batch=1, canali=1, H=5, W=5)\n",
    "x = torch.from_numpy(img)\n",
    "y = modelC(x)\n",
    "print(\"-------Conv PyTorch-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bad0a3",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff4adf",
   "metadata": {},
   "source": [
    "**Actors:**\n",
    "1. W is the kernel\n",
    "2. $\\delta$ is the gradient\n",
    "3. x is the input to the convolution layer during forward\n",
    "4. b is the bias\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Derive delta**\n",
    "\n",
    "Deriving delta with respect to ReLU activation consists in the hadamard product (element-wise product) of the gradient ($\\delta$) and the mask obtained at the forward step, that is, all the elements in the convolved image greater than zero are put to one, the rest is zero.\n",
    "$$\n",
    "\\delta^{(i)} = \\delta_{\\text{flat reshaped}} \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "- **Gradient with respect to W**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(x^{(i)}, \\delta)\n",
    "$$\n",
    "This convolution creates a matrix for every channel of input image $x^{i}$ and for every channel of output image $\\delta$, thus resulting in the correct number of channels\n",
    "\n",
    "- **Gradient w.r.t. the input \\( x \\)** (To go to the preceding layer):\n",
    "\n",
    "$$\n",
    "\\delta^{(i-1)} = \\text{Full\\_Convolution}(\\delta^{(i)}, W^{(i)})\n",
    "$$\n",
    "\n",
    "- **Gradient w.r.t the bias**\n",
    "\n",
    "Since the bias is added equally across the spatial dimensions of each output channel, the gradient is the sum of all elements in each output channel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{h,w} \\delta^{(i)}_{c,h,w}\n",
    "$$\n",
    "\n",
    "For batched inputs, sum also across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n,h,w} \\delta^{(i)}_{n,c,h,w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672622d3",
   "metadata": {},
   "source": [
    "### \"Slow\" Convolution Backward Pass (Explicit Loops)\n",
    "\n",
    "`Slow_ReLU_Gradient` computes gradients $\\frac{\\partial L}{\\partial X}$ (input gradient, `gi`), $\\frac{\\partial L}{\\partial W}$ (kernel gradient, `gk`), and $\\frac{\\partial L}{\\partial b}$ (bias gradient, `gb`), given $\\frac{\\partial L}{\\partial A}$ (output activation gradient, `d_img`).\n",
    "\n",
    "**Steps based on the formulas in the preceding markdown cell:**\n",
    "\n",
    "1.  **Backward ReLU:** $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\text{mask}$. (`d_img = np.multiply(d_img,mask)`)\n",
    "2.  **Gradient w.r.t. Bias (`gb`):** $\\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} (\\frac{\\partial L}{\\partial Z})_{n,f,h,w}$. (`gb = d_img.sum((-1,-2))`, note: sum over batch is missing if batch_s > 1 and gradients are not accumulated externally).\n",
    "3.  **Gradient w.r.t. Kernel (`gk`):** $\\frac{\\partial L}{\\partial W} = \\text{Conv}(X_{padded}, \\frac{\\partial L}{\\partial Z})$.\n",
    "    *   The code iterates through each element of `gk` and computes its value by summing products of corresponding elements from `img` (original input, padded with forward pass `pad`) and `d_img` (which is $\\frac{\\partial L}{\\partial Z}$, after dilation if stride > 1 for this specific calculation, but the code uses the already dilated `d_img` from the `gi` section for the `dimg_height`, `dimg_width` loops).\n",
    "4.  **Gradient w.r.t. Input (`gi`):** $\\frac{\\partial L}{\\partial X} = \\text{FullConv}(\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}, W_{rot180})$.\n",
    "    *   $\\frac{\\partial L}{\\partial Z}$ is dilated if `stride==2` (`d_img = delateOne(d_img)`).\n",
    "    *   It's then padded: `d_imgPadded = np.pad(d_img, ..., (k_height-1-pad, ...))`.\n",
    "    *   Kernel `ker` is rotated 180 degrees (`ker180`).\n",
    "    *   The code then performs the convolution using nested loops to compute `gi`.\n",
    "\n",
    "This \"NEW APPROACH\" in comments refers to the direct loop-based implementation of these gradient convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1928d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imAge: (1, 1, 7, 7)\n",
      "kerNel: (2, 1, 2, 2)\n",
      "dimAge: (1, 2, 4, 4)\n",
      "ggi: (1, 1, 7, 7)\n",
      "ggk: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "def Slow_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "    ############################################# Gradient of Input Image ####################################\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image delated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel 180 degrees rotation (flipped vertically and then horizontally)\n",
    "    # FullConvolution(d_imgDelated, Rotated180Deg(kernel)) with stride 1\n",
    "    out_ch, in_ch, k_height, k_width = ker.shape\n",
    "    batch_s, in_ch, img_height, img_width = img.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "\n",
    "    # Delating the gradient of output\n",
    "    if stride == 2:\n",
    "        d_img = delateOne(d_img)\n",
    "    elif stride > 2:\n",
    "        raise ValueError(f\"Stride greater than 2 is not acceptable\")\n",
    "    d_imgPadded = np.pad(d_img,((0,0),(0,0),(k_height-1-pad,k_height-1-pad),(k_width-1-pad,k_width-1-pad)))\n",
    "    batch_s, out_ch, dimg_height, dimg_width = d_img.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    ker180 = np.rot90(ker,2,(-2,-1))\n",
    "\n",
    "    # Computation\n",
    "    gi = np.zeros_like(img)\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(img_height):\n",
    "            for i_giw in range(img_width):\n",
    "                for i_outch in range(out_ch):\n",
    "                    for i_inch in range(in_ch):\n",
    "                        for i_kh in range(k_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(k_width):\n",
    "                                x = i_gih + i_kw\n",
    "\n",
    "                                if 0 <= y < d_imgPadded.shape[-2] and 0 <= x < d_imgPadded.shape[-1]:\n",
    "                                    input_val = d_imgPadded[bs,i_outch,y,x]\n",
    "                                    ker_val = ker180[i_outch,i_inch,i_kh,i_kw] \n",
    "                                else:\n",
    "                                    break\n",
    "                                current_sum += input_val*ker_val\n",
    "                    gi[bs,i_inch,i_gih,i_giw] = current_sum\n",
    "                    current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the delated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    gk = np.zeros_like(ker)\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(k_height):\n",
    "            for i_giw in range(k_width):\n",
    "                for i_inch in range(in_ch):\n",
    "                    for i_outch in range(out_ch):\n",
    "                        for i_kh in range(dimg_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(dimg_width):\n",
    "                                x = i_gih + i_kw\n",
    "                                if 0 <= y < img_height and 0 <= x < img_width:\n",
    "                                    input_val = img[bs,i_inch,y,x]\n",
    "                                    ker_val = d_img[bs,i_outch,i_kh,i_kw] \n",
    "                                    current_sum += input_val*ker_val\n",
    "                                else:\n",
    "                                    break\n",
    "                        gk[i_outch,i_inch,i_gih,i_giw] = current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gb = d_img.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "in_ch = 1\n",
    "out_ch = 2\n",
    "idim = 7\n",
    "kdim = 2\n",
    "s = 2\n",
    "p = 1\n",
    "imAge = np.arange(1,1*in_ch*idim*idim+1).reshape(1,in_ch,idim,idim)\n",
    "kerNel = np.arange(1,out_ch*in_ch*(kdim**2)+1).reshape(out_ch,in_ch,kdim,kdim)\n",
    "dimAge,mask = Slow_ReLU_Conv(imAge,kerNel,stride=s,pad=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "ggi,ggk,ggb = Slow_ReLU_Gradient(imAge,dimAge,kerNel,mask,stride=s,pad=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggk: {ggk.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# OLD APPROACH ######################\n",
    "# def Slow_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "#     \"\"\"\n",
    "#     Performs the backward pass of the convolution layer. It takes the original image, \n",
    "#     the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "#     It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "#     \"\"\" \n",
    "\n",
    "#     out_ch, in_ch, k_height, k_width  = ker.shape \n",
    "#                                             # Example #\n",
    "#     # Convolving an RGB image with 32 2x2 kernels will give a shape of (32, 3, 2, 2) to the kernel. #\n",
    "    \n",
    "#     n_images, channels, i_height, i_width  = img.shape\n",
    "#     n_images, dch, di_height, di_width  = d_img.shape\n",
    "    \n",
    "#     ni_height = (i_height-1)*stride-(2*pad)+k_height # new image height\n",
    "#     ni_width =  (i_width-1)*stride-(2*pad)+k_width # new image width\n",
    "#     height_to_pad = ni_height-i_height\n",
    "#     width_to_pad = ni_width-i_width\n",
    "#     d_img = np.multiply(d_img,mask)\n",
    "#     d_imgP = np.pad(d_img,((0,0),(0,0),(height_to_pad,height_to_pad),(width_to_pad,width_to_pad)))\n",
    "#     gi = np.zeros_like(img).astype(np.float32) # gradient of original image\n",
    "#     gk = np.zeros_like(ker).astype(np.float32) # gradient of kernel\n",
    "\n",
    "# ############################## Computing the gradient of the original image ######################################\n",
    "#     current_sum = 0.0 # convolution sum for the specific output cell\n",
    "#     for one_img in range(n_images):\n",
    "#         for channel in range(channels):\n",
    "#             for i_nih in range(i_height): # which cycles row by row of the new image\n",
    "#                 for i_niw in range(i_width): # which cycles column by column of the new image\n",
    "#                     # Convolution cycles\n",
    "#                     for one_k_channel in range(out_ch): # channels == out_ch\n",
    "#                         for i_kh in range(k_height):\n",
    "#                             input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "#                             for i_kw in range(k_width):\n",
    "#                                 input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "#                                 # check that everything stays in the measures\n",
    "#                                 if 0 <= input_y < d_imgP.shape[2] and 0 <= input_x < d_imgP.shape[3]:\n",
    "#                                     input_val = d_imgP[one_img, one_k_channel, input_y, input_x]\n",
    "#                                     kernel_val = ker[one_k_channel,channel, i_kh, i_kw]\n",
    "#                                     current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "#                     gi[one_img, channel, i_nih, i_niw] = current_sum\n",
    "#                     current_sum = 0.0\n",
    "    \n",
    "# ############################## Computing the gradient of the kernel ##############################################\n",
    "# # Need to convolve the gradient of the image with the original image, using the gradient of the image as the kernel and \n",
    "# # keeping the same stride and padding (Otherwise the kernel won't work)\n",
    "#     current_sum = 0.0\n",
    "#     for one_img in range(n_images):\n",
    "#         for in_k_ch in range(in_ch): # which in the example is 3\n",
    "#             for out_k_ch in range(out_ch): # which in the example is 32\n",
    "#                 for k_gh in range(k_height):\n",
    "#                     for k_gw in range(k_width):\n",
    "#                     # gk[out_k_ch,in_k_ch,k_gh,k_gw] = something\n",
    "#                         for i_dh in range(di_height):\n",
    "#                             input_y = (k_gh * stride) + i_dh # get the y location, the height\n",
    "#                             for i_dw in range(di_width):\n",
    "#                                 input_x = (k_gw * stride) + i_dw\n",
    "#                                 # check that everything stays in the measures\n",
    "#                                 if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "#                                     input_val = img[one_img, in_k_ch, input_y, input_x]\n",
    "#                                     kernel_val = d_img[one_img, out_k_ch, i_dh, i_dw]\n",
    "#                                     current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "#                         gk[out_k_ch, in_k_ch, k_gh, k_gw] += current_sum\n",
    "\n",
    "# ############################## Computing the gradient of the bias ##############################################\n",
    "#     gb = d_img.sum((0,-1,-2)) # sum over batch, height and width\n",
    "# ################################################### Return Results ###############################################\n",
    "#     return gi,gk,gb\n",
    "\n",
    "# img = np.arange(1,17).reshape(1,1,4,4)\n",
    "# ker = np.arange(1,9).reshape(2,1,2,2)\n",
    "# bias = np.array([1,1])\n",
    "# d_img,mask=Slow_ReLU_Conv(img,ker,bias)\n",
    "# print(\"-------------d_img--------------\")\n",
    "# d_img = d_img - 2\n",
    "# print(d_img)\n",
    "# print(d_img.shape)\n",
    "# print(\"--------------------------------\")\n",
    "# a,b,c = Slow_ReLU_Gradient(img,d_img,ker,mask)\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae10455",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c44f",
   "metadata": {},
   "source": [
    "### \"Fast\" Convolution Forward Pass (Im2Col with `sliding_window_view`)\n",
    "\n",
    "`Fast_ReLU_Conv` implements 2D convolution more efficiently using an Im2Col-like approach facilitated by NumPy's `sliding_window_view` and optimized matrix multiplication.\n",
    "\n",
    "**Im2Col Core Idea:**\n",
    "\n",
    "1.  **Input Patch Extraction:** Instead of sliding a kernel, extract all input patches that the kernel would cover. `np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))` creates views of these patches. `[:,:,::stride,::stride]` applies striding.\n",
    "2.  **Matrix Reshaping:**\n",
    "    *   `window_m = ... .reshape((-1,(kw*kh*nc)))`: Flattens each extracted patch `(nc, kw, kh)` into a row vector of size `kw*kh*nc`. `window_m` (our $X_{col}$) thus has shape `(N_patches, patch_size)`.\n",
    "    *   `kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)`: Flattens each filter `(kc, ac, kw, kh)` -> `(kc, ac*kw*kh)`, then transposes to `(ac*kw*kh, kc)`. This is $W_{col}$, shape `(patch_size, C_out)`.\n",
    "3.  **Convolution as Matrix Multiplication:**\n",
    "    $$ Z_{col} = X_{col} \\cdot W_{col} $$\n",
    "    `c_m = (window_m @ kernel)` results in `(N_patches, C_out)`.\n",
    "4.  **Output Reshaping (Col2Im):**\n",
    "    `output_temp = c_m.reshape(bs, nih, niw, kc)` and `reshaped_correct_order = output_temp.transpose(0,3,1,2)` reshape the flat output back to the standard image batch format `(bs, C_out, O_H, O_W)`.\n",
    "5.  **Bias and ReLU:** Applied as in the slow version.\n",
    "\n",
    "This method leverages NumPy's highly optimized matrix multiplication for speed.\n",
    "\n",
    "*(Suggested GIF: Im2Col animation showing patch extraction, flattening, matrix multiplication, and reshaping.)*\n",
    "<img src=\"https://leonardoaraujosantos.gitbook.io/assets/ConvolutionArithmetic/im2col.gif\" alt=\"Im2Col Animation\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Conv Slow-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Conv(batch_of_images,kernel,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    kc, ac, kw, kh = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    # im2col: Window creation\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    window_m = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))[:,:,::stride,::stride].reshape((-1,(kw*kh*nc))) # window matrix\n",
    "    # Convolution\n",
    "    kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)\n",
    "    c_m = (window_m @ kernel).astype(np.float32) # convolved image matrix\n",
    "    # ReLU activation\n",
    "    nih = int(((ih-kh) / stride) + 1) # new image height # Padding is already added\n",
    "    niw = int(((iw-kw) / stride) + 1) # new image width\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output_temp = c_m.reshape(bs, nih, niw, kc)\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    reshaped_correct_order = output_temp.transpose(0,3,1,2).astype(np.float32)\n",
    "    if bias.any() != 0:\n",
    "        reshaped_correct_order = (reshaped_correct_order + bias.reshape(1,-1,1,1))\n",
    "    if applyReLU:\n",
    "        reshaped_correct_order = np.maximum(0,reshaped_correct_order)\n",
    "    mask = np.copy(reshaped_correct_order)\n",
    "    mask[mask>0]=1\n",
    "    return reshaped_correct_order,mask\n",
    "\n",
    "\n",
    "\n",
    "img = np.arange(1,2*3*3+1).reshape(1,2,3,3).astype(np.float32)\n",
    "# print(\"-------img-------\")\n",
    "# print(img)\n",
    "ker = np.arange(1,16+1).reshape(2,2,2,2)\n",
    "# print(\"-------ker-------\")\n",
    "# print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(img,ker,bias,pad = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)\n",
    "res,mask = Slow_ReLU_Conv(res,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(X_c,ker,bias,pad = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6912ed",
   "metadata": {},
   "source": [
    "Dimensional reshape test to see if everything goes into the right place (it does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1245. 1323.]\n",
      "   [1479. 1557.]]\n",
      "\n",
      "  [[2973. 3195.]\n",
      "   [3639. 3861.]]\n",
      "\n",
      "  [[4701. 5067.]\n",
      "   [5799. 6165.]]\n",
      "\n",
      "  [[6429. 6939.]\n",
      "   [7959. 8469.]]]]\n",
      "[[ 1  2  4  5]\n",
      " [ 2  3  5  6]\n",
      " [ 4  5  7  8]\n",
      " [ 5  6  8  9]\n",
      " [10 11 13 14]\n",
      " [11 12 14 15]\n",
      " [13 14 16 17]\n",
      " [14 15 17 18]\n",
      " [19 20 22 23]\n",
      " [20 21 23 24]\n",
      " [22 23 25 26]\n",
      " [23 24 26 27]]\n",
      "[[1245. 1323. 1479. 1557.]\n",
      " [2973. 3195. 3639. 3861.]\n",
      " [4701. 5067. 5799. 6165.]\n",
      " [6429. 6939. 7959. 8469.]]\n",
      "[[1245. 2973. 4701. 6429.]\n",
      " [1323. 3195. 5067. 6939.]\n",
      " [1479. 3639. 5799. 7959.]\n",
      " [1557. 3861. 6165. 8469.]]\n",
      "[[ 17592.  43224.  68856.  94488.]\n",
      " [ 23196.  56892.  90588. 124284.]\n",
      " [ 34404.  84228. 134052. 183876.]\n",
      " [ 40008.  97896. 155784. 213672.]\n",
      " [ 68028. 166236. 264444. 362652.]\n",
      " [ 73632. 179904. 286176. 392448.]\n",
      " [ 84840. 207240. 329640. 452040.]\n",
      " [ 90444. 220908. 351372. 481836.]\n",
      " [118464. 289248. 460032. 630816.]\n",
      " [124068. 302916. 481764. 660612.]\n",
      " [135276. 330252. 525228. 720204.]\n",
      " [140880. 343920. 546960. 750000.]]\n",
      "[[17592. 23196.]\n",
      " [34404. 40008.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "s = 1\n",
    "p = 0\n",
    "in_ch = 3\n",
    "out_ch = 4\n",
    "i_dim = 3\n",
    "k_dim = 2\n",
    "img = np.arange(1,i_dim*in_ch*i_dim+1).reshape(1,in_ch,i_dim,i_dim)\n",
    "ker = np.arange(1,out_ch*k_dim*in_ch*k_dim+1).reshape(out_ch,in_ch,k_dim,k_dim)\n",
    "d_img,mask = Fast_ReLU_Conv(img,ker,stride=s,pad=p)\n",
    "print(d_img)\n",
    "_,_,dimg_height,dimg_width = d_img.shape\n",
    "windom_pimg = np.lib.stride_tricks.sliding_window_view(img,(1,1,dimg_height,dimg_width)).reshape(-1,dimg_height*dimg_width)\n",
    "print(windom_pimg)\n",
    "print(d_img.reshape(-1,dimg_height*dimg_width))\n",
    "d_img = d_img.reshape(-1,dimg_height*dimg_width).transpose(1,0)\n",
    "print(d_img)\n",
    "iop = windom_pimg @ d_img\n",
    "print(iop)\n",
    "print(iop.transpose(1,0).reshape(out_ch,in_ch,k_dim,kdim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c414872",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e1403",
   "metadata": {},
   "source": [
    "### \"Fast\" Convolution Backward Pass (Im2Col-based Gradients)\n",
    "\n",
    "This revised `Fast_ReLU_Gradient` computes gradients $\\frac{\\partial L}{\\partial X}$ (`gi`), $\\frac{\\partial L}{\\partial W}$ (`gk`), and $\\frac{\\partial L}{\\partial b}$ (`gb`) using Im2Col-like principles with `sliding_window_view` for more operations.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Backward ReLU:** $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\text{mask}$. (`d_img = np.multiply(d_img,mask)`). Let this be `dL_dZ`.\n",
    "\n",
    "2.  **Gradient w.r.t. Bias (`gb`):** $\\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} (\\frac{\\partial L}{\\partial Z})_{n,f,h,w}$. (`gb = d_img.sum((-1,-2))`).\n",
    "\n",
    "3.  **Gradient w.r.t. Kernel (`gk`):**\n",
    "    This is $\\frac{\\partial L}{\\partial W} = \\text{Conv}(X_{padded}, \\frac{\\partial L}{\\partial Z})$.\n",
    "    *   `imgPad = np.pad(img, ...)`: Original input $X$ is padded as in the forward pass.\n",
    "    *   `windom_pimg = np.lib.stride_tricks.sliding_window_view(imgPad,(1,1,dimg_height,dimg_width)).reshape(-1,dimg_height*dimg_width)`: This extracts patches from `imgPad`. Each patch has the spatial dimensions of `dL_dZ`. This effectively creates an $X_{col}$-like matrix where rows are patches from $X_{padded}$ that align with elements of $dL_dZ$. Note: the `(1,1,...` implies it's processing per input channel.\n",
    "    *   `d_img_reshaped = dL_dZ.reshape(-1,dimg_height*dimg_width).transpose(1,0)`: This reshapes `dL_dZ` into columns, $(\\frac{\\partial L}{\\partial Z})_{col}$.\n",
    "    *   `gk = (windom_pimg @ d_img_reshaped).astype(np.float32).transpose(1,0).reshape(out_ch,in_ch,k_height,k_width)`:\n",
    "        This computes $X_{col} \\cdot (\\frac{\\partial L}{\\partial Z})_{col}$. The result is then reshaped to the kernel's original dimensions. This calculates $\\frac{\\partial L}{\\partial W_{f,c,ky,kx}} = \\sum X(c, ky', kx') \\cdot \\frac{\\partial L}{\\partial Z}(f, y_{out}, x_{out})$ over appropriate indices. The reshaping must correctly map the flattened result to `(C_out, C_in, K_H, K_W)`.\n",
    "\n",
    "4.  **Gradient w.r.t. Input (`gi`):**\n",
    "    This is $\\frac{\\partial L}{\\partial X} = \\text{FullConv}(\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}, W_{rot180})$.\n",
    "    *   `dL_dZ` is dilated if `stride==2`.\n",
    "    *   `d_imgPadded = np.pad(dL_dZ_dilated, ..., (k_height-1-pad, ...))`: The (dilated) `dL_dZ` is padded for the full convolution.\n",
    "    *   `ker_reshaped = ker.reshape((-1,(k_width*k_height*out_ch))).transpose(1,0)`: The original kernel $W$ is reshaped into $W_{col}$ (this uses `out_ch` from $W$, which is `in_ch` for this specific transposed convolution).\n",
    "    *   `window_dPad = np.lib.stride_tricks.sliding_window_view(d_imgPadded,(1,out_ch,k_width,k_height)).reshape(-1,(k_width*k_height*out_ch))`: Patches are extracted from `d_imgPadded` to form $(\\frac{\\partial L}{\\partial Z})_{col\\_padded}$.\n",
    "    *   `gi = (window_dPad @ ker_reshaped).astype(np.float32).transpose(1,0).reshape(img.shape)`:\n",
    "        This computes $(\\frac{\\partial L}{\\partial Z})_{col\\_padded} \\cdot W_{col\\_rotated\\_transposed}$ (effectively). The kernel `ker` used here should be $W_{rot180}$ and transposed appropriately for channel alignment in the matrix multiply. The current code uses the original `ker` (not rotated), which might lead to incorrect results unless the `reshape` handles the rotation implicitly. Standard transposed convolution uses $W_{rot180}$.\n",
    "\n",
    "This implementation aims for a more consistent Im2Col-style backward pass, but careful verification of kernel transformations and reshaping logic for both `gk` and `gi` is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5583e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimAge: (1, 4, 2, 2)\n",
      "imAge: (1, 3, 5, 5)\n",
      "kerNel: (4, 3, 3, 3)\n",
      "dimAge: (1, 4, 2, 2)\n",
      "ggi: (1, 3, 5, 5)\n",
      "ggk: (4, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "    ############################################# Gradient of Input Image ####################################\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image delated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel 180 degrees rotation (flipped vertically and then horizontally)\n",
    "    # FullConvolution(d_imgDelated, Rotated180Deg(kernel)) with stride 1\n",
    "    out_ch, in_ch, k_height, k_width = ker.shape\n",
    "    batch_s, in_ch, img_height, img_width = img.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "\n",
    "    # Delating the gradient of output\n",
    "    if stride == 2:\n",
    "        d_img = delateOne(d_img)\n",
    "    elif stride > 2:\n",
    "        raise ValueError(f\"Stride greater than 2 is not acceptable\")\n",
    "    d_imgPadded = np.pad(d_img,((0,0),(0,0),(k_height-1-pad,k_height-1-pad),(k_width-1-pad,k_width-1-pad)))\n",
    "\n",
    "    batch_s, out_ch, dimg_height, dimg_width = d_img.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    ker180 = np.rot90(ker,2,(-2,-1))\n",
    "\n",
    "    window_dPad = np.lib.stride_tricks.sliding_window_view(d_imgPadded,(1,out_ch,k_width,k_height)).reshape(-1,(k_width*k_height*out_ch)) # window matrix\n",
    "    # Convolution\n",
    "    ker = ker.reshape((-1,(k_width*k_height*out_ch))).transpose(1,0)\n",
    "\n",
    "    gi = (window_dPad @ ker).astype(np.float32).transpose(1,0).reshape(img.shape)\n",
    "    \n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the delated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    imgPad = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    windom_pimg = np.lib.stride_tricks.sliding_window_view(imgPad,(1,1,dimg_height,dimg_width)).reshape(-1,dimg_height*dimg_width)\n",
    "    d_img = d_img.reshape(-1,dimg_height*dimg_width).transpose(1,0)\n",
    "    gk = (windom_pimg @ d_img).astype(np.float32).transpose(1,0).reshape(out_ch,in_ch,k_height,k_width)\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gb = d_img.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "in_ch = 3\n",
    "out_ch = 4\n",
    "idim = 5\n",
    "kdim = 3\n",
    "s = 2\n",
    "p = 0\n",
    "imAge = np.arange(1,1*in_ch*idim*idim+1).reshape(1,in_ch,idim,idim)\n",
    "kerNel = np.arange(1,out_ch*in_ch*(kdim**2)+1).reshape(out_ch,in_ch,kdim,kdim)\n",
    "dimAge,mask = Fast_ReLU_Conv(imAge,kerNel,stride=s,pad=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "\n",
    "ggi,ggk,ggb = Fast_ReLU_Gradient(imAge,dimAge,kerNel,mask,stride=s,pad=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggk: {ggk.shape}\")\n",
    "\n",
    "# in_ch = 1\n",
    "# out_ch = 2\n",
    "# idim = 7\n",
    "# kdim = 2\n",
    "# s = 1\n",
    "# p = 0\n",
    "\n",
    "# imAge: (1, 1, 7, 7)\n",
    "# kerNel: (2, 1, 2, 2)\n",
    "# dimAge: (1, 2, 6, 6)\n",
    "# ggi: (1, 1, 7, 7)\n",
    "# ggk: (2, 1, 2, 2)\n",
    "\n",
    "# in_ch = 1\n",
    "# out_ch = 2\n",
    "# idim = 7\n",
    "# kdim = 2\n",
    "# s = 2\n",
    "# p = 1\n",
    "\n",
    "# imAge: (1, 1, 7, 7)\n",
    "# kerNel: (2, 1, 2, 2)\n",
    "# dimAge: (1, 2, 4, 4)\n",
    "# ggi: (1, 1, 7, 7)\n",
    "# ggk: (2, 1, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df32d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "48/3/4/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1dccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------img--------------\n",
      "(1, 3, 28, 28)\n",
      "-------------ker--------------\n",
      "(32, 3, 3, 3)\n",
      "################################\n",
      "-------------d_img--------------\n",
      "(1, 32, 14, 14)\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "# def Fast_ReLU_Gradient(batch_of_images,d_image,kernel,mask,pad=0,stride=1):\n",
    "#     out_ch, in_ch, kh, kw = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "#     bs, nc, i_height,i_width = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "\n",
    "#     batchSize, out_ch, dh, dw = d_image.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "#     ni_height = int(((i_height-1)*stride)+kh) # new image height\n",
    "#     ni_width =  int(((i_width-1)*stride)+kw) # new image width\n",
    "#     height_to_pad = (ni_height-dh)\n",
    "#     width_to_pad = (ni_width-dw)\n",
    "\n",
    "#     half_htp = height_to_pad//2\n",
    "#     half_wtp = width_to_pad//2\n",
    "\n",
    "#     d_image = np.multiply(d_image,mask)\n",
    "#     d_imgP = np.pad(d_image,((0,0),(0,0),(half_htp,half_htp),(half_wtp,half_wtp)))\n",
    "\n",
    "#     batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "#     bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    \n",
    "#     ############################## Computing the gradient of the bias ##############################################\n",
    "#     gb = d_image.sum((0,-1,-2)) # sum over batch, height and width\n",
    "\n",
    "#     ########################################## Gradient of Kernel ###################################################\n",
    "#     window_boi = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,1,dh,dw))[:,:,::stride,::stride].reshape((-1,(dw*dh*1))) # window matrix\n",
    "#     d_image = d_image.reshape((-1,(dw*dh*1))).transpose(1,0)\n",
    "#     gk = (window_boi @ d_image).transpose(1,0).reshape(out_ch, in_ch, kh, kw,).astype(np.float32) # convolved image matrix\n",
    "\n",
    "#     ########################################## Gradient of Image ###################################################\n",
    "#     gi,_ = Fast_ReLU_Conv(d_imgP,kernel.transpose(1,0,2,3),stride = stride,pad=pad,applyReLU=False)\n",
    "#     # window_dboi = np.lib.stride_tricks.sliding_window_view(d_imgP,(1,out_ch,kh,kw))[:,:,::stride,::stride].reshape((-1,(kw*kh*out_ch))) # window matrix\n",
    "#     # kernel = kernel.reshape((-1,(kw*kh*out_ch))).transpose(1,0)\n",
    "#     # gi = (window_dboi @ kernel).reshape(bs, i_height, i_width, nc).transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "#     ################################################### Return Results ###############################################\n",
    "#     return gi,gk,gb\n",
    "\n",
    "# s = 2\n",
    "# p = 1\n",
    "# in_ch = 3\n",
    "# out_ch = 32\n",
    "# i_dim = 28\n",
    "# k_dim = 3\n",
    "# img = np.arange(1,i_dim*in_ch*i_dim+1).reshape(1,in_ch,i_dim,i_dim)\n",
    "# ker = np.arange(1,out_ch*k_dim*in_ch*k_dim+1).reshape(out_ch,in_ch,k_dim,k_dim)\n",
    "# bias = np.ones(out_ch)\n",
    "# d_img,mask = Fast_ReLU_Conv(img,ker,bias,stride=s,pad=p)\n",
    "\n",
    "# print(\"-------------img--------------\")\n",
    "# #print(img)\n",
    "# print(img.shape)\n",
    "# print(\"-------------ker--------------\")\n",
    "# #print(ker)\n",
    "# print(ker.shape)\n",
    "# print(\"################################\")\n",
    "# print(\"-------------d_img--------------\")\n",
    "# #print(d_img-2)\n",
    "# print(d_img.shape)\n",
    "\n",
    "# print(\"************************************\")\n",
    "# # a,b,c = Fast_ReLU_Gradient(img,d_img-2,ker,mask,stride=s,pad=p)\n",
    "# # print(\"-------------gi-----------------\")\n",
    "# # print(a.shape)\n",
    "# # print(\"--------------gk-----------------\")\n",
    "# # print(b.shape)\n",
    "# # print(\"--------------gb-----------------\")\n",
    "# # print(c.shape)\n",
    "\n",
    "# ####################### Expected result ##########################\n",
    "# # [[[[ 1  2  3  4]\n",
    "# #    [ 5  6  7  8]\n",
    "# #    [ 9 10 11 12]\n",
    "# #    [13 14 15 16]]]]\n",
    "# # -------------d_img--------------\n",
    "# # [[[[ 43.  53.  63.]\n",
    "# #    [ 83.  93. 103.]\n",
    "# #    [123. 133. 143.]]\n",
    "\n",
    "# #   [[ 99. 125. 151.]\n",
    "# #    [203. 229. 255.]\n",
    "# #    [307. 333. 359.]]]]\n",
    "# # (1, 2, 3, 3)\n",
    "# # --------------------------------\n",
    "# # [[[[ 964. 2034. 2494. 1246.]\n",
    "# #    [2636. 5268. 6044. 2912.]\n",
    "# #    [4332. 8372. 9148. 4320.]\n",
    "# #    [2088. 3922. 4238. 1938.]]]]\n",
    "# # [[[[ 6042.  6879.]\n",
    "# #    [ 9390. 10227.]]]\n",
    "\n",
    "\n",
    "# #  [[[15018. 17079.]\n",
    "# #    [23262. 25323.]]]]\n",
    "# # [ 837. 2061.]\n",
    "# ##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249b2ef",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3f21f",
   "metadata": {},
   "source": [
    "### MLP Forward Pass\n",
    "\n",
    "`ReLU_SoftMax_FullyConnected` executes the forward pass of a two-layer Multi-Layer Perceptron (one hidden layer, one output layer), typically used for classification after feature extraction by convolutional layers.\n",
    "\n",
    "**Operations:**\n",
    "\n",
    "1.  **Input:** `input_array` ($X_{mlp}$), the flattened output from conv layers.\n",
    "2.  **Hidden Layer (fc1):**\n",
    "    *   Linear: $Z_1 = X_{mlp} W_1 + b_1$ (output `fl`)\n",
    "    *   ReLU Activation: $A_1 = \\max(0, Z_1)$ (output `fa`)\n",
    "3.  **Output Layer (fc2):**\n",
    "    *   Linear: $Z_2 = A_1 W_2 + b_2$ (output `sl`, logits)\n",
    "    *   Softmax Activation: $P = \\text{Softmax}(Z_2)$ (output `sa`, probabilities)\n",
    "        $$ \\text{Softmax}(z)_j = \\frac{e^{z_j - \\max(z)}}{\\sum_{k} e^{z_k - \\max(z)}} $$\n",
    "        (The subtraction of $\\max(z)$ aids numerical stability).\n",
    "\n",
    "Returns `fl, fa, sl, sa` (pre-activations, activations, logits, probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68206d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa\n",
    "\n",
    "#print(softmax([1,2,3,100000]))\n",
    "#print(softmax_no_NS([1,2,3,1000]))\n",
    "#r = np.array(np.array([1,2,777,2]))\n",
    "#print(softmax(r))\n",
    "#r = np.array((np.array([1,2,777,2]),np.array([1,2,777,2]),np.array([1,2,777,2])))\n",
    "#print(softmax(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3eaeb8",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7b0a1",
   "metadata": {},
   "source": [
    "### MLP Backward Pass\n",
    "\n",
    "`ReLU_SoftMax_FC_Backward` computes gradients for the MLP. Inputs: batch size `bs`, predictions `pred` ($P$), true `labels` ($Y$), weights $W_1, W_2$, hidden activation `fa` ($A_1$), hidden pre-activation `fl` ($Z_1$), MLP input `i_mlp` ($X_{mlp}$).\n",
    "\n",
    "**Gradients (from output layer backwards):**\n",
    "\n",
    "1.  $\\frac{\\partial L}{\\partial Z_2} = P - Y$ (`dL_dz2`)\n",
    "2.  $\\frac{\\partial L}{\\partial W_2} = A_1^T \\frac{\\partial L}{\\partial Z_2}$ (`dL_dw2`)\n",
    "3.  $\\frac{\\partial L}{\\partial b_2} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_2}$ (`dL_db2`)\n",
    "4.  $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} W_2^T$ (`dL_dfa`)\n",
    "5.  $\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\text{ReLU}'(Z_1)$ (`dL_dfl`, where $\\text{ReLU}'(Z_1)$ is 1 if $Z_1 > 0$, else 0)\n",
    "6.  $\\frac{\\partial L}{\\partial W_1} = X_{mlp}^T \\frac{\\partial L}{\\partial Z_1}$ (`dL_dw1`)\n",
    "7.  $\\frac{\\partial L}{\\partial b_1} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_1}$ (`dL_db1`)\n",
    "8.  $\\frac{\\partial L}{\\partial X_{mlp}} = \\frac{\\partial L}{\\partial Z_1} W_1^T$ (`dL_i_mlp`) (gradient to pass to conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10736bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels[0:bs]\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd7105",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf96a8c",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy\n",
    "\n",
    "`crossEntropy` calculates the Categorical Cross-Entropy loss for a single sample.\n",
    "\n",
    "**Formula:** Given predicted probabilities $P=(p_1, ..., p_K)$ and one-hot true label $Y=(y_1, ..., y_K)$:\n",
    "$$ L(P, Y) = - \\sum_{k=1}^{K} y_k \\log(p_k) $$\n",
    "If class $c$ is the true class ($y_c=1$), $L = - \\log(p_c)$.\n",
    "A small epsilon (`1/100000`) is added to $p$ to prevent $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1291609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(p,t):\n",
    "    # p stands for prediction and t stands for true label\n",
    "    # p = [0,0,1] and t = [1,0,0]\n",
    "    p = p+(1/100000) # for numerical stability\n",
    "    return -np.dot(t,np.log(p).T)\n",
    "\n",
    "#c = [1,1000000000000000,1,1]\n",
    "#c = softmax(c)\n",
    "#print(c)\n",
    "#c = crossEntropy(c,[0,1,0,0])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49079",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e3c9f",
   "metadata": {},
   "source": [
    "## Inference: Comparing Implementations\n",
    "\n",
    "This section compares the inference (prediction) performance and correctness of three CNN implementations: PyTorch, \"Slow\" NumPy (loop-based), and \"Fast\" NumPy (Im2Col-based). All use identical pre-trained weights.\n",
    "\n",
    "**Objectives:**\n",
    "1.  **Correctness:** Verify that all three models yield the same predictions.\n",
    "2.  **Speed:** Compare average inference time per image.\n",
    "\n",
    "The loop iterates through test images, runs each model, records predictions and times. This demonstrates the efficiency gains from optimized libraries (PyTorch) and vectorized NumPy (Fast) over naive loops (Slow). The `pad` and `stride` parameters in the NumPy calls (`Slow_ReLU_Conv`, `Fast_ReLU_Conv`) are set to match the PyTorch model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 100/100 [05:13<00:00,  3.13s/it, average_times=t: 0.0027 s, s: 3.1252 s, f: 0.0024 s, correct_predictions=100.0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward execution time in seconds: \n",
      "PyTorch: 0.0027 s, \n",
      "Slow: 3.1252 s, \n",
      "Fast: 0.0024 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "\n",
    "#length = test_labels.shape[0]\n",
    "length = 100\n",
    "correct = 0\n",
    "skip = True\n",
    "loop = tqdm(range(length),desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "    ############### CNN Slow Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted2 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "    ############### CNN Fast Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = Fast_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2f,mask2f = Fast_ReLU_Conv(c1f.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3f,mask3f = Fast_ReLU_Conv(c2f.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted3 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    t = int(predicted1[0])\n",
    "    s = int(predicted2[0])\n",
    "    f = int(predicted3[0])\n",
    "    if t == s and t == f:\n",
    "        correct+=1\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    tat = round(sum(dict_times['ctorch'])/(i+1),4)\n",
    "    sat = round(sum(dict_times['cslow'])/(i+1),4)\n",
    "    fat = round(sum(dict_times['cfast'])/(i+1),4)\n",
    "    loop.set_postfix(average_times =f\"t: {tat} s, s: {sat} s, f: {fat} s\" , correct_predictions=f\"{100*correct/(i+1)}%\")\n",
    "tat = round(sum(dict_times['ctorch'])/length,4)\n",
    "sat = round(sum(dict_times['cslow'])/length,4)\n",
    "fat = round(sum(dict_times['cfast'])/length,4)\n",
    "print(f\"Average forward execution time in seconds: \\nPyTorch: {tat} s, \\nSlow: {sat} s, \\nFast: {fat} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abe784",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3b44b",
   "metadata": {},
   "source": [
    "### Test for Slow approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f8f80",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d1c78",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a476a",
   "metadata": {},
   "source": [
    "### NumPy Model Training: Weights Initialization\n",
    "\n",
    "For training our NumPy CNNs from scratch, weights and biases are initialized randomly.\n",
    "The shapes are taken from `numpy_weights` (derived from the PyTorch model) to maintain architectural consistency. `np.random.rand()` provides initial values (uniform in [0,1)). While more advanced initializers exist, this suffices for observing basic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382d816",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b10d0",
   "metadata": {},
   "source": [
    "### Training the \"Slow\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This tests training the loop-based `Slow_ReLU_Conv` and `Slow_ReLU_Gradient` implementations on a single image.\n",
    "\n",
    "**Per-Epoch Steps:**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> Slow_ReLU_Conv (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> Slow_ReLU_Conv (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> Slow_ReLU_Conv (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa` (probabilities)\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** Gradients are computed using `ReLU_SoftMax_FC_Backward` for MLP, then `Slow_ReLU_Gradient` is called sequentially for conv layers, propagating gradients backward.\n",
    "4.  **Weight Update:** Parameters updated via $W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W_{old}}$.\n",
    "\n",
    "The loss is plotted to observe learning. The padding and stride parameters in `Slow_ReLU_Conv` calls are set to match the PyTorch model architecture, ensuring the flattened features `imlps` have the correct dimension (2048) for the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToBeTrained = True\n",
    "if ToBeTrained:\n",
    "    avg_loss = []\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    numEpochs = 20\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    loop = tqdm(range(numEpochs))\n",
    "    for i in loop:\n",
    "        c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "        \n",
    "        # Forward\n",
    "        sfts = time.time() # slow forward time start\n",
    "        c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "        c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "        c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "\n",
    "        imlps = c3s.reshape(1,-1)\n",
    "        fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "        sfte = time.time() # slow forward time end\n",
    "        sft = sfte - sfts\n",
    "        forward_time.append(sft)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa,train_labels[0])\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        sbts = time.time() # slow backward time start\n",
    "        dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "        dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "        gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "\n",
    "        gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "        gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "        sbte = time.time() # slow backward time end\n",
    "        sbt = sbte - sbts\n",
    "        backward_time.append(sbt)\n",
    "\n",
    "        # Weights update\n",
    "        w1 -= lr*dL_dw1\n",
    "        b1 -= lr*dL_db1\n",
    "        w2 -= lr*dL_dw2\n",
    "        b2 -= lr*dL_db2\n",
    "        k3 -= lr*gk3\n",
    "        k2 -= lr*gk2\n",
    "        k1 -= lr*gk1\n",
    "        bc3 -= lr*gb3.reshape(-1)\n",
    "        bc2 -= lr*gb2.reshape(-1)\n",
    "        bc1 -= lr*gb1.reshape(-1)\n",
    "        \n",
    "        if len(avg_loss) >= 2:\n",
    "            loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "    plt.plot(avg_loss)\n",
    "    plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef545289",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 3.6265 s\n",
    "- average backward time : 9.8262 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Slow Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea8a5",
   "metadata": {},
   "source": [
    "### Test for Fast approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1f5ac",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a336a9",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d6af1",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fff279",
   "metadata": {},
   "source": [
    "### Training the \"Fast\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This tests training using the Im2Col-based `Fast_ReLU_Conv` and the revised `Fast_ReLU_Gradient` (from cell `c808bdb6`) on a single image.\n",
    "\n",
    "**Per-Epoch Steps (differences from \"Slow\" are conv/grad functions):**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> Fast_ReLU_Conv (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> Fast_ReLU_Conv (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> Fast_ReLU_Conv (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa`\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** `ReLU_SoftMax_FC_Backward` for MLP, then `Fast_ReLU_Gradient` (using `sliding_window_view` for `gi` and `gk`) for conv layers.\n",
    "4.  **Weight Update:** Standard gradient descent.\n",
    "\n",
    "The loss is plotted. Consistent padding/stride ensures correct feature dimensions for the MLP. This setup tests the learning capability and performance of the more optimized NumPy convolution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = Fast_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "    c2s,mask2s = Fast_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = Fast_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = Fast_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "    gi2,gk2,gb2 = Fast_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "    gi1,gk1,gb1 = Fast_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd84d2",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 0.0022 s\n",
    "- average backward time : 0.0097 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Fast Approach.png\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
