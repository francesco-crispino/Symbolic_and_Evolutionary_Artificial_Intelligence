{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106183a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05471163",
   "metadata": {},
   "source": [
    "This section handles the loading and initial preparation of the MNIST dataset. MNIST contains 28x28 pixel grayscale images of handwritten digits (0-9).\n",
    "\n",
    "**Key Operations:**\n",
    "\n",
    "1.  **Data Loading (`load_mnist_images`, `load_mnist_labels`):**\n",
    "    *   These functions read the MNIST dataset from its specific binary file format.\n",
    "    *   Image data is reshaped to `(num_images, rows, cols)`.\n",
    "\n",
    "2.  **One-Hot Encoding Labels:**\n",
    "    *   For multi-class classification with a softmax output and categorical cross-entropy loss, integer labels (e.g., digit `5`) are converted into a one-hot vector format (e.g., `[0,0,0,0,0,1,0,0,0,0]` for 10 classes).\n",
    "    *   This represents the true label as a probability distribution where the correct class has a probability of 1.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"images/mnist_digits.png\", style=\"border-radius:20px;\", width=\"50%\">\n",
    "    <figcaption>Samples from the MNIST dataset</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9edcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "#-------------- Data Extraction ---------------------------\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097cf66",
   "metadata": {},
   "source": [
    "## PyTorch CNN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8be5b",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is defined using PyTorch's `nn.Module` to serve as a reference and source of pre-trained weights.\n",
    "\n",
    "**Architecture (defined as `SimpleCNN` class):**\n",
    "\n",
    "1.  **Conv1 + ReLU1:** `nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 1, 28, 28)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1 = \\lfloor \\frac{(28 - 2 + 0)}{2} \\rfloor + 1 = 14$\n",
    "    *   Output: `(B, 32, 14, 14)`\n",
    "\n",
    "2.  **Conv2 + ReLU2:** `nn.Conv2d(32, 64, 2, 2, 1)`\n",
    "    *   Input: `(B, 32, 14, 14)`\n",
    "    *   Padded input dimension: $14 + 2*1 = 16$\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(16 - 2 + 0)}{2} \\rfloor + 1 = 8$\n",
    "    *   Output: `(B, 64, 8, 8)`\n",
    "\n",
    "3.  **Conv3 + ReLU3:** `nn.Conv2d(64, 128, 2, 2, 0)`\n",
    "    *   Input: `(B, 64, 8, 8)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(8 - 2 + 0)}{2} \\rfloor + 1 = 4$\n",
    "    *   Output: `(B, 128, 4, 4)`\n",
    "\n",
    "4.  **Flatten:** `nn.Flatten()`\n",
    "    *   Input: `(B, 128, 4, 4)`\n",
    "    *   Output: `(B, 128 * 4 * 4)` which is `(B, 2048)`\n",
    "\n",
    "5.  **FC1 + ReLU4:** `nn.Linear(in_features=2048, out_features=250)`\n",
    "    *   Input: `(B, 2048)`\n",
    "    *   Operation: $Y = XW^T + b$\n",
    "    *   Output: `(B, 250)`\n",
    "\n",
    "6.  **FC2:** `nn.Linear(in_features=250, out_features=10)` (Output layer)\n",
    "    *   Input: `(B, 250)`\n",
    "    *   Output: `(B, 10)` (logits for 10 classes)\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"images/cnn.png\", style=\"border-radius:20px;\", height=300>\n",
    "    <figcaption>CNN Architecture (B: Batch size)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dabdea",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff84faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# # 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8a955",
   "metadata": {},
   "source": [
    "### Extracting Pre-trained Weights from PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee8995",
   "metadata": {},
   "source": [
    "This section loads weights from a pre-trained PyTorch model (`simple_cnn_mnist.pth`) and converts them into NumPy arrays. These NumPy weights will be used for our custom CNN implementations to ensure consistency for inference comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e62c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 2, 2), NumPy Shape=(64, 32, 2, 2)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 2, 2), NumPy Shape=(128, 64, 2, 2)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # A good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_1 = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pytorch_weights_of_kernels_in_layer_1\n",
    "\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pytorch_weights_of_kernels_in_layer_1.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_2 = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pytorch_weights_of_kernels_in_layer_2\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pytorch_weights_of_kernels_in_layer_2.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_3 = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pytorch_weights_of_kernels_in_layer_3\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pytorch_weights_of_kernels_in_layer_3.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pytorch_fc1_layer_weights = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pytorch_fc1_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pytorch_fc1_layer_biases = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pytorch_fc1_layer_biases.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pytorch_fc1_layer_weights.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pytorch_fc1_layer_biases.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pytorch_fc2_layer_weights = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pytorch_fc2_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pytorch_fc2_layer_biases = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pytorch_fc2_layer_biases.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pytorch_fc2_layer_weights.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pytorch_fc2_layer_biases.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06822b",
   "metadata": {},
   "source": [
    "## CNN - NumPy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c17c8",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ca069",
   "metadata": {},
   "source": [
    "Zero-padding adds a border of zeros around an input image or feature map before convolution. For example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\quad \\xrightarrow{\\textcolor{lightgreen}{\\textnormal{zero padding}}} \\quad\n",
    "\\begin{bmatrix}\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 1 & 2 & 3 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 4 & 5 & 6 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 7 & 8 & 9 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It's important for:\n",
    "\n",
    "1.  **Controlling Output Spatial Dimensions:** Padding can be used to maintain or control the reduction in height/width of feature maps. The output dimension (e.g., height $O_H$) is given by:\n",
    "    $$ O_H = \\left\\lfloor \\frac{I_H - K_H + 2P_H}{S_H} \\right\\rfloor + 1 $$\n",
    "    where $I_H$ is input height, $K_H$ kernel height, $P_H$ padding on one side of height, and $S_H$ stride.\n",
    "2.  **Improving Feature Extraction at Borders:** Allows the kernel to process edge pixels more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70273b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "image_3_by_3 = np.arange(1,37).reshape(2,2,3,3)\n",
    "padded_image_3_by_3 = np.pad(image_3_by_3,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(image_3_by_3)\n",
    "print(padded_image_3_by_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00035f0",
   "metadata": {},
   "source": [
    "### Matrix Dilatation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6aa4a",
   "metadata": {},
   "source": [
    "The `dilateOne` function \"dilates\" an input matrix by inserting a single row and column of zeros between existing rows and columns along its last two spatial dimensions. This means it inserts $S-1$ zeros when the forward convolution stride $S$ was 2.\n",
    "\n",
    "**Relevance in Backpropagation for $\\frac{\\partial L}{\\partial X}$:**\n",
    "\n",
    "This dilation operation is a critical step when computing the gradient of the loss with respect to the input of a convolutional layer ($\\frac{\\partial L}{\\partial X}$), especially if the forward pass utilized a stride $S > 1$. Here is why:\n",
    "* When a forward convolution uses a stride $S > 1$, it effectively downsamples the input, resulting in an output feature map $Z$ with smaller spatial dimensions.\n",
    "* To calculate $\\frac{\\partial L}{\\partial X}$, we need to use the gradient flowing back from the subsequent layer, $\\frac{\\partial L}{\\partial Z}$ (where $Z$ is the output of the strided convolution). **Since the original input $X$ has larger spatial dimensions than $Z$, the gradient $\\frac{\\partial L}{\\partial Z}$ must be \"upsampled\" or \"spread out\" before it can be convolved with the kernel weights to produce a gradient of the correct shape for $X$.**\n",
    "\n",
    "**Dilation Step:** This upsampling is achieved by inserting $S-1$ rows and columns of zeros between the elements of $\\frac{\\partial L}{\\partial Z}$. The `dilateOne` function in this notebook performs this specific operation for a stride $S=2$ (inserting one row/column of zeros).\n",
    "\n",
    "After $\\frac{\\partial L}{\\partial Z}$ is dilated to form $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}$, it is then typically padded (with $K-1$ zeros where $K$ is the kernel dimension, adjusted for any original padding) and subsequently convolved with the 180-degree rotated (or flipped) kernel ($W_{rot180}$). This entire sequence of operations (padding the dilated output gradient and convolving it with the flipped kernel) is what yields $\\frac{\\partial L}{\\partial X}$ and is often referred to as a \"full convolution\" in this context (see \"A guide to convolution arithmetic for deep learning\" by Dumoulin and Visin, or the provided articles by Mayank Kaushik).\n",
    "\n",
    "The image below illustrates the concept of dilating an output gradient tensor. This dilation is a preparatory step before the gradient is used in further convolution operations during backpropagation for layers that had striding in their forward pass.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*luRORFyTmj9mJ7rVhzlbZA.png\" height=250, style=\"border-radius:20px;\">\n",
    "</figure>\n",
    "\n",
    "Illustrative example of dilating an output gradient tensor (like $\\frac{\\partial L}{\\partial Z}$) by inserting $S-1$ (stride minus one) zeros. For `dilateOne`, $S=2$, so one zero is inserted between elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd18f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Shape: (1, 1, 2, 2)\n",
      "Dilated Image Shape: (1, 1, 4, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dilate(tensor, stride):\n",
    "    if stride == 1:\n",
    "        return tensor\n",
    "\n",
    "    batch_size, num_channels, height, width = tensor.shape\n",
    "    \n",
    "    dilated_height = height + (height - 1) * (stride - 1)\n",
    "    dilated_width = width + (width - 1) * (stride - 1)\n",
    "    \n",
    "    dilated_tensor = np.zeros((batch_size, num_channels, dilated_height, dilated_width))\n",
    "    dilated_tensor[:, :, ::stride, ::stride] = tensor\n",
    "    return dilated_tensor\n",
    "\n",
    "# Example usage of dilate function\n",
    "example_image = np.arange(5, 9).reshape(1, 1, 2, 2)\n",
    "dilated_image = dilate(example_image, 3)\n",
    "print(\"Original Image Shape:\", example_image.shape)\n",
    "print(\"Dilated Image Shape:\", dilated_image.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 2), gridspec_kw={'width_ratios': [0.3, 1]})\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(example_image[0, 0], cmap='gray', vmin=0, vmax=8)\n",
    "plt.xticks([0, 1], [1, 2])\n",
    "plt.yticks([0, 1], [1, 2])\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(dilated_image[0, 0], cmap='gray', vmin=0, vmax=8)\n",
    "plt.xticks([0, 1, 2, 3], [1, 2, 3, 4])\n",
    "plt.yticks([0, 1, 2, 3], [1, 2, 3, 4])\n",
    "plt.title('Dilated Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa841de1",
   "metadata": {},
   "source": [
    "### Benchmark network (debug and testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c783ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class TesterCNN(nn.Module):\n",
    "    def __init__(self, kernels: torch.Tensor, biases: torch.Tensor = None, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernels.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(biases is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernels)\n",
    "            if biases is not None:\n",
    "                self.conv.bias.copy_(biases)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if biases is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817601c0",
   "metadata": {},
   "source": [
    "## Nested-Loops Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4351c2f",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26d187",
   "metadata": {},
   "source": [
    "`nested_loop_convolution` implements a 2D convolution followed by ReLU activation using explicit nested loops. This is computationally highly inefficient.\n",
    "\n",
    "**Inputs:**\n",
    "    *   `batch_of_images`: size `(N, C_in, H_in, W_in)`.\n",
    "    *   `kernels`: size `(C_out, C_in, K_H, K_W)`.\n",
    "    *   `biases`: Per-filter biases `(C_out,)`.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Padding & Output Size:** Input `batch_of_images` is padded to better extract information from tha borders of the images. Output dimensions $(O_H, O_W)$ are calculated using the formula in the Padding section.\n",
    "2.  **Convolution:** The convolution is computed explicitly, iterating over each element of the patches from the input tensor and multiplying them by the \"sliding\" kernels. For each output element $(n, f, y_{out}, x_{out})$:\n",
    "    $$ \\text{Output}(n, f, y_{out}, x_{out}) = \\left( \\sum_{c=0}^{C_{in}-1} \\sum_{k_y=0}^{K_H-1} \\sum_{k_x=0}^{K_W-1} \\text{Img}_{pad}(n, c, y_{out}S + k_y, x_{out}S + k_x) \\cdot \\text{Ker}(f, c, k_y, k_x) \\right) + \\text{Bias}(f) $$\n",
    "3.  **ReLU Activation:** If `applyReLU=True`: $\\text{ActivatedOutput} = \\max(0, \\text{Output})$. A binary `mask` (1 where Output > 0, else 0) is also returned for backpropagation.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/refs/heads/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Convolution of a 7x7 image (blue) with a 3x3 kernel (dark blue), resulting in a 5x5 output (green). <br><i>(Source: Dumoulin & Visin, A guide to convolution arithmetic for deep learning)</i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51071b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------example_image--------\n",
      "[[[[1. 2. 3.]\n",
      "   [4. 5. 6.]\n",
      "   [7. 8. 9.]]]]\n",
      "-------example_kernel-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Nested-Loop approach-------\n",
      "[[[[  5.  19.]\n",
      "   [ 37.  78.]]\n",
      "\n",
      "  [[ 10.  40.]\n",
      "   [ 82. 191.]]]]\n",
      "-------PyTorch model-------\n",
      "tensor([[[[  5.,  19.],\n",
      "          [ 37.,  78.]],\n",
      "\n",
      "         [[ 10.,  40.],\n",
      "          [ 82., 191.]]]])\n"
     ]
    }
   ],
   "source": [
    "def nested_loop_convolution(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        output_channels, input_channels, kernel_width, kernel_height = kernels.shape\n",
    "        kernel_channels = output_channels\n",
    "    else: # Backward case\n",
    "        input_channels, output_channels, kernel_width, kernel_height = kernels.shape\n",
    "        kernel_channels = input_channels\n",
    "\n",
    "    # biases has shape (output_channels, 1, 1). It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # The number of channels taken in input by the kernel 'input_channels' must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    batch_size, channels, image_height, image_width  = batch_of_images.shape\n",
    "    output_image_height = int(((image_height - kernel_height) / stride) + 1) # new image height # Padding is already added\n",
    "    output_image_width = int(((image_width - kernel_width) / stride) + 1) # new image width\n",
    "    output = np.zeros((batch_size, output_channels, output_image_height, output_image_width)).astype(np.float32) # new image\n",
    "\n",
    "    if input_channels != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({input_channels}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for single_image in range(batch_size):\n",
    "        for single_kernel_channel in range(kernel_channels):\n",
    "            for output_row_idx in range(output_image_height): # which cycles row by row of the new image\n",
    "                for output_col_idx in range(output_image_width): # which cycles column by column of the new image\n",
    "                    output_cell_accumulator = 0.0  # sum for the current output cell (accumulates the convolution result)\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(input_channels): # channels == input_channels\n",
    "                        for kernel_row_idx in range(kernel_height):\n",
    "                            # Position of the kernel over the input image: row\n",
    "                            input_row_idx = (output_row_idx * stride) + kernel_row_idx\n",
    "                            for kernel_col_idx in range(kernel_width):\n",
    "                                # Position of the kernel over the input image: column\n",
    "                                input_col_idx = (output_col_idx * stride) + kernel_col_idx\n",
    "\n",
    "                                # Check if the position is inside the image\n",
    "                                if 0 <= input_row_idx < image_height and 0 <= input_col_idx < image_width:\n",
    "                                    input_element = batch_of_images[single_image, channel, input_row_idx, input_col_idx]\n",
    "                                    kernel_element = kernels[single_kernel_channel, channel, kernel_row_idx, kernel_col_idx]\n",
    "                                    # Compute the convolution sum (to be done for each element of the sliding kernel over the image)\n",
    "                                    output_cell_accumulator += (input_element * kernel_element).astype(np.float32)\n",
    "\n",
    "                    # Assign the result to the output image\n",
    "                    output[single_image, single_kernel_channel, output_row_idx, output_col_idx] = output_cell_accumulator\n",
    "\n",
    "    if biases.all() != 0:\n",
    "        biases = biases.reshape(biases.shape[0],1,1)\n",
    "\n",
    "        if biases.shape[0] != output_channels:\n",
    "            raise ValueError(f\"biases dimension ({biases.shape[0]}) doesn't match kernel's number of channels ({output_channels})\")\n",
    "        \n",
    "        output = output + biases\n",
    "\n",
    "    output = output.astype(np.float32)\n",
    "\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0, output)\n",
    "        mask = output.copy()\n",
    "        mask[mask > 0] = 1\n",
    "\n",
    "        return output, mask\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "example_image = np.arange(1,3*3+1).reshape(1,1,3,3).astype(np.float32)\n",
    "print(\"-------example_image--------\")\n",
    "print(example_image)\n",
    "\n",
    "example_kernel = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------example_kernel-------\")\n",
    "print(example_kernel)\n",
    "\n",
    "example_biases = np.array([1,2]).reshape(2,1,1)\n",
    "outpout, mask = nested_loop_convolution(example_image, example_kernel, biases=example_biases, padding=1, stride=2)\n",
    "print(\"-------Nested-Loop approach-------\")\n",
    "print(outpout)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "example_kernel = torch.from_numpy(example_kernel).float()\n",
    "example_biases = torch.from_numpy(np.array([1,2])).float()\n",
    "testerCNNmodel = TesterCNN(kernels=example_kernel,biases=example_biases, stride=2, padding=1)\n",
    "\n",
    "x = torch.from_numpy(example_image)\n",
    "y = testerCNNmodel(x)\n",
    "print(\"-------PyTorch model-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bad0a3",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff4adf",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Backward\n",
    "\n",
    "We now need to implement the backward pass of a convolutional layer, calculating the gradients of the loss function with respect to the layer's weights (or kernels) ($W^{(i)}$), biases ($b^{(i)}$), and the layer's input (images) ($X^{(i)}$).\n",
    "\n",
    "**Notation:**\n",
    "*   $L$: The Loss function.\n",
    "*   $X^{(i)}$: The input to convolutional layer $i$.\n",
    "*   $W^{(i)}$: The weights (kernels) of layer $i$.\n",
    "*   $b^{(i)}$: The biases of layer $i$.\n",
    "*   $Z^{(i)}$: The pre-activation output of layer $i$ ($Z^{(i)} = \\text{conv}(X^{(i)}, W^{(i)}) + b^{(i)}$).\n",
    "*   $A^{(i)}$: The activated output of layer $i$ (e.g., $A^{(i)} = \\text{ReLU}(Z^{(i)})$).\n",
    "*   $\\frac{\\partial L}{\\partial A^{(i)}}$: The gradient of the loss with respect to the activated output $A^{(i)}$ of the current layer (propagated from the next layer).\n",
    "*   $\\frac{\\partial L}{\\partial Z^{(i)}}$: The gradient of the loss with respect to the pre-activation output $Z^{(i)}$ of the current layer. This is often denoted as $\\delta^{(i)}$ in your original text after passing through the ReLU derivative.\n",
    "*   $\\text{mask}^{(i)}$: The binary mask derived from the ReLU activation in the forward pass (1 if $Z^{(i)} > 0$, else 0).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Gradient through ReLU Activation (Backward ReLU):**\n",
    "    The derivative of ReLU is 1 for positive inputs and 0 otherwise. This is efficiently implemented by element-wise multiplying the incoming gradient $\\frac{\\partial L}{\\partial A^{(i)}}$ with the `mask` computed during the forward pass.\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial Z^{(i)}} = \\frac{\\partial L}{\\partial A^{(i)}} \\odot \\text{mask}^{(i)}\n",
    "    $$\n",
    "    where $\\odot$ denotes the Hadamard (element-wise) product. Henceforth, for brevity, we will denote $\\frac{\\partial L}{\\partial Z^{(i)}}$ as $\\delta_Z^{(i)}$.\n",
    "\n",
    "2.  **Gradient with respect to Weights ($W^{(i)}$):**\n",
    "    To calculate $\\frac{\\partial L}{\\partial W^{(i)}}$, we need to correlate the layer's input $X^{(i)}$ with the pre-activation output gradient $\\delta_Z^{(i)}$. Each weight $W_{f,c,k_y,k_x}$ contributes to multiple elements of the output $Z^{(i)}$. Its gradient is the sum of all these contributions. Mathematically, this is equivalent to a convolution between the input $X^{(i)}$ (appropriately padded as in the forward pass) and the gradient $\\delta_Z^{(i)}$ (which is treated as the \"kernel\" for this convolution).\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(X^{(i)}_{padded}, \\delta_Z^{(i)})\n",
    "    $$\n",
    "    This convolution must always have a stride = 1.\n",
    "* **Dilation of $\\delta_Z^{(i)}$ (if forward stride > 1):** If the forward pass used a stride $S > 1$, $\\delta_Z^{(i)}$ must be dilated by inserting $S-1$ zeros between its elements *before* this convolution for $\\frac{\\partial L}{\\partial W^{(i)}}$. **This technique allows the use of a standard convolution**, where the \"kernel\" ($\\delta_Z^{(i)}$) slides over the \"image\" ($X^{(i)}$), and makes the output's size equal to the one of the original kernel.\n",
    "    \n",
    "    (If $X^{(i)}$ is $(N, C_{in}, H_{in}, W_{in})$ and $\\delta_Z^{(i)}$ is $(N, C_{out}, H_{out}, W_{out})$, the result $\\frac{\\partial L}{\\partial W^{(i)}}$ will have shape $(C_{out}, C_{in}, K_H, K_W)$, as the original kernel.)\n",
    "\n",
    "3.  **Gradient with respect to Input ($X^{(i)}$):**\n",
    "    To propagate the gradient to the previous layer, we also need to calculate $\\frac{\\partial L}{\\partial X^{(i)}}$. This operation is known as a \"Full Convolution\". It involves convolving the gradient $\\delta_Z^{(i)}$ with the kernels $W^{(i)}$ rotated by 180 degrees (or flipped both horizontally and vertically) ($W^{(i)}_{rot180}$).\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial X^{(i)}} = \\text{FullConvolution}(\\delta_Z^{(i)}, W^{(i)}_{rot180})\n",
    "    $$\n",
    "    ***Note:*** *The name \"Full Convolution\" refers to the fact that the first term of the operation is padded as much as possible (a larger padding would make some patches contain only zeros, so it would not make sense).*\n",
    "    \n",
    "    Full Convolution is implemented as follows:\n",
    "    *   **Dilation of $\\delta_Z^{(i)}$:** As for the gradient w.r.t. the weights, if the stride $S$ of the forward pass was greater than 1, $\\delta_Z^{(i)}$ is \"dilated\". Let's call this $\\delta_{Z,dilated}^{(i)}$. If $S=1$, $\\delta_{Z,dilated}^{(i)} = \\delta_Z^{(i)}$.\n",
    "    *   **Padding of $\\delta_{Z,dilated}^{(i)}$:** The dilated gradient is padded. If $P_{fwd}$ was the forward pass padding and $K$ the kernel dimension, the padding here is $K-1-P_{fwd}$ on each side.\n",
    "    *   **Convolution:** The padded $\\delta_{Z,dilated}^{(i)}$ is convolved with $W^{(i)}_{rot180}$. The stride of this convolution is always 1.\n",
    "    The result $\\frac{\\partial L}{\\partial X^{(i)}}$ will have the same spatial dimensions as the original input $X^{(i)}$.\n",
    "\n",
    "4.  **Gradient with respect to Biases ($b^{(i)}$):**\n",
    "    Since the bias $b_c^{(i)}$ (corresponding to filter $c$) is added to all elements of channel $c$ of the pre-activation output $Z^{(i)}$, its gradient is simply the sum of all elements of $\\delta_Z^{(i)}$ (the gradient *before* dilation) over that channel, across the spatial dimensions (height and width) and across all examples in the batch.\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n} \\sum_{h,w} (\\delta_Z^{(i)})_{n,c,h,w}\n",
    "    $$\n",
    "---\n",
    "<div style=\"text-align:center;\">\n",
    "<b>Visual example of forward and backward pass, with padding = 1 and striding = 2:</b>\n",
    "<div style=\"display:flex; flex-direction:row; justify-content:space-between; align-items:flex-start; margin-top:15px;\">\n",
    "<figure style=\"text-align:center; width:50%; flex-grow:1; margin-top:0px;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/padding_strides.gif\", style=\"border-radius:20px; height:200px;\"/>\n",
    "    <figcaption> The input image (blue) is 5x5, and the kernel (gray) is 3x3. The input has padding = 1 a stride = 2 is used. As we can see, the output is 3x3.</figcaption>\n",
    "</figure>\n",
    "<figure style=\"text-align:center; width:50%; flex-grow:1; margin-top:0px;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif\", style=\"border-radius:20px; height:200px;\"/>\n",
    "    <figcaption>Corresponding backward phase: the output gradient of the previous convolution is the 3x3 blue input. It is padded and dilated by stride-1=2-1=1 and convolved with the rotated kernel (gray) to produce the input gradient (5x5 green output).</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<figcaption style=\"text-align:center;\"><i><a href\"https://arxiv.org/pdf/1603.07285\">(Source: A guide to convolution arithmetic for deep learning - Dumoulin & Visin)</a></i>\n",
    "</fgcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1928d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (1, 1, 7, 7)\n",
      "kernel: (2, 1, 2, 2)\n",
      "d_image: (1, 2, 4, 4)\n",
      "gradient w.r.t. image: (1, 1, 7, 7)\n",
      "gradient w.r.t. kernel: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "def nested_loop_gradient(batch_of_images, d_image, kernels, mask, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution.\n",
    "    The mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image and the gradient of the kernel,\n",
    "    to backpropagate the error.\n",
    "    The gradient of the bias is also returned.\n",
    "    \"\"\" \n",
    "    ############### Gradient of Input Image ###############\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image\n",
    "    # dilated (zeros between matrix elements) by stride-1 and padded by kernel-1 in height and width.\n",
    "    # The kernel is flipped by 180 degrees and the stride is set to 1.\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, image_channels, image_height, iamge_width = batch_of_images.shape\n",
    "\n",
    "    # Backward ReLU\n",
    "    d_image = np.multiply(d_image, mask)\n",
    "\n",
    "    # Dilate the gradient of the output\n",
    "    d_image = dilate(d_image, stride)\n",
    "\n",
    "    d_image = np.pad(d_image,\n",
    "                     (\n",
    "                      (0,0),(0,0),\n",
    "                      (kernel_height-1-padding, kernel_height-1-padding),\n",
    "                      (kernel_width-1-padding, kernel_width-1-padding)\n",
    "                     ))\n",
    "    \n",
    "    batch_size, kernels_number, d_image_height, d_iamge_width = d_image.shape\n",
    "    \n",
    "    # Flip the kernel\n",
    "    flipped_kernel = np.rot90(kernels,2,(-2,-1))\n",
    "\n",
    "    # Computation\n",
    "    gradient_wrt_image = np.zeros_like(batch_of_images)\n",
    "    current_sum = 0.0\n",
    "    for single_image in range(batch_size):\n",
    "        for image_row_idx in range(image_height):\n",
    "            for image_col_idx in range(iamge_width):\n",
    "                for single_kernel in range(kernels_number):\n",
    "                    for input_channel in range(kernel_channels):\n",
    "                        for kernel_row_idx in range(kernel_height):\n",
    "                            y = image_row_idx + kernel_row_idx\n",
    "\n",
    "                            for kernel_col_idx in range(kernel_width):\n",
    "                                x = image_row_idx + kernel_col_idx\n",
    "\n",
    "                                if 0 <= y < d_image.shape[-2] and 0 <= x < d_image.shape[-1]:\n",
    "                                    input_element = d_image[single_image,single_kernel,y,x]\n",
    "                                    kernel_element = flipped_kernel[single_kernel,input_channel,kernel_row_idx,kernel_col_idx] \n",
    "                                else:\n",
    "                                    break\n",
    "\n",
    "                                current_sum += input_element*kernel_element\n",
    "\n",
    "                    gradient_wrt_image[single_image,input_channel,image_row_idx,image_col_idx] = current_sum\n",
    "                    current_sum = 0.0\n",
    "\n",
    "    ############### Gradient of Kernel ###############\n",
    "    # The computation consists in a convolution between the original image and the dilated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    gradient_wrt_kernel = np.zeros_like(kernels)\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    current_sum = 0.0\n",
    "\n",
    "    for single_image in range(batch_size):\n",
    "        for kernel_row_idx in range(kernel_height):\n",
    "            for kernel_col_idx in range(kernel_width):\n",
    "                for input_channel in range(kernel_channels):\n",
    "                    # The number of kernels determines the number of channels of the output tensor\n",
    "                    for outpot_channel in range(kernels_number):\n",
    "                        for d_image_row_idx in range(d_image_height):\n",
    "                            y = kernel_row_idx + d_image_row_idx\n",
    "\n",
    "                            for d_image_col_idx in range(d_iamge_width):\n",
    "                                x = kernel_row_idx + d_image_col_idx\n",
    "\n",
    "                                if 0 <= y < image_height and 0 <= x < iamge_width:\n",
    "                                    input_element = batch_of_images[single_image,input_channel,y,x]\n",
    "                                    kernel_element = d_image[single_image,outpot_channel,d_image_row_idx,d_image_col_idx] \n",
    "                                    current_sum += input_element*kernel_element\n",
    "                                else:\n",
    "                                    break\n",
    "\n",
    "                        gradient_wrt_kernel[outpot_channel,input_channel,kernel_row_idx,kernel_col_idx] = current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "    ############### Gradient of Bias ###############\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gradient_wrt_bias = d_image.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    return gradient_wrt_image, gradient_wrt_kernel, gradient_wrt_bias\n",
    "\n",
    "kernel_channels = 1\n",
    "kernels_number = 2\n",
    "image_side_len = 7\n",
    "kernel_side_len = 2\n",
    "example_stride = 2\n",
    "example_padding = 1\n",
    "\n",
    "example_image = np.arange(1, kernel_channels * image_side_len * image_side_len + 1)\n",
    "example_image = example_image.reshape(1, kernel_channels, image_side_len, image_side_len)\n",
    "\n",
    "example_kernel = np.arange(1, kernels_number * kernel_channels * (kernel_side_len**2) + 1)\n",
    "example_kernel = example_kernel.reshape(kernels_number, kernel_channels, kernel_side_len, kernel_side_len)\n",
    "\n",
    "example_d_image, mask = nested_loop_convolution(example_image, example_kernel, stride=example_stride, padding=example_padding) \n",
    "example_d_image = example_d_image / np.mean(example_d_image)\n",
    "\n",
    "image_grad, kernel_grad, bias_grad = nested_loop_gradient(example_image, example_d_image, example_kernel, mask, stride=example_stride, padding=example_padding)\n",
    "\n",
    "print(f\"image: {example_image.shape}\")\n",
    "print(f\"kernel: {example_kernel.shape}\")\n",
    "print(f\"d_image: {example_d_image.shape}\")\n",
    "print(f\"gradient w.r.t. image: {image_grad.shape}\")\n",
    "print(f\"gradient w.r.t. kernel: {kernel_grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2fbc0",
   "metadata": {},
   "source": [
    "## Im2Col Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae10455",
   "metadata": {},
   "source": [
    "### Im2Col approach: Convolutional Layer - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c44f",
   "metadata": {},
   "source": [
    "`im2col_convolution` implements 2D convolution more efficiently using an Im2Col approach using`sliding_window_view` and optimized matrix multiplication.\n",
    "\n",
    "**Im2Col Core Idea:**\n",
    "\n",
    "* **Input Patch Extraction:**\n",
    "    *   `window_m = ... .reshape((-1,(kw*kh*nc)))`: Flattens each extracted patch into a row vector of size `kw*kh*nc`. `window_m` (our $X_{col}$) thus has shape `(N_patches, patch_size)`.\n",
    "    *   `kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)`: Flattens each filter `(kc, ac, kw, kh)` -> `(kc, ac*kw*kh)`. This is $W_{col}$, shape `(patch_size, C_out)`.\n",
    "    Following the example in the animation below, we have a 4x4 RGB image (so **3x4x4**) that has to be convoluted by a **2x2** kernel (represented as the red outlines sliding over the image). The kernel will slide **9** times over the image and, for each slide, the number of multiplications and values to be summed is 4 (elements of the kernel) times 3 (channels of both the image and the kernel) that equals **12**. Hence, as we can see in the animation below, the flattened patches are vectors of **12** elements, and in total they are **9**, as the slides performed by the kernel.\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/1.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Each patch is extracted and flattened into a vector</figcaption>\n",
    "</figure>\n",
    "\n",
    "* Kernel Reshaping:\n",
    "    To perform the convolution as a vetor-matrix multilication (or matrix-matrix multiplication) we then have to flatten the kernel(s) into row vectors\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/10.png\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Kernels are flattened into a vector</figcaption>\n",
    "</figure>\n",
    "\n",
    "* **Convolution as Matrix Multiplication:**\n",
    "    At this point, the convolution boils down to a simple matrix multiplication, with enormous gains in efficiency and simplicity\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/11.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Each element of the output is given by a simple and optimized dot product</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c9deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[  1.   2.   3.   4.   5.]\n",
      "   [  6.   7.   8.   9.  10.]\n",
      "   [ 11.  12.  13.  14.  15.]\n",
      "   [ 16.  17.  18.  19.  20.]\n",
      "   [ 21.  22.  23.  24.  25.]]\n",
      "\n",
      "  [[ 26.  27.  28.  29.  30.]\n",
      "   [ 31.  32.  33.  34.  35.]\n",
      "   [ 36.  37.  38.  39.  40.]\n",
      "   [ 41.  42.  43.  44.  45.]\n",
      "   [ 46.  47.  48.  49.  50.]]\n",
      "\n",
      "  [[ 51.  52.  53.  54.  55.]\n",
      "   [ 56.  57.  58.  59.  60.]\n",
      "   [ 61.  62.  63.  64.  65.]\n",
      "   [ 66.  67.  68.  69.  70.]\n",
      "   [ 71.  72.  73.  74.  75.]]\n",
      "\n",
      "  [[ 76.  77.  78.  79.  80.]\n",
      "   [ 81.  82.  83.  84.  85.]\n",
      "   [ 86.  87.  88.  89.  90.]\n",
      "   [ 91.  92.  93.  94.  95.]\n",
      "   [ 96.  97.  98.  99. 100.]]]]\n",
      "-------ker-------\n",
      "[[[[ 1  2]\n",
      "   [ 3  4]]\n",
      "\n",
      "  [[ 5  6]\n",
      "   [ 7  8]]\n",
      "\n",
      "  [[ 9 10]\n",
      "   [11 12]]\n",
      "\n",
      "  [[13 14]\n",
      "   [15 16]]]\n",
      "\n",
      "\n",
      " [[[17 18]\n",
      "   [19 20]]\n",
      "\n",
      "  [[21 22]\n",
      "   [23 24]]\n",
      "\n",
      "  [[25 26]\n",
      "   [27 28]]\n",
      "\n",
      "  [[29 30]\n",
      "   [31 32]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[ 7689.]]\n",
      "\n",
      "  [[18314.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[ 7689.]]\n",
      "\n",
      "  [[18314.]]]]\n"
     ]
    }
   ],
   "source": [
    "def im2col_convolution(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "\n",
    "    ###### im2col approach steps: ######\n",
    "    # 1. Pad the input image as needed\n",
    "    if padding > 0:\n",
    "        batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    # 2. Extract sliding windows from the input image, considering the kernel size and stride\n",
    "    sliding_windows = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,input_channels,kernel_height,kernel_width))[:,:,::stride,::stride]\n",
    "\n",
    "    # 3. Flatten the sliding windows and the kernel to prepare for matrix multiplication\n",
    "    sliding_windows = sliding_windows.reshape((-1,(kernel_height * kernel_width * input_channels)))\n",
    "\n",
    "    kernels = kernels.reshape((-1,(kernel_height*kernel_width*input_channels))).transpose(1,0)\n",
    "\n",
    "    # Now, the convolution can be performed as a matrix multiplication, where each row of the sliding windows\n",
    "    # corresponds to a flattened patch of the input image, and each column of the kernels corresponds to a flattened kernel (channel-wise)\n",
    "    images_dot_kernels = np.matmul(sliding_windows, kernels).astype(np.float32) # It's called 'images_dot_kernels' because it contains the result of the whole convolution operation\n",
    "\n",
    "    # Compute the output dimensions to reshape the resulting matrix (each row corresponds to a patch)\n",
    "    output_width = int(((image_width - kernel_width) / stride) + 1)\n",
    "    output_height = int(((image_height - kernel_height) / stride) + 1)\n",
    "\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output = images_dot_kernels.reshape(batch_size, output_width, output_height, kernels_number)\n",
    "\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    output = output.transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "    # Add biases if they are provided\n",
    "    if biases.any() != 0:\n",
    "        output = (output + biases.reshape(1,-1,1,1))\n",
    "\n",
    "    # Apply ReLU activation if specified\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0, output)\n",
    "\n",
    "    # Create a mask for the backward operation of ReLU activation\n",
    "    mask = np.copy(output)\n",
    "    mask[mask > 0] = 1\n",
    "\n",
    "    return output, mask\n",
    "\n",
    "img = np.arange(1,4*5*5+1).reshape(1,4,5,5).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,32+1).reshape(2,4,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "\n",
    "s = 4\n",
    "\n",
    "res,mask = nested_loop_convolution(img, ker, bias,padding=0,stride=s)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = im2col_convolution(img, ker, bias,padding=0,stride=s)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c414872",
   "metadata": {},
   "source": [
    "### Im2Col approach: Convolutional Layer - Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e1403",
   "metadata": {},
   "source": [
    "`im2col_gradient` computes the following gradients using again the im2col approach:\n",
    "* $\\frac{\\partial L}{\\partial X}$ (`gi`): gradient w.r.t. the input images;\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial W}$ (`gk`): gradient w.r.t. the kernels;\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial b}$ (`gb`): gradient w.r.t. the biases.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Backward ReLU:** $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\text{mask}$<br>\n",
    "    \"$\\text{mask}$\" is a matrix (tensor) whose elements are 1 if the corrisponding elements in the output of a given layer were > 0. It enables the flowing of the gradient only through elements that contributed to form the output in the first place.\n",
    "\n",
    "2.  **Gradient w.r.t. Input (`gi`):**\n",
    "    $\\frac{\\partial L}{\\partial X} = \\text{FullConv}(\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}, W_{rot180})$<br>\n",
    "    A full convolution is needed, since we must obtain a result with the same size as the input image. As you remember, the output size is given by this formula: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1$, so we must \"invert\" it. In particular, the output tensor must be:\n",
    "    * dilated by $stride - 1$ and\n",
    "    * padded by:\n",
    "    $(kernel\\_height-1-input\\_padding{\\textnormal{, }}kernel\\_width-1-input\\_padding)$\n",
    "\n",
    "2.  **Gradient w.r.t. Kernel (`gk`):**\n",
    "    This is $\\frac{\\partial L}{\\partial W} = \\text{Conv}(X_{padded}, \\frac{\\partial L}{\\partial Z})$.\n",
    "   \n",
    "\n",
    "3.  **Gradient w.r.t. Bias (`gb`):** $\\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} (\\frac{\\partial L}{\\partial Z})_{n,f,h,w}$. (`gb = d_img.sum((-1,-2))`).\n",
    "\n",
    "**But what if, in the forward phase, the stride was greater than 1?**<br>\n",
    "Dilation was already mentioned, but here we will give you again an explanation, but with a detailed visual example. Consider the case in which the stride was $S= 2$ in the forward pass. The output gradient tensor needs to be dilated (in order to obtain a tensor with the same size as the input image) before being convolved with the kernel gradient, to be effectively useful to backpropagate the loss. A visualization of why dilation enables this fundamental result is in the gif below:\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KOHfsOHX5ujcMfr6Xjy6zQ.png\" height=\"250\", style=\"border-radius:20px 0 0 20px;\"/>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QvTW-pNwJAlbfj1LrZe8JA.gif\" height=\"250\", style=\"border-radius:0 20px 20px 0;\"/>\n",
    "    <figcaption>The backpropagation operation is identical to a stride = 1 convolution of a padded, dilated version of the output gradient tensor with a flipped version of the filter.<br><i><a href=\"https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710\">(Source: Backpropagation for Convolution with Strides, Mayank Kaushik)</a></i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c5583e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimAge: (1, 4, 3, 3)\n",
      "imAge: (1, 3, 9, 9)\n",
      "kerNel: (4, 3, 3, 3)\n",
      "dimAge: (1, 4, 3, 3)\n",
      "ggi: (1, 3, 9, 9)\n",
      "ggradient_wrt_kernels: (4, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "def im2col_gradient(batch_of_images, d_image, kernels, mask, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\"\n",
    "    ############ Gradient of Input Image ############\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image dilated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel is rotated by 180 degrees (flipped vertically and horizontally)\n",
    "    # FullConvolution(d_imageDilated, Rotated180Deg(kernel)) with stride 1\n",
    "    output_channels, input_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_image = np.multiply(d_image, mask)\n",
    "\n",
    "    # Delating the gradient of the output\n",
    "    d_image = dilate(d_image, stride)\n",
    "  \n",
    "    # Padding the gradient of the output\n",
    "    d_image_padded = np.pad(d_image,((0,0),(0,0),(kernel_height-1-padding,kernel_height-1-padding),(kernel_width-1-padding,kernel_width-1-padding)))\n",
    "\n",
    "    batch_size, output_channels, d_image_height, d_image_width = d_image.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    kernels = np.rot90(kernels,2,(-2,-1))\n",
    "\n",
    "    sliding_windows_d_image = np.lib.stride_tricks.sliding_window_view(d_image_padded,(1,output_channels,kernel_width,kernel_height))\n",
    "    sliding_windows_d_image = sliding_windows_d_image.reshape(-1,(kernel_width*kernel_height*output_channels)) # window matrix\n",
    "    \n",
    "    # Convolution\n",
    "    kernels = kernels.reshape((-1,(kernel_width*kernel_height*output_channels))).transpose(1,0)\n",
    "\n",
    "    gradient_wrt_images = np.matmul(sliding_windows_d_image, kernels).astype(np.float32).transpose(1,0).reshape(batch_of_images.shape)\n",
    "    \n",
    "    ############## Gradient of Kernel ##############\n",
    "    # The computation consists in a convolution between the original image and the dilated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    sliding_windows_batch_of_images = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,1,d_image_height,d_image_width)).reshape(-1,d_image_height*d_image_width)\n",
    "    \n",
    "    d_image = d_image.reshape(-1, d_image_height * d_image_width).transpose(1,0)\n",
    "    \n",
    "    gradient_wrt_kernels = np.matmul(sliding_windows_batch_of_images, d_image).astype(np.float32).transpose(1,0).reshape(output_channels,input_channels,kernel_height,kernel_width)\n",
    "    \n",
    "    ############### Gradient of Bias ###############\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    grdient_wrt_biases = d_image.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gradient_wrt_images, gradient_wrt_kernels, grdient_wrt_biases\n",
    "\n",
    "input_channels = 3\n",
    "output_channels = 4\n",
    "idim = 9\n",
    "kdim = 3\n",
    "s = 3\n",
    "p = 0\n",
    "\n",
    "imAge = np.arange(1,input_channels*idim*idim+1).reshape(1,input_channels,idim,idim)\n",
    "kerNel = np.arange(1,output_channels*input_channels*(kdim**2)+1).reshape(output_channels,input_channels,kdim,kdim)\n",
    "dimAge,mask = im2col_convolution(imAge,kerNel,stride=s,padding=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "\n",
    "ggi,ggradient_wrt_kernels,ggrdient_wrt_biases = im2col_gradient(imAge,dimAge,kerNel,mask,stride=s,padding=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggradient_wrt_kernels: {ggradient_wrt_kernels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d96bd4",
   "metadata": {},
   "source": [
    "### Optimized version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef184b5",
   "metadata": {},
   "source": [
    "Reading the extremely interesting article [Faster Matrix Multiplications in Numpy](https://www.benjaminjohnston.com.au/matmul) by Benjamin Johnson, we extracted fundamental tips and techniques that allowed us to significantly improve the performances of our im2col approach. Following the order of the steps in the article, we now will briefly present them and apply them to our code.\n",
    "\n",
    "1. **Use BLAS directly, instead of relying on BLAS-based operations of NumPy:** *\"BLAS is a high-performance matrix library. Even though NumPy uses BLAS, I've noticed performance can be improved by calling BLAS directly. Perhaps this is simply because using direct calls to BLAS forces you to shape your data ready for use with BLAS.<br>Replace numpy.matmul with `scipy.linalg.blas.sgemm(...)` for float32 matrix-matrix multiplication and `scipy.linalg.blas.sgemv(...)` for float32 matrix-vector multiplication.\"*\n",
    "2. **Use the fastest BLAS possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67935e54",
   "metadata": {},
   "source": [
    "* Specifi-transposition (to optimize BLAS operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa1dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_optimized(batch_of_images, kernels, biases=None, padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    if kernel_channels != input_channels:\n",
    "        raise ValueError(f\"Numero di canali del kernel ({kernel_channels}) non corrisponde ai canali dell'input ({input_channels})\")\n",
    "\n",
    "    # Calcola dimensioni output\n",
    "    output_height = (image_height - kernel_height + 2 * padding) // stride + 1\n",
    "    output_width = (image_width - kernel_width + 2 * padding) // stride + 1\n",
    "\n",
    "    # Padding\n",
    "    if padding > 0:\n",
    "        batch_of_images_padded = np.pad(batch_of_images, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    else:\n",
    "        batch_of_images_padded = batch_of_images\n",
    "\n",
    "    # Estrarre le patch usando sliding_window_view (im2col)\n",
    "    # La finestra ha le dimensioni del kernel e si muove sull'immagine paddata\n",
    "    # La shape della finestra è (1 (per batch), input_channels, kernel_height, kernel_width)\n",
    "    # L'ultimo '1' è per la dimensione del batch delle patch, che è sempre 1 qui.\n",
    "    # Il '1' prima di input_channels è per il canale di output virtuale delle patch, che non ci serve qui.\n",
    "    patches = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )\n",
    "    # patches.shape ora è (batch_size, 1, H_out_stride1, W_out_stride1, 1, input_channels, kernel_height, kernel_width)\n",
    "    # H_out_stride1 e W_out_stride1 sono le dimensioni dell'output se stride fosse 1\n",
    "\n",
    "    # Applichiamo lo stride alle patch selezionate\n",
    "    patches_strided = patches[:, :, ::stride, ::stride, :, :, :, :]\n",
    "    # patches_strided.shape: (batch_size, 1, output_height, output_width, 1, input_channels, kernel_height, kernel_width)\n",
    "\n",
    "    # Reshape delle patch per la moltiplicazione matriciale (X_col)\n",
    "    # Ogni riga di X_col è una patch appiattita\n",
    "    # X_col shape: (batch_size * output_height * output_width, input_channels * kernel_height * kernel_width)\n",
    "    X_col = patches_strided.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * output_height * output_width,\n",
    "        input_channels * kernel_height * kernel_width\n",
    "    )\n",
    "\n",
    "    # Reshape dei kernel per la moltiplicazione matriciale (W_col)\n",
    "    # Ogni colonna di W_col è un kernel appiattito\n",
    "    # W_col shape: (input_channels * kernel_height * kernel_width, kernels_number)\n",
    "    W_col = kernels.reshape(kernels_number, -1).T\n",
    "    # kernels.shape (OC, IC, KH, KW) -> (OC, IC*KH*KW) -> .T -> (IC*KH*KW, OC)\n",
    "\n",
    "    # Moltiplicazione matriciale per ottenere l'output srotolato\n",
    "    # output_col shape: (batch_size * output_height * output_width, kernels_number)\n",
    "    output_col = np.matmul(X_col, W_col)\n",
    "\n",
    "    # Reshape dell'output nella forma 4D desiderata\n",
    "    # (batch_size, output_height, output_width, kernels_number)\n",
    "    output = output_col.reshape(batch_size, output_height, output_width, kernels_number)\n",
    "    # Trasponi per avere (batch_size, kernels_number, output_height, output_width)\n",
    "    output = output.transpose(0, 3, 1, 2)\n",
    "\n",
    "    # Aggiunta dei bias\n",
    "    if biases is not None and biases.any() != 0:\n",
    "        output = output + biases.reshape(1, -1, 1, 1) # biases.shape = (kernels_number,)\n",
    "\n",
    "    mask = None # Inizializza la maschera\n",
    "\n",
    "    if applyReLU:\n",
    "        output_activated = np.maximum(0, output)\n",
    "        mask = (output_activated > 0).astype(output.dtype) # Maschera binaria (0 o 1)\n",
    "        output = output_activated\n",
    "    else: # Se non applichiamo ReLU, la maschera può essere considerata tutta 1 (per il backward)\n",
    "        mask = np.ones_like(output)\n",
    "\n",
    "    return output.astype(np.float32), mask.astype(np.float32)\n",
    "\n",
    "def im2col_gradient_optimized(\n",
    "    original_forward_input, # X\n",
    "    d_image, # dL/dA (gradiente rispetto all'output attivato del layer corrente)\n",
    "    kernels, # W\n",
    "    mask,    # Maschera della ReLU\n",
    "    padding, # Padding usato nella forward pass\n",
    "    stride   # Stride usato nella forward pass\n",
    "    ):\n",
    "\n",
    "    # 0. Backward ReLU: dL/dZ = dL/dA * mask\n",
    "    # (dL/dZ è il gradiente rispetto all'output del layer PRIMA della ReLU)\n",
    "    gradient_pre_activation = np.multiply(d_image, mask) # dL/dZ\n",
    "\n",
    "    # --- Parametri dimensionali ---\n",
    "    kernels_number, input_channels_kernel, kernel_height, kernel_width = kernels.shape # OC, IC, KH, KW\n",
    "    batch_size, input_channels_orig, input_height, input_width = original_forward_input.shape # BS, IC, IH, IW\n",
    "    # _, _, output_height_grad, output_width_grad = gradient_pre_activation.shape # BS, OC, OH, OW\n",
    "\n",
    "    # --- 1. Gradiente rispetto ai Bias (gradient_wrt_biases = dL/db) ---\n",
    "    # Somma `gradient_pre_activation` (dL/dZ) lungo le dimensioni batch, altezza, larghezza.\n",
    "    # gradient_pre_activation ha shape (batch_size, kernels_number, oh_grad, ow_grad)\n",
    "    # gradient_wrt_biases deve avere shape (kernels_number,)\n",
    "    gradient_wrt_biases = np.sum(gradient_pre_activation, axis=(0, 2, 3))\n",
    "\n",
    "    # --- 2. Gradiente rispetto ai Pesi (gradient_wrt_kernels = dL/dW) ---\n",
    "    # dL/dW = conv(X_padded, dL/dZ_dilated_se_necessario) o più precisamente X_col.T @ dL/dZ_col\n",
    "    # X_col sono le patch dell'input originale che hanno generato l'output.\n",
    "    # dL/dZ_col è il gradiente pre-attivazione appiattito.\n",
    "\n",
    "    # Padding dell'input originale (come nella forward)\n",
    "    if padding > 0:\n",
    "        X_padded_for_dW = np.pad(original_forward_input, ((0,0), (0,0), (padding,padding), (padding,padding)), mode='constant')\n",
    "    else:\n",
    "        X_padded_for_dW = original_forward_input\n",
    "\n",
    "    # Estrarre X_col dall'input originale paddato\n",
    "    # La finestra ha le dimensioni del kernel originale\n",
    "    patches_X_for_dW = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_padded_for_dW,\n",
    "        (1, input_channels_orig, kernel_height, kernel_width)\n",
    "    )[:, :, ::stride, ::stride, :, :, :, :] # Applica stride originale\n",
    "    # Shape: (bs, 1, oh, ow, 1, ic, kh, kw)\n",
    "\n",
    "    X_col_for_dW = patches_X_for_dW.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3], # bs * oh * ow\n",
    "        input_channels_orig * kernel_height * kernel_width # ic * kh * kw\n",
    "    )\n",
    "\n",
    "    # Reshape di dL/dZ (gradient_pre_activation)\n",
    "    # Shape: (bs, oc, oh, ow) -> (bs * oh * ow, oc)\n",
    "    dLdZ_col = gradient_pre_activation.transpose(0, 2, 3, 1).reshape(\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        kernels_number # oc\n",
    "    )\n",
    "\n",
    "    # Calcolo di dL/dW\n",
    "    # (ic*kh*kw, bs*oh*ow) @ (bs*oh*ow, oc) -> (ic*kh*kw, oc)\n",
    "    gradient_wrt_kernels_flat = np.matmul(X_col_for_dW.T, dLdZ_col)\n",
    "\n",
    "    # Reshape nella forma dei kernel originali (oc, ic, kh, kw)\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(\n",
    "        input_channels_orig, kernel_height, kernel_width, kernels_number\n",
    "    ).transpose(3, 0, 1, 2)\n",
    "\n",
    "\n",
    "    # --- 3. Gradiente rispetto all'Input (gradient_wrt_input = dL/dX) ---\n",
    "    # Questo è una convoluzione trasposta (o \"full convolution\")\n",
    "    # dL/dX = FullConv(dL/dZ_dilated, W_rot180)\n",
    "\n",
    "    # a. Dilatazione di gradient_pre_activation (dL/dZ) se lo stride della forward era > 1\n",
    "    dilated_grad_pre_activation = dilate(gradient_pre_activation, stride)\n",
    "\n",
    "    # b. Kernel per dL/dX: ruotati di 180° e canali scambiati\n",
    "    # kernels.shape = (OC, IC, KH, KW)\n",
    "    # Per la convoluzione trasposta, i canali di input/output del kernel si invertono\n",
    "    # flipped_kernels_for_dX.shape deve essere (IC_new, OC_new, KH, KW)\n",
    "    # dove IC_new = OC (canali di dL/dZ), OC_new = IC (canali di dL/dX)\n",
    "    # Quindi, shape finale (IC, OC, KH, KW)\n",
    "    flipped_kernels_for_dX = np.rot90(kernels, 2, axes=(2, 3)).transpose(1, 0, 2, 3)\n",
    "\n",
    "    # c. Padding per la convoluzione interna di dL/dX (convoluzione trasposta)\n",
    "    # Padding_effettivo_trasposta = KernelDim - 1 - Padding_Forward_Originale\n",
    "    # Questo padding si applica a dilated_grad_pre_activation\n",
    "    padding_for_dX_conv = kernel_height - 1 - padding # padding è il padding della forward originale\n",
    "\n",
    "    # d. Esegui la convoluzione (stride interno SEMPRE 1)\n",
    "    # Stiamo usando la nostra funzione di convoluzione efficiente\n",
    "    gradient_wrt_input_raw, _ = im2col_optimized(\n",
    "        dilated_grad_pre_activation, # Immagine di input per questa convoluzione\n",
    "        flipped_kernels_for_dX,      # Kernels per questa convoluzione\n",
    "        biases=None,                 # Nessun bias nel calcolo del gradiente dell'input\n",
    "        padding=padding_for_dX_conv,\n",
    "        stride=1,                    # Lo stride è sempre 1 qui\n",
    "        applyReLU=False              # Nessuna attivazione qui\n",
    "    )\n",
    "\n",
    "    # e. Crop/slice finale per assicurare le dimensioni corrette di dL/dX\n",
    "    # L'output della convoluzione trasposta potrebbe essere leggermente più grande dell'input originale\n",
    "    # a seconda delle formule di padding e dimensioni.\n",
    "    # Vogliamo che abbia le dimensioni di original_forward_input: (batch_size, input_channels_orig, input_height, input_width)\n",
    "    gradient_wrt_input = gradient_wrt_input_raw[:, :, :input_height, :input_width]\n",
    "\n",
    "    return gradient_wrt_input, gradient_wrt_kernels, gradient_wrt_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c4a34",
   "metadata": {},
   "source": [
    "* Direct call to underlying BLAS functions, without NumPy intermediation (works best if the tensors are really large):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3edc08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import sgemm\n",
    "\n",
    "def im2col_optimized_sgemm(batch_of_images, kernels, biases=None, padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "    if kernel_channels != input_channels:\n",
    "        raise ValueError(f\"Numero di canali del kernel ({kernel_channels}) non corrisponde ai canali dell'input ({input_channels})\")\n",
    "    output_height = (image_height - kernel_height + 2 * padding) // stride + 1\n",
    "    output_width = (image_width - kernel_width + 2 * padding) // stride + 1\n",
    "    \n",
    "    current_dtype = batch_of_images.dtype # Per mantenere la precisione originale finché possibile\n",
    "    if current_dtype != np.float32: # sgemm richiede float32\n",
    "        current_dtype = np.float32\n",
    "\n",
    "    if padding > 0:\n",
    "        batch_of_images_padded = np.pad(batch_of_images.astype(current_dtype, copy=False),\n",
    "                                        ((0, 0), (0, 0), (padding, padding), (padding, padding)),\n",
    "                                        mode='constant')\n",
    "    else:\n",
    "        batch_of_images_padded = batch_of_images.astype(current_dtype, copy=False)\n",
    "\n",
    "    patches = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )\n",
    "    patches_strided = patches[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col_shape = (batch_size * output_height * output_width, input_channels * kernel_height * kernel_width)\n",
    "    X_col_transposed_view = patches_strided.transpose(0, 2, 3, 5, 6, 7, 1, 4)\n",
    "    # Assicura C-contiguità per la matrice 'a' di sgemm\n",
    "    X_col = np.ascontiguousarray(X_col_transposed_view.reshape(X_col_shape), dtype=np.float32)\n",
    "\n",
    "    # Prepara i kernel per sgemm con trans_b=True\n",
    "    # kernels_reshaped avrà forma (OC, patch_size) ed è C-contigua\n",
    "    kernels_reshaped_for_sgemm = np.ascontiguousarray(kernels.reshape(kernels_number, -1), dtype=np.float32)\n",
    "\n",
    "    # sgemm calcola alpha * A @ B oppure A.T @ B ecc.\n",
    "    # Vogliamo X_col @ W_col, dove W_col = kernels.reshape(OC, -1).T\n",
    "    # Quindi, W_col ha forma (patch_size, OC).\n",
    "    # Passando kernels_reshaped_for_sgemm (OC, patch_size) e trans_b=True,\n",
    "    # sgemm calcolerà X_col @ kernels_reshaped_for_sgemm.T\n",
    "    output_col = sgemm(alpha=1.0, a=X_col, b=kernels_reshaped_for_sgemm, trans_b=True)\n",
    "\n",
    "    output = output_col.reshape(batch_size, output_height, output_width, kernels_number).transpose(0, 3, 1, 2)\n",
    "\n",
    "    if biases is not None and biases.any() != 0:\n",
    "        output = output + biases.astype(np.float32, copy=False).reshape(1, -1, 1, 1)\n",
    "\n",
    "    mask_type = output.dtype\n",
    "    if applyReLU:\n",
    "        # np.maximum può cambiare il dtype se uno degli argomenti è 0 (int). Forziamo float32.\n",
    "        output = np.maximum(0, output, dtype=np.float32)\n",
    "        mask = (output > 0).astype(mask_type) # Maschera binaria con il dtype originale (prima di forzare float32)\n",
    "    else:\n",
    "        mask = np.ones_like(output, dtype=mask_type)\n",
    "\n",
    "    return output.astype(np.float32, copy=False), mask.astype(np.float32, copy=False)\n",
    "\n",
    "def im2col_gradient_optimized_sgemm(original_forward_input, d_image, kernels, mask, padding, stride):\n",
    "    # Assicura che mask sia float32 se usata in moltiplicazioni\n",
    "    gradient_pre_activation = np.multiply(d_image, mask.astype(d_image.dtype, copy=False)) # dL/dZ\n",
    "\n",
    "    kernels_number, input_channels_kernel, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels_orig, input_height, input_width = original_forward_input.shape\n",
    "\n",
    "    gradient_wrt_biases = np.sum(gradient_pre_activation, axis=(0, 2, 3)).astype(np.float32, copy=False)\n",
    "\n",
    "    current_dtype = original_forward_input.dtype\n",
    "    if current_dtype != np.float32:\n",
    "        current_dtype = np.float32\n",
    "\n",
    "    if padding > 0:\n",
    "        X_padded_for_dW = np.pad(original_forward_input.astype(current_dtype, copy=False),\n",
    "                                 ((0,0), (0,0), (padding,padding), (padding,padding)),\n",
    "                                 mode='constant')\n",
    "    else:\n",
    "        X_padded_for_dW = original_forward_input.astype(current_dtype, copy=False)\n",
    "\n",
    "    patches_X_for_dW = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_padded_for_dW,\n",
    "        (1, input_channels_orig, kernel_height, kernel_width)\n",
    "    )[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col_for_dW_shape = (\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        input_channels_orig * kernel_height * kernel_width\n",
    "    )\n",
    "    X_col_for_dW_transposed_view = patches_X_for_dW.transpose(0, 2, 3, 5, 6, 7, 1, 4)\n",
    "    # Assicura C-contiguità per 'a' in sgemm(trans_a=True)\n",
    "    X_col_for_dW = np.ascontiguousarray(X_col_for_dW_transposed_view.reshape(X_col_for_dW_shape), dtype=np.float32)\n",
    "\n",
    "    dLdZ_col_shape = (\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        kernels_number\n",
    "    )\n",
    "    dLdZ_col_transposed_view = gradient_pre_activation.transpose(0, 2, 3, 1)\n",
    "    # Assicura C-contiguità per 'b' in sgemm(trans_a=True)\n",
    "    # (anche se F-contiguità sarebbe ideale per 'b' se 'a' non è trasposta,\n",
    "    # BLAS gestisce C-contigue per entrambe con trans_a=True)\n",
    "    dLdZ_col = np.ascontiguousarray(dLdZ_col_transposed_view.reshape(dLdZ_col_shape), dtype=np.float32)\n",
    "\n",
    "    # Calcola X_col_for_dW.T @ dLdZ_col\n",
    "    gradient_wrt_kernels_flat = sgemm(alpha=1.0, a=X_col_for_dW, b=dLdZ_col, trans_a=True)\n",
    "\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(\n",
    "        input_channels_orig, kernel_height, kernel_width, kernels_number\n",
    "    ).transpose(3, 0, 1, 2).astype(np.float32, copy=False)\n",
    "\n",
    "    # --- Calcolo di gradient_wrt_input (dL/dX) ---\n",
    "    dilated_grad_pre_activation = dilate(gradient_pre_activation.astype(np.float32, copy=False), stride)\n",
    "\n",
    "    # Ottimizzazione per rotazione kernel: W_rot180 = W[:, :, ::-1, ::-1] (flip per KH, KW)\n",
    "    # Poi transpose per scambiare canali input/output per la convoluzione trasposta: (IC, OC, KH, KW)\n",
    "    flipped_kernels_for_dX = kernels.astype(np.float32, copy=False)[:,:,::-1,::-1].transpose(1,0,2,3)\n",
    "    # Assicura che i kernel flippati siano C-contigui per la chiamata ricorsiva a im2col_optimized_v3\n",
    "    flipped_kernels_for_dX = np.ascontiguousarray(flipped_kernels_for_dX)\n",
    "\n",
    "\n",
    "    padding_for_dX_conv = kernel_height - 1 - padding\n",
    "\n",
    "    gradient_wrt_input_raw, _ = im2col_optimized_sgemm(\n",
    "        dilated_grad_pre_activation,\n",
    "        flipped_kernels_for_dX,\n",
    "        biases=None,\n",
    "        padding=padding_for_dX_conv,\n",
    "        stride=1,\n",
    "        applyReLU=False\n",
    "    )\n",
    "\n",
    "    # Crop per ottenere le dimensioni corrette\n",
    "    gradient_wrt_input = gradient_wrt_input_raw[:, :, :input_height, :input_width].astype(np.float32, copy=False)\n",
    "\n",
    "    return gradient_wrt_input, gradient_wrt_kernels, gradient_wrt_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a64bb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from im2col_cython import im2col_optimized_cython, im2col_gradient_optimized_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249b2ef",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3f21f",
   "metadata": {},
   "source": [
    "`ReLU_SoftMax_FullyConnected` executes the forward pass of a two-layer Multi-Layer Perceptron (one hidden layer, one output layer), typically used for classification after feature extraction by convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68206d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array, w1, b1, w2, b2):\n",
    "    first_layer_output = np.matmul(input_array, w1) + b1\n",
    "    first_layer_activation = np.maximum(0, first_layer_output)\n",
    "    second_layer_output = np.matmul(first_layer_activation, w2) + b2\n",
    "    second_layer_activation = softmax(second_layer_output)\n",
    "    \n",
    "    return first_layer_output, first_layer_activation, second_layer_output, second_layer_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabba506",
   "metadata": {},
   "source": [
    "#### Comparing optimized versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c80b2f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TRAINING TIME SUMMARY ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time (Im2Col Optimized): 2.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time (Im2Col Optimized SGEMM): 3.78s\n",
      "\n",
      "========== INFERENCE COMPARISON ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time/sample (Im2Col Optimized): 0.000899s (Accuracy: 14.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time/sample (Im2Col Optimized SGEMM): 0.000992s (Accuracy: 14.00%)\n",
      "\n",
      "Descriptive Statistics of Inference Times per sample (seconds):\n",
      "       Im2Col Optimized  Im2Col Optimized SGEMM\n",
      "count        100.000000              100.000000\n",
      "mean           0.000899                0.000992\n",
      "std            0.001842                0.002114\n",
      "min            0.000000                0.000000\n",
      "25%            0.000000                0.000000\n",
      "50%            0.000999                0.000998\n",
      "75%            0.001000                0.001001\n",
      "max            0.015807                0.017949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "import os\n",
    "import pandas as pd # Per una migliore visualizzazione delle statistiche\n",
    "\n",
    "# --- Funzioni di caricamento MNIST e SimpleCNN (dalle tue celle) ---\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "class SimpleCNN(nn.Module): # Necessaria per ottenere le forme dei pesi\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# --- Funzioni MLP e Loss (dalle tue celle) ---\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array, w1, b1, w2, b2):\n",
    "    first_layer_output = np.matmul(input_array, w1) + b1\n",
    "    first_layer_activation = np.maximum(0, first_layer_output)\n",
    "    second_layer_output = np.matmul(first_layer_activation, w2) + b2\n",
    "    second_layer_activation = softmax(second_layer_output)\n",
    "    return first_layer_output, first_layer_activation, second_layer_output, second_layer_activation\n",
    "\n",
    "def ReLU_SoftMax_FC_Backward(batch_size, predictions, labels, w1, w2, output_after_activation, layer_output, input_to_mlp):\n",
    "    dL_dz2 = predictions - labels # Assumiamo che labels sia già della dimensione corretta per il batch\n",
    "    dL_dw2 = np.matmul(output_after_activation.T, dL_dz2)\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_doutput_after_activation = np.matmul(dL_dz2, w2.T)\n",
    "    dReLU = (layer_output > 0).astype(float)\n",
    "    dL_dlayer_output = dL_doutput_after_activation * dReLU\n",
    "    dL_dw1 = np.matmul(input_to_mlp.reshape(batch_size, -1).T, dL_dlayer_output)\n",
    "    dL_db1 = np.sum(dL_dlayer_output, axis=0)\n",
    "    dL_input_to_mlp = np.matmul(dL_dlayer_output, w1.T)\n",
    "    return dL_input_to_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2\n",
    "\n",
    "def crossEntropy(predictions,true_labels):\n",
    "    predictions = predictions + 1e-7 # Aggiunto epsilon più piccolo per stabilità\n",
    "    return -np.sum(true_labels * np.log(predictions)) # Somma su tutte le classi\n",
    "\n",
    "# --- Funzione di inizializzazione pesi NumPy ---\n",
    "def initialize_numpy_weights(shapes_source_dict):\n",
    "    np_weights = {}\n",
    "    np_weights['k1'] = np.random.randn(*shapes_source_dict['k1'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b_conv1'] = np.zeros(shapes_source_dict['b_conv1'].shape, dtype=np.float32)\n",
    "    np_weights['k2'] = np.random.randn(*shapes_source_dict['k2'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b_conv2'] = np.zeros(shapes_source_dict['b_conv2'].shape, dtype=np.float32)\n",
    "    np_weights['k3'] = np.random.randn(*shapes_source_dict['k3'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b_conv3'] = np.zeros(shapes_source_dict['b_conv3'].shape, dtype=np.float32)\n",
    "    np_weights['w1'] = np.random.randn(*shapes_source_dict['w1'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b1'] = np.zeros(shapes_source_dict['b1'].shape, dtype=np.float32)\n",
    "    np_weights['w2'] = np.random.randn(*shapes_source_dict['w2'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b2'] = np.zeros(shapes_source_dict['b2'].shape, dtype=np.float32)\n",
    "    return np_weights\n",
    "\n",
    "# --- Funzione di Training Generica per NumPy CNN ---\n",
    "def train_numpy_cnn_generic(train_images_np, train_labels_np,\n",
    "                            conv_forward_fn, conv_backward_fn,\n",
    "                            model_name, initial_np_weights,\n",
    "                            num_epochs=3, batch_size=32, learning_rate=0.001):\n",
    "    # print(f\"\\n--- Training NumPy CNN: {model_name} ---\")\n",
    "\n",
    "    k1, b_conv1 = np.copy(initial_np_weights['k1']), np.copy(initial_np_weights['b_conv1'])\n",
    "    k2, b_conv2 = np.copy(initial_np_weights['k2']), np.copy(initial_np_weights['b_conv2'])\n",
    "    k3, b_conv3 = np.copy(initial_np_weights['k3']), np.copy(initial_np_weights['b_conv3'])\n",
    "    w1_fc, b1_fc = np.copy(initial_np_weights['w1']), np.copy(initial_np_weights['b1'])\n",
    "    w2_fc, b2_fc = np.copy(initial_np_weights['w2']), np.copy(initial_np_weights['b2'])\n",
    "\n",
    "    num_samples = train_images_np.shape[0]\n",
    "    epoch_losses = []\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_loss = 0.0\n",
    "        \n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        shuffled_images = train_images_np[permutation] / 255.0\n",
    "        shuffled_labels = train_labels_np[permutation]\n",
    "\n",
    "        progress_bar = tqdm(range(0, num_samples, batch_size), desc=f\"Epoch {epoch+1}/{num_epochs} ({model_name})\", leave=False)\n",
    "        \n",
    "        num_batches_processed = 0\n",
    "        for i in progress_bar:\n",
    "            num_batches_processed += 1\n",
    "            batch_images = shuffled_images[i:i+batch_size]\n",
    "            batch_labels = shuffled_labels[i:i+batch_size]\n",
    "            \n",
    "            c0 = batch_images.reshape(-1, 1, 28, 28).astype(np.float32)\n",
    "            current_bs = c0.shape[0]\n",
    "\n",
    "            # --- Forward Pass ---\n",
    "            c1, mask1 = conv_forward_fn(c0, k1, b_conv1, padding=0, stride=2, applyReLU=True)\n",
    "            c2, mask2 = conv_forward_fn(c1, k2, b_conv2, padding=1, stride=2, applyReLU=True)\n",
    "            c3, mask3 = conv_forward_fn(c2, k3, b_conv3, padding=0, stride=2, applyReLU=True)\n",
    "            \n",
    "            input_to_mlp = c3.reshape(current_bs, -1)\n",
    "            fl, fa, sl, sa = ReLU_SoftMax_FullyConnected(input_to_mlp, w1_fc, b1_fc, w2_fc, b2_fc)\n",
    "            \n",
    "            batch_loss_val = crossEntropy(sa, batch_labels) / current_bs # Average loss for the batch\n",
    "            current_epoch_loss += batch_loss_val\n",
    "\n",
    "            # --- Backward Pass ---\n",
    "            dL_i_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2 = \\\n",
    "                ReLU_SoftMax_FC_Backward(current_bs, sa, batch_labels, w1_fc, w2_fc, fa, fl, input_to_mlp)\n",
    "            dL_dA3 = dL_i_mlp.reshape(c3.shape) \n",
    "            \n",
    "            gi3, gk3, gb3 = conv_backward_fn(c2, dL_dA3, k3, mask3, padding=0, stride=2)\n",
    "            gi2, gk2, gb2 = conv_backward_fn(c1, gi3, k2, mask2, padding=1, stride=2)\n",
    "            gi1, gk1, gb1 = conv_backward_fn(c0, gi2, k1, mask1, padding=0, stride=2)\n",
    "            \n",
    "            # Normalize gradients by batch size\n",
    "            dL_dw1 /= current_bs; dL_db1 /= current_bs\n",
    "            dL_dw2 /= current_bs; dL_db2 /= current_bs\n",
    "            gk1 /= current_bs; gb1 /= current_bs\n",
    "            gk2 /= current_bs; gb2 /= current_bs\n",
    "            gk3 /= current_bs; gb3 /= current_bs\n",
    "            \n",
    "            # --- Weight Update ---\n",
    "            w1_fc  -= learning_rate * dL_dw1\n",
    "            b1_fc  -= learning_rate * dL_db1.reshape(b1_fc.shape)\n",
    "            w2_fc  -= learning_rate * dL_dw2\n",
    "            b2_fc  -= learning_rate * dL_db2.reshape(b2_fc.shape)\n",
    "            k3  -= learning_rate * gk3\n",
    "            b_conv3 -= learning_rate * gb3.reshape(b_conv3.shape)\n",
    "            k2  -= learning_rate * gk2\n",
    "            b_conv2 -= learning_rate * gb2.reshape(b_conv2.shape)\n",
    "            k1  -= learning_rate * gk1\n",
    "            b_conv1 -= learning_rate * gb1.reshape(b_conv1.shape)\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{batch_loss_val:.4f}\")\n",
    "\n",
    "        avg_epoch_loss = current_epoch_loss / num_batches_processed if num_batches_processed > 0 else 0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs} ({model_name}) - Time: {epoch_time:.2f}s - Avg Training Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "    # print(f\"{model_name} NumPy Training Complete. Total time: {total_training_time:.2f}s\")\n",
    "\n",
    "    # Grafico della loss\n",
    "    # plt.figure(figsize=(6, 4))\n",
    "    # plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "    # plt.title(f'{model_name} NumPy Training Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    trained_weights = {'k1': k1, 'b_conv1': b_conv1, 'k2': k2, 'b_conv2': b_conv2, 'k3': k3, 'b_conv3': b_conv3,\n",
    "                       'w1': w1_fc, 'b1': b1_fc, 'w2': w2_fc, 'b2': b2_fc}\n",
    "\n",
    "    return trained_weights, total_training_time\n",
    "\n",
    "# --- Funzione di Inferenza Generica per NumPy CNN ---\n",
    "def run_numpy_inference_generic(test_images_np, test_labels_np, weights, \n",
    "                                conv_forward_fn, model_name, num_samples_to_test=200):\n",
    "    # print(f\"\\n--- Running Inference for NumPy CNN: {model_name} ---\")\n",
    "    k1, b_conv1 = weights['k1'], weights['b_conv1']\n",
    "    k2, b_conv2 = weights['k2'], weights['b_conv2']\n",
    "    k3, b_conv3 = weights['k3'], weights['b_conv3']\n",
    "    w1_fc, b1_fc = weights['w1'], weights['b1']\n",
    "    w2_fc, b2_fc = weights['w2'], weights['b2']\n",
    "\n",
    "    inference_times = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    # Usa un subset per l'inferenza per velocizzare\n",
    "    test_images_subset = test_images_np[:num_samples_to_test] / 255.0\n",
    "    test_labels_subset = test_labels_np[:num_samples_to_test]\n",
    "\n",
    "    progress_bar = tqdm(range(test_images_subset.shape[0]), desc=f\"Inferring ({model_name})\", leave=False)\n",
    "\n",
    "    for i in progress_bar:\n",
    "        c0 = test_images_subset[i].reshape(1, 1, 28, 28).astype(np.float32)\n",
    "        true_label_idx = np.argmax(test_labels_subset[i])\n",
    "\n",
    "        start_time = time.time()\n",
    "        c1, _ = conv_forward_fn(c0, k1, b_conv1, padding=0, stride=2, applyReLU=True)\n",
    "        c2, _ = conv_forward_fn(c1, k2, b_conv2, padding=1, stride=2, applyReLU=True)\n",
    "        c3, _ = conv_forward_fn(c2, k3, b_conv3, padding=0, stride=2, applyReLU=True)\n",
    "        input_to_mlp = c3.reshape(1, -1)\n",
    "        _, _, _, sa = ReLU_SoftMax_FullyConnected(input_to_mlp, w1_fc, b1_fc, w2_fc, b2_fc)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "        predicted_idx = np.argmax(sa)\n",
    "        if predicted_idx == true_label_idx:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    accuracy = (correct_predictions / test_images_subset.shape[0]) * 100\n",
    "    # print(f\"{model_name} - Avg Inference Time: {avg_inference_time:.6f}s - Accuracy: {accuracy:.2f}% on {test_images_subset.shape[0]} samples\")\n",
    "    return avg_inference_time, inference_times, accuracy\n",
    "\n",
    "# ============================ Main Execution ============================\n",
    "# # --- Caricamento e Preprocessing Dati MNIST ---\n",
    "try:\n",
    "    raw_train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "    raw_train_labels_int = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "    raw_test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "    raw_test_labels_int = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "except FileNotFoundError:\n",
    "    print(\"MNIST data files not found. Please download them and place in 'MNIST/' directory.\")\n",
    "    exit()\n",
    "\n",
    "train_labels_one_hot = np.zeros((raw_train_labels_int.shape[0], 10), dtype=np.float32)\n",
    "for i in range(len(raw_train_labels_int)):\n",
    "    train_labels_one_hot[i][raw_train_labels_int[i]] = 1\n",
    "test_labels_one_hot = np.zeros((raw_test_labels_int.shape[0], 10), dtype=np.float32)\n",
    "for i in range(len(raw_test_labels_int)):\n",
    "    test_labels_one_hot[i][raw_test_labels_int[i]] = 1\n",
    "\n",
    "# --- Subset per test rapido ---\n",
    "NUM_TRAIN_SAMPLES = len(raw_train_images) // 100\n",
    "NUM_TEST_SAMPLES_INFERENCE = len(raw_test_images) // 100\n",
    "NUM_EPOCHS_NP = 3\n",
    "BATCH_SIZE_NP = 64\n",
    "LEARNING_RATE_NP = 0.001\n",
    "\n",
    "train_images_subset = raw_train_images[:NUM_TRAIN_SAMPLES]\n",
    "train_labels_subset = train_labels_one_hot[:NUM_TRAIN_SAMPLES]\n",
    "test_images_subset_inf = raw_test_images[:NUM_TEST_SAMPLES_INFERENCE]\n",
    "test_labels_subset_inf = test_labels_one_hot[:NUM_TEST_SAMPLES_INFERENCE]\n",
    "\n",
    "# --- Ottieni forme dei pesi (da un modello PyTorch temporaneo) ---\n",
    "shapes_source = {}\n",
    "temp_model = SimpleCNN(num_classes=10)\n",
    "shapes_source['k1'] = temp_model.conv1.weight.data.numpy()\n",
    "shapes_source['b_conv1'] = temp_model.conv1.bias.data.numpy()\n",
    "shapes_source['k2'] = temp_model.conv2.weight.data.numpy()\n",
    "shapes_source['b_conv2'] = temp_model.conv2.bias.data.numpy()\n",
    "shapes_source['k3'] = temp_model.conv3.weight.data.numpy()\n",
    "shapes_source['b_conv3'] = temp_model.conv3.bias.data.numpy()\n",
    "shapes_source['w1'] = temp_model.fc1.weight.data.numpy().T \n",
    "shapes_source['b1'] = temp_model.fc1.bias.data.numpy().reshape(1, -1)\n",
    "shapes_source['w2'] = temp_model.fc2.weight.data.numpy().T\n",
    "shapes_source['b2'] = temp_model.fc2.bias.data.numpy().reshape(1, -1)\n",
    "\n",
    "same_weights = initialize_numpy_weights(shapes_source)\n",
    "\n",
    "models = {\n",
    "    \"Im2Col Optimized\": {\n",
    "        \"conv_forward_fn\": im2col_optimized,\n",
    "        \"conv_backward_fn\": im2col_gradient_optimized,\n",
    "        \"weights\": same_weights\n",
    "    },\n",
    "    \"Im2Col Optimized SGEMM\": {\n",
    "        \"conv_forward_fn\": im2col_optimized_sgemm,\n",
    "        \"conv_backward_fn\": im2col_gradient_optimized_sgemm,\n",
    "        \"weights\": same_weights\n",
    "    },\n",
    "    # \"Im2Col Optimized Clean\": {\n",
    "    #     \"conv_forward_fn\": im2col_optimized_clean,\n",
    "    #     \"conv_backward_fn\": im2col_gradient_optimized_clean,\n",
    "    #     \"weights\": same_weights\n",
    "    # },\n",
    "    # \"Im2Col Cython\": {\n",
    "    #     \"conv_forward_fn\": im2col_optimized_cython,\n",
    "    #     \"conv_backward_fn\": im2col_gradient_optimized_cython,\n",
    "    # }\n",
    "}\n",
    "\n",
    "print(\"\\n========== TRAINING TIME SUMMARY ==========\")\n",
    "\n",
    "for model_name, model_fns in models.items():\n",
    "    if model_name == \"Im2Col Optimized SGEMM\":\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"14\"\n",
    "\n",
    "    trained_weights, train_time = train_numpy_cnn_generic(\n",
    "        train_images_subset, train_labels_subset,\n",
    "        conv_forward_fn=model_fns[\"conv_forward_fn\"],\n",
    "        conv_backward_fn=model_fns[\"conv_backward_fn\"],\n",
    "        model_name=model_name,\n",
    "        initial_np_weights=model_fns[\"weights\"],\n",
    "        num_epochs=NUM_EPOCHS_NP, batch_size=BATCH_SIZE_NP, learning_rate=LEARNING_RATE_NP\n",
    "    )\n",
    "    print(f\"Total Training Time ({model_name}): {train_time:.2f}s\")\n",
    "\n",
    "    model_fns[\"weights\"] = trained_weights\n",
    "\n",
    "    if model_name == \"Im2Col Optimized SGEMM\":\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "    trained_weights = None\n",
    "    train_time = None\n",
    "\n",
    "# --- Inferenza ---\n",
    "print(\"\\n========== INFERENCE COMPARISON ==========\")\n",
    "\n",
    "df_inference_times = pd.DataFrame()\n",
    "\n",
    "for model_name, model_fns in models.items():\n",
    "    if model_name == \"Im2Col Optimized SGEMM\":\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"14\"\n",
    "\n",
    "    avg_inf_time, all_inf_times, acc = run_numpy_inference_generic(\n",
    "        test_images_subset_inf, test_labels_subset_inf,\n",
    "        weights=model_fns[\"weights\"],\n",
    "        conv_forward_fn=model_fns[\"conv_forward_fn\"],\n",
    "        model_name=model_name,\n",
    "        num_samples_to_test=NUM_TEST_SAMPLES_INFERENCE\n",
    "    )\n",
    "    print(f\"Avg Inference Time/sample ({model_name}): {avg_inf_time:.6f}s (Accuracy: {acc:.2f}%)\")\n",
    "    df_inference_times[model_name] = all_inf_times\n",
    "\n",
    "    if model_name == \"Im2Col Optimized SGEMM\":\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "    avg_inf_time = None\n",
    "    all_inf_times = None\n",
    "    acc = None\n",
    "\n",
    "print(\"\\nDescriptive Statistics of Inference Times per sample (seconds):\")\n",
    "print(df_inference_times.describe())\n",
    "\n",
    "# Boxplot dei tempi di inferenza\n",
    "# plt.figure(figsize=(7, 8))\n",
    "# df_inference_times.boxplot(showfliers=False)\n",
    "# plt.title('Boxplot of Inference Times per Sample')\n",
    "# plt.ylabel('Time (s)')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# Boxplot only for Im2Col Optimized and Im2Col Optimized clean\n",
    "# plt.figure(figsize=(7, 8))\n",
    "# df_inference_times[['Im2Col Optimized', 'Im2Col Optimized Clean']].boxplot(showfliers=False)\n",
    "# plt.title('Boxplot of Inference Times for Selected Models')\n",
    "# plt.ylabel('Time (s)')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# batch_size = \n",
    "# ========== TRAINING TIME SUMMARY ==========\n",
    "                                                                                          \n",
    "# Total Training Time (Im2Col Optimized): 5.58s\n",
    "# Total Training Time (Im2Col Optimized SGEMM): 8.90s\n",
    "# Total Training Time (Im2Col Optimized Clean): 5.53s\n",
    "\n",
    "# ========== INFERENCE COMPARISON ==========\n",
    "                                                                                 \n",
    "# Avg Inference Time/sample (Im2Col Optimized): 0.000615s (Accuracy: 11.00%)\n",
    "# Avg Inference Time/sample (Im2Col Optimized SGEMM): 0.000923s (Accuracy: 17.80%)\n",
    "# Avg Inference Time/sample (Im2Col Optimized Clean): 0.000548s (Accuracy: 4.40%)\n",
    "\n",
    "# Descriptive Statistics of Inference Times per sample (seconds):\n",
    "#        Im2Col Optimized  Im2Col Optimized SGEMM  Im2Col Optimized Clean\n",
    "# count        500.000000              500.000000              500.000000\n",
    "# mean           0.000615                0.000923                0.000548\n",
    "# std            0.002119                0.002832                0.001908\n",
    "# min            0.000000                0.000000                0.000000\n",
    "# 25%            0.000000                0.000000                0.000000\n",
    "# 50%            0.000000                0.000000                0.000000\n",
    "# 75%            0.000988                0.000981                0.000948\n",
    "# max            0.016086                0.021756                0.016135\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddc2e4",
   "metadata": {},
   "source": [
    "### JAX implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1b0bac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TRAINING TIME SUMMARY ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time (Im2Col Optimized): 2.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time (Im2Col Optimized SGEMM): 3.94s\n",
      "\n",
      "--- Training JAX CNN: JAX CNN ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time (JAX CNN): 1.16s\n",
      "\n",
      "--- Training PyTorch CNN: PyTorch CNN ---\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time (PyTorch CNN): 0.26s\n",
      "\n",
      "========== INFERENCE COMPARISON ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time/sample (Im2Col Optimized): 0.000851s (Accuracy: 14.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time/sample (Im2Col Optimized SGEMM): 0.001331s (Accuracy: 14.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time/sample (JAX CNN): 0.001266s (Accuracy: 14.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time/sample (PyTorch CNN): 0.000518s (Accuracy: 60.00%)\n",
      "\n",
      "Descriptive Statistics of Inference Times per sample (seconds):\n",
      "       Im2Col Optimized  Im2Col Optimized SGEMM     JAX CNN  PyTorch CNN\n",
      "count        100.000000              100.000000  100.000000   100.000000\n",
      "mean           0.000851                0.001331    0.001266     0.000518\n",
      "std            0.001745                0.002815    0.008886     0.001510\n",
      "min            0.000000                0.000000    0.000000     0.000000\n",
      "25%            0.000000                0.000000    0.000000     0.000000\n",
      "50%            0.000000                0.000000    0.000000     0.000000\n",
      "75%            0.001001                0.001054    0.000000     0.000999\n",
      "max            0.010705                0.016887    0.087980     0.010412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import copy\n",
    "import struct\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Queste sono necessarie per il training PyTorch e per CNNDataset\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import torch.optim as optim\n",
    "\n",
    "# --- JAX CNN Definition ---\n",
    "def init_jax_weights(key, shapes_source_dict):\n",
    "    keys = jax.random.split(key, 10)\n",
    "    params = {\n",
    "        'conv1': {'w': jax.random.normal(keys[0], shapes_source_dict['k1'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b_conv1'].shape, dtype=jnp.float32)},\n",
    "        'conv2': {'w': jax.random.normal(keys[1], shapes_source_dict['k2'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b_conv2'].shape, dtype=jnp.float32)},\n",
    "        'conv3': {'w': jax.random.normal(keys[2], shapes_source_dict['k3'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b_conv3'].shape, dtype=jnp.float32)},\n",
    "        'fc1':   {'w': jax.random.normal(keys[3], shapes_source_dict['w1'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b1'].shape[1], dtype=jnp.float32)},\n",
    "        'fc2':   {'w': jax.random.normal(keys[4], shapes_source_dict['w2'].shape, dtype=jnp.float32) * 0.01,\n",
    "                  'b': jnp.zeros(shapes_source_dict['b2'].shape[1], dtype=jnp.float32)}\n",
    "    }\n",
    "    return params\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3, 4, 5, 6)) # stride, applyReLU, dimension_numbers sono statici\n",
    "def jax_conv_layer_core(x, W, b, stride, padding_config, applyReLU, dimension_numbers=('NCHW', 'OIHW', 'NCHW')):\n",
    "    out = jax.lax.conv_general_dilated(\n",
    "        x, W, window_strides=(stride, stride), padding=padding_config,\n",
    "        dimension_numbers=dimension_numbers\n",
    "    )\n",
    "    out = out + jnp.reshape(b, (1, -1, 1, 1))\n",
    "    if applyReLU:\n",
    "        return jax.nn.relu(out)\n",
    "    return out\n",
    "\n",
    "def jax_conv_layer(x, W, b, stride, padding_input, applyReLU=True, dimension_numbers=('NCHW', 'OIHW', 'NCHW')):\n",
    "    padding_config = None\n",
    "    if isinstance(padding_input, str):\n",
    "        padding_input_upper = padding_input.upper()\n",
    "        if padding_input_upper == 'VALID' or padding_input_upper == 'SAME':\n",
    "            padding_config = padding_input_upper\n",
    "        else:\n",
    "            raise ValueError(f\"Stringa padding_input deve essere 'VALID' o 'SAME'. Ricevuto: {padding_input}\")\n",
    "    elif isinstance(padding_input, int):\n",
    "        padding_config = tuple([(padding_input, padding_input), (padding_input, padding_input)]) # Restituisce una tupla\n",
    "    elif isinstance(padding_input, tuple): # Ora ci aspettiamo direttamente una tupla\n",
    "        # Potresti aggiungere controlli sulla struttura della tupla se necessario\n",
    "        padding_config = padding_input\n",
    "    else:\n",
    "        raise ValueError(f\"padding_input deve essere 'VALID', 'SAME', un intero, o una tupla di coppie. Ricevuto: {padding_input}, tipo: {type(padding_input)}\")\n",
    "    \n",
    "    out_conv = jax_conv_layer_core(x, W, b, stride, padding_config, applyReLU, dimension_numbers)\n",
    "    \n",
    "    mask = (out_conv > 0).astype(out_conv.dtype) if applyReLU else jnp.ones_like(out_conv, dtype=out_conv.dtype)\n",
    "    return out_conv, mask\n",
    "\n",
    "def jax_dense_layer(x, W, b):\n",
    "    return jnp.dot(x, W) + b\n",
    "\n",
    "def jax_cnn_forward(params, x_batch):\n",
    "    # Conv1: k=2, s=2, p=0.\n",
    "    conv1_out, _ = jax_conv_layer(x_batch, params['conv1']['w'], params['conv1']['b'], \n",
    "                                  stride=2, padding_input='VALID', applyReLU=True) # 'VALID' è una stringa, hashable\n",
    "    \n",
    "    # Conv2: k=2, s=2, p=1. Passa una TUPLA di TUPLE\n",
    "    conv2_out, _ = jax_conv_layer(conv1_out, params['conv2']['w'], params['conv2']['b'], \n",
    "                                  stride=2, padding_input=((1,1),(1,1)), applyReLU=True) # ((1,1),(1,1)) è hashable\n",
    "    \n",
    "    # Conv3: k=2, s=2, p=0.\n",
    "    conv3_out, _ = jax_conv_layer(conv2_out, params['conv3']['w'], params['conv3']['b'], \n",
    "                                  stride=2, padding_input='VALID', applyReLU=True) # 'VALID' è hashable\n",
    "    \n",
    "    flattened = conv3_out.reshape((conv3_out.shape[0], -1))\n",
    "    fc1_out = jax_dense_layer(flattened, params['fc1']['w'], params['fc1']['b'])\n",
    "    fc1_activated = jax.nn.relu(fc1_out)\n",
    "    logits = jax_dense_layer(fc1_activated, params['fc2']['w'], params['fc2']['b'])\n",
    "    return logits\n",
    "\n",
    "def jax_loss_fn(params_model, x_batch, y_batch_one_hot):\n",
    "    logits = jax_cnn_forward(params_model, x_batch)\n",
    "    loss = optax.softmax_cross_entropy(logits=logits, labels=y_batch_one_hot).mean()\n",
    "    return loss\n",
    "\n",
    "# Funzione di step di training JAX (sarà jittata)\n",
    "def jax_train_step_core(params, x_batch, y_batch_one_hot, opt_state, optimizer):\n",
    "    loss_value, grads = jax.value_and_grad(jax_loss_fn)(params, x_batch, y_batch_one_hot)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    updated_params = optax.apply_updates(params, updates)\n",
    "    return updated_params, opt_state, loss_value\n",
    "\n",
    "# Jitta la funzione di step di training\n",
    "jax_train_step_jitted = jax.jit(jax_train_step_core, static_argnames=['optimizer'])\n",
    "\n",
    "@jax.jit\n",
    "def jax_predict(params, x_batch):\n",
    "    logits = jax_cnn_forward(params, x_batch)\n",
    "    probabilities = jax.nn.softmax(logits, axis=-1)\n",
    "    return probabilities\n",
    "\n",
    "def train_jax_cnn(train_images_np, train_labels_np,\n",
    "                  model_name, initial_jax_params,\n",
    "                  num_epochs=3, batch_size=32, learning_rate=0.001):\n",
    "    print(f\"\\n--- Training JAX CNN: {model_name} ---\")\n",
    "    params = initial_jax_params\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "    num_samples = train_images_np.shape[0]\n",
    "    epoch_losses = []\n",
    "    total_training_time = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_loss = 0.0\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        shuffled_images_np = train_images_np[permutation] / 255.0\n",
    "        shuffled_labels_np = train_labels_np[permutation]\n",
    "        progress_bar = tqdm(range(0, num_samples, batch_size), desc=f\"Epoch {epoch+1}/{num_epochs} (JAX)\", leave=False)\n",
    "        num_batches_processed = 0\n",
    "        for i in progress_bar:\n",
    "            num_batches_processed += 1\n",
    "            batch_images_np = shuffled_images_np[i:i+batch_size]\n",
    "            batch_labels_one_hot_np = shuffled_labels_np[i:i+batch_size]\n",
    "            batch_images_jax = jnp.asarray(batch_images_np.reshape(-1, 1, 28, 28))\n",
    "            batch_labels_jax = jnp.asarray(batch_labels_one_hot_np)\n",
    "            params, opt_state, loss_val = jax_train_step_jitted(\n",
    "                params, batch_images_jax, batch_labels_jax, opt_state, optimizer\n",
    "            )\n",
    "            current_epoch_loss += loss_val.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss_val.item():.4f}\")\n",
    "        avg_epoch_loss = current_epoch_loss / num_batches_processed if num_batches_processed > 0 else 0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "    return params, total_training_time\n",
    "\n",
    "def run_jax_inference(test_images_np, test_labels_np, jax_weights,\n",
    "                      model_name, num_samples_to_test=200):\n",
    "    inference_times = []\n",
    "    correct_predictions = 0\n",
    "    test_images_subset_np = test_images_np[:num_samples_to_test] / 255.0\n",
    "    test_labels_subset_np = test_labels_np[:num_samples_to_test]\n",
    "    progress_bar = tqdm(range(test_images_subset_np.shape[0]), desc=f\"Inferring (JAX {model_name})\", leave=False)\n",
    "    for i in progress_bar:\n",
    "        img_np = test_images_subset_np[i].reshape(1, 1, 28, 28)\n",
    "        true_label_idx = np.argmax(test_labels_subset_np[i])\n",
    "        img_jax = jnp.asarray(img_np)\n",
    "        start_time = time.time()\n",
    "        probabilities = jax_predict(jax_weights, img_jax)\n",
    "        predicted_idx_jax = jnp.argmax(probabilities, axis=-1)\n",
    "        predicted_idx = np.array(predicted_idx_jax)[0]\n",
    "        end_time = time.time()\n",
    "        probabilities.block_until_ready()\n",
    "        inference_times.append(end_time - start_time)\n",
    "        if predicted_idx == true_label_idx:\n",
    "            correct_predictions += 1\n",
    "    avg_inference_time = np.mean(inference_times) if inference_times else 0\n",
    "    accuracy = (correct_predictions / test_images_subset_np.shape[0]) * 100 if test_images_subset_np.shape[0] > 0 else 0\n",
    "    return avg_inference_time, inference_times, accuracy\n",
    "\n",
    "# --- PyTorch Dataset e Training/Inference ---\n",
    "class CNNDataset(Dataset): # Dal tuo notebook, per PyTorch\n",
    "    def __init__(self, digits, labels, transform=None):\n",
    "        assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "        self.digits = digits\n",
    "        self.labels = labels # Per PyTorch CrossEntropyLoss, dovrebbero essere indici di classe, non one-hot\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.digits)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digits[idx]\n",
    "        label = self.labels[idx] # Restituisce l'indice di classe intero\n",
    "        digit = digit.unsqueeze(0) # (H, W) -> (1, H, W)\n",
    "        return digit, label\n",
    "\n",
    "def train_pytorch_cnn(train_images_np, train_labels_int_np, # Ora riceve etichette intere\n",
    "                      model_name,\n",
    "                      num_epochs=3, batch_size=128, learning_rate=0.001):\n",
    "    print(f\"\\n--- Training PyTorch CNN: {model_name} ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tri_tensor = torch.from_numpy(train_images_np).float() / 255.0\n",
    "    trl_tensor = torch.from_numpy(train_labels_int_np).long() # Etichette intere come LongTensor\n",
    "\n",
    "    train_dataset = CNNDataset(tri_tensor, trl_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    pytorch_model = SimpleCNN(num_classes=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_losses = []\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        pytorch_model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (PyTorch)\", leave=False)\n",
    "\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = pytorch_model(inputs)\n",
    "            loss = criterion(outputs, labels) # labels qui sono indici di classe\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "    \n",
    "    return pytorch_model, total_training_time\n",
    "\n",
    "def run_pytorch_inference(pytorch_model, test_images_np, test_labels_one_hot_np, # Riceve one-hot per confronto\n",
    "                          model_name, num_samples_to_test=200):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pytorch_model.to(device)\n",
    "    pytorch_model.eval()\n",
    "\n",
    "    inference_times = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    test_images_subset_np = test_images_np[:num_samples_to_test] / 255.0\n",
    "    test_labels_subset_np = test_labels_one_hot_np[:num_samples_to_test]\n",
    "\n",
    "    progress_bar = tqdm(range(test_images_subset_np.shape[0]), desc=f\"Inferring (PyTorch {model_name})\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in progress_bar:\n",
    "            img_np = test_images_subset_np[i].reshape(1, 1, 28, 28) # NCHW\n",
    "            true_label_idx = np.argmax(test_labels_subset_np[i])\n",
    "            \n",
    "            img_tensor = torch.from_numpy(img_np).float().to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = pytorch_model(img_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=-1)\n",
    "            predicted_idx = torch.argmax(probabilities).item()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            inference_times.append(end_time - start_time)\n",
    "            if predicted_idx == true_label_idx:\n",
    "                correct_predictions += 1\n",
    "                \n",
    "    avg_inference_time = np.mean(inference_times) if inference_times else 0\n",
    "    accuracy = (correct_predictions / test_images_subset_np.shape[0]) * 100 if test_images_subset_np.shape[0] > 0 else 0\n",
    "    return avg_inference_time, inference_times, accuracy\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Caricamento Dati MNIST ---\n",
    "    raw_train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "    raw_train_labels_int = load_mnist_labels('MNIST/train-labels-idx1-ubyte') # Etichette intere\n",
    "    raw_test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "    raw_test_labels_int = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')   # Etichette intere\n",
    "\n",
    "    # One-hot per NumPy e JAX (e per il calcolo dell'accuracy in inferenza PyTorch)\n",
    "    train_labels_one_hot = np.zeros((raw_train_labels_int.shape[0], 10), dtype=np.float32)\n",
    "    for i in range(len(raw_train_labels_int)):\n",
    "        train_labels_one_hot[i][raw_train_labels_int[i]] = 1\n",
    "    test_labels_one_hot = np.zeros((raw_test_labels_int.shape[0], 10), dtype=np.float32)\n",
    "    for i in range(len(raw_test_labels_int)):\n",
    "        test_labels_one_hot[i][raw_test_labels_int[i]] = 1\n",
    "\n",
    "    NUM_TRAIN_SAMPLES = len(raw_train_images) // 100 \n",
    "    NUM_TEST_SAMPLES_INFERENCE = len(raw_test_images) // 100\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE_NP = 32 \n",
    "    BATCH_SIZE_JAX = 128\n",
    "    BATCH_SIZE_PYTORCH = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    train_images_subset = raw_train_images[:NUM_TRAIN_SAMPLES]\n",
    "    train_labels_one_hot_subset = train_labels_one_hot[:NUM_TRAIN_SAMPLES]\n",
    "    train_labels_int_subset = raw_train_labels_int[:NUM_TRAIN_SAMPLES] # Per training PyTorch\n",
    "\n",
    "    test_images_subset_inf = raw_test_images[:NUM_TEST_SAMPLES_INFERENCE]\n",
    "    test_labels_one_hot_subset_inf = test_labels_one_hot[:NUM_TEST_SAMPLES_INFERENCE]\n",
    "\n",
    "    shapes_source = {}\n",
    "    temp_model = SimpleCNN(num_classes=10)\n",
    "    shapes_source['k1'] = temp_model.conv1.weight.data.numpy()\n",
    "    shapes_source['b_conv1'] = temp_model.conv1.bias.data.numpy()\n",
    "    shapes_source['k2'] = temp_model.conv2.weight.data.numpy()\n",
    "    shapes_source['b_conv2'] = temp_model.conv2.bias.data.numpy()\n",
    "    shapes_source['k3'] = temp_model.conv3.weight.data.numpy()\n",
    "    shapes_source['b_conv3'] = temp_model.conv3.bias.data.numpy()\n",
    "    shapes_source['w1'] = temp_model.fc1.weight.data.numpy().T\n",
    "    shapes_source['b1'] = temp_model.fc1.bias.data.numpy().reshape(1, -1)\n",
    "    shapes_source['w2'] = temp_model.fc2.weight.data.numpy().T\n",
    "    shapes_source['b2'] = temp_model.fc2.bias.data.numpy().reshape(1, -1)\n",
    "\n",
    "    numpy_initial_weights = initialize_numpy_weights(shapes_source)\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    jax_initial_params = init_jax_weights(key, shapes_source)\n",
    "\n",
    "    models_numpy = {\n",
    "        \"Im2Col Optimized\": {\n",
    "            \"conv_forward_fn\": im2col_optimized,\n",
    "            \"conv_backward_fn\": im2col_gradient_optimized,\n",
    "            \"weights\": copy.deepcopy(numpy_initial_weights)\n",
    "        },\n",
    "        \"Im2Col Optimized SGEMM\": {\n",
    "            \"conv_forward_fn\": im2col_optimized_sgemm,\n",
    "            \"conv_backward_fn\": im2col_gradient_optimized_sgemm,\n",
    "            \"weights\": copy.deepcopy(numpy_initial_weights)\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    print(\"\\n========== TRAINING TIME SUMMARY ==========\")\n",
    "    # NumPy models training\n",
    "    for model_name, model_fns in models_numpy.items():\n",
    "        trained_np_weights, train_time_np = train_numpy_cnn_generic(\n",
    "            train_images_subset, train_labels_one_hot_subset, # NumPy usa one-hot\n",
    "            conv_forward_fn=model_fns[\"conv_forward_fn\"],\n",
    "            conv_backward_fn=model_fns[\"conv_backward_fn\"],\n",
    "            model_name=model_name,\n",
    "            initial_np_weights=model_fns[\"weights\"],\n",
    "            num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE_NP, learning_rate=LEARNING_RATE\n",
    "        )\n",
    "        print(f\"Total Training Time ({model_name}): {train_time_np:.2f}s\")\n",
    "        model_fns[\"weights\"] = trained_np_weights\n",
    "\n",
    "    # JAX model training\n",
    "    trained_jax_params, train_time_jax = train_jax_cnn(\n",
    "        train_images_subset, train_labels_one_hot_subset, # JAX usa one-hot\n",
    "        model_name=\"JAX CNN\",\n",
    "        initial_jax_params=jax_initial_params,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE_JAX,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    print(f\"Total Training Time (JAX CNN): {train_time_jax:.2f}s\")\n",
    "\n",
    "    # PyTorch model training\n",
    "    trained_pytorch_model, train_time_pytorch = train_pytorch_cnn(\n",
    "        train_images_subset, train_labels_int_subset, # PyTorch CrossEntropyLoss usa etichette intere\n",
    "        model_name=\"PyTorch CNN\",\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE_PYTORCH,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    print(f\"Total Training Time (PyTorch CNN): {train_time_pytorch:.2f}s\")\n",
    "\n",
    "\n",
    "    print(\"\\n========== INFERENCE COMPARISON ==========\")\n",
    "    df_inference_times = pd.DataFrame()\n",
    "    # NumPy models inference\n",
    "    for model_name, model_fns in models_numpy.items():\n",
    "        avg_inf_time_np, all_inf_times_np, acc_np = run_numpy_inference_generic(\n",
    "            test_images_subset_inf, test_labels_one_hot_subset_inf, # Per accuracy usa one-hot\n",
    "            weights=model_fns[\"weights\"],\n",
    "            conv_forward_fn=model_fns[\"conv_forward_fn\"],\n",
    "            model_name=model_name,\n",
    "            num_samples_to_test=NUM_TEST_SAMPLES_INFERENCE\n",
    "        )\n",
    "        print(f\"Avg Inference Time/sample ({model_name}): {avg_inf_time_np:.6f}s (Accuracy: {acc_np:.2f}%)\")\n",
    "        df_inference_times[model_name] = all_inf_times_np\n",
    "    \n",
    "    # JAX model inference\n",
    "    avg_inf_time_jax, all_inf_times_jax, acc_jax = run_jax_inference(\n",
    "        test_images_subset_inf, test_labels_one_hot_subset_inf, # Per accuracy usa one-hot\n",
    "        jax_weights=trained_jax_params,\n",
    "        model_name=\"JAX CNN\",\n",
    "        num_samples_to_test=NUM_TEST_SAMPLES_INFERENCE\n",
    "    )\n",
    "    print(f\"Avg Inference Time/sample (JAX CNN): {avg_inf_time_jax:.6f}s (Accuracy: {acc_jax:.2f}%)\")\n",
    "    df_inference_times[\"JAX CNN\"] = all_inf_times_jax\n",
    "\n",
    "    # PyTorch model inference\n",
    "    avg_inf_time_pytorch, all_inf_times_pytorch, acc_pytorch = run_pytorch_inference(\n",
    "        trained_pytorch_model,\n",
    "        test_images_subset_inf, test_labels_one_hot_subset_inf, # Per accuracy usa one-hot\n",
    "        model_name=\"PyTorch CNN\",\n",
    "        num_samples_to_test=NUM_TEST_SAMPLES_INFERENCE\n",
    "    )\n",
    "    print(f\"Avg Inference Time/sample (PyTorch CNN): {avg_inf_time_pytorch:.6f}s (Accuracy: {acc_pytorch:.2f}%)\")\n",
    "    df_inference_times[\"PyTorch CNN\"] = all_inf_times_pytorch\n",
    "\n",
    "\n",
    "    print(\"\\nDescriptive Statistics of Inference Times per sample (seconds):\")\n",
    "    print(df_inference_times.describe())\n",
    "\n",
    "    plt.figure(figsize=(10, 8)) # Aumentato per più modelli\n",
    "    df_inference_times.boxplot(showfliers=False)\n",
    "    plt.title('Boxplot of Inference Times per Sample')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3eaeb8",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7b0a1",
   "metadata": {},
   "source": [
    "### MLP Backward Pass\n",
    "\n",
    "`ReLU_SoftMax_FC_Backward` computes gradients for the MLP. Inputs: batch size `bs`, predictions `pred` ($P$), true `labels` ($Y$), weights $W_1, W_2$, hidden activation `fa` ($A_1$), hidden pre-activation `fl` ($Z_1$), MLP input `i_mlp` ($X_{mlp}$).\n",
    "\n",
    "**Gradients (from output layer backwards):**\n",
    "\n",
    "1.  $\\frac{\\partial L}{\\partial Z_2} = P - Y$ (`dL_dz2`)\n",
    "2.  $\\frac{\\partial L}{\\partial W_2} = A_1^T \\frac{\\partial L}{\\partial Z_2}$ (`dL_dw2`)\n",
    "3.  $\\frac{\\partial L}{\\partial b_2} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_2}$ (`dL_db2`)\n",
    "4.  $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} W_2^T$ (`dL_dfa`)\n",
    "5.  $\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\text{ReLU}'(Z_1)$ (`dL_dfl`, where $\\text{ReLU}'(Z_1)$ is 1 if $Z_1 > 0$, else 0)\n",
    "6.  $\\frac{\\partial L}{\\partial W_1} = X_{mlp}^T \\frac{\\partial L}{\\partial Z_1}$ (`dL_dw1`)\n",
    "7.  $\\frac{\\partial L}{\\partial b_1} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_1}$ (`dL_db1`)\n",
    "8.  $\\frac{\\partial L}{\\partial X_{mlp}} = \\frac{\\partial L}{\\partial Z_1} W_1^T$ (`dL_i_mlp`) (gradient to pass to conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10736bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(batch_size, predictions, labels, w1, w2, output_after_activation, layer_output, input_to_mlp):\n",
    "    dL_dz2 = predictions - labels[0:batch_size]\n",
    "    dL_dw2 = np.matmul(output_after_activation.T, dL_dz2)\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_doutput_after_activation = np.matmul(dL_dz2, w2.T)\n",
    "    dReLU = (layer_output > 0).astype(float)\n",
    "    dL_dlayer_output = dL_doutput_after_activation * dReLU\n",
    "    dL_dw1 = np.matmul(input_to_mlp.reshape(batch_size, -1).T, dL_dlayer_output)\n",
    "    dL_db1 = np.sum(dL_dlayer_output, axis=0)\n",
    "    dL_input_to_mlp = np.matmul(dL_dlayer_output, w1.T)\n",
    "    return dL_input_to_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd7105",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf96a8c",
   "metadata": {},
   "source": [
    "`crossEntropy` calculates the Categorical Cross-Entropy loss for a single sample.\n",
    "\n",
    "**Formula:** Given predicted probabilities $P=(p_1, ..., p_K)$ and one-hot true label $Y=(y_1, ..., y_K)$:\n",
    "$$ L(P, Y) = - \\sum_{k=1}^{K} y_k \\log(p_k) $$\n",
    "If class $c$ is the true class ($y_c=1$), $L = - \\log(p_c)$.\n",
    "A small epsilon (`1/100000`) is added to $p$ to prevent $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1291609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(predictions,true_labels):\n",
    "    predictions = predictions+(1/100000) # for numerical stability\n",
    "    return -np.dot(true_labels,np.log(predictions).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49079",
   "metadata": {},
   "source": [
    "## Inference: Comparing Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e3c9f",
   "metadata": {},
   "source": [
    "This section compares the inference performance and correctness of three CNN implementations: PyTorch, nested_loops and im2col. All use identical pre-trained weights.\n",
    "\n",
    "**Objectives:**\n",
    "1.  **Correctness:** Verify that all three models yield the same predictions.\n",
    "2.  **Performance measurement:** Compare average inference time per image.\n",
    "\n",
    "The loop iterates through test images, runs each model, records predictions and times. This demonstrates the efficiency gains from optimized libraries (PyTorch) and im2col approach over the naive loops one. The `padding` and `stride` parameters are set to match the PyTorch model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab6c9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 2000/2000 [00:06<00:00, 288.18it/s, average_times=pytorch: 0.0002197458 s, im2c: 0.000188076 s, im2c_opt: 0.0001674559 s, correct_predictions=2.4409763905562225%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time in seconds:\n",
      "PyTorch:\t0.0010982897281646728 s,\n",
      "im2col:\t\t0.0009400037527084351 s, \n",
      "im2col_optim:\t0.0008369446992874146 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "PyTorch",
         "type": "scatter",
         "y": [
          0.0039327144622802734,
          0.0009999275207519531,
          0,
          0,
          0,
          0,
          0.0009963512420654297,
          0.000997781753540039,
          0.0010008811950683594,
          0.0010001659393310547,
          0.00099945068359375,
          0,
          0.0010001659393310547,
          0,
          0.0009989738464355469,
          0.0010673999786376953,
          0.0007357597351074219,
          0.0010006427764892578,
          0.0010008811950683594,
          0.0010004043579101562,
          0.0008833408355712891,
          0.0012276172637939453,
          0.0010006427764892578,
          0.0010008811950683594,
          0.0009992122650146484,
          0.001065969467163086,
          0.0010006427764892578,
          0,
          0,
          0,
          0,
          0.0004558563232421875,
          0,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.002008676528930664,
          0,
          0,
          0,
          0,
          0.0010068416595458984,
          0,
          0,
          0,
          0,
          0.014272212982177734,
          0,
          0,
          0,
          0,
          0.014060258865356445,
          0,
          0,
          0,
          0,
          0,
          0.0009663105010986328,
          0,
          0,
          0,
          0.013677597045898438,
          0,
          0,
          0,
          0,
          0,
          0.000614166259765625,
          0,
          0,
          0,
          0,
          0.0014255046844482422,
          0,
          0,
          0,
          0,
          0.0010099411010742188,
          0,
          0,
          0,
          0,
          0.0010097026824951172,
          0,
          0,
          0,
          0,
          0.014191865921020508,
          0,
          0,
          0,
          0,
          0.014159917831420898,
          0,
          0,
          0,
          0,
          0.013745784759521484,
          0,
          0,
          0,
          0,
          0.014239072799682617,
          0,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010097026824951172,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014166593551635742,
          0,
          0,
          0,
          0,
          0,
          0.0004532337188720703,
          0,
          0,
          0,
          0,
          0.0008678436279296875,
          0,
          0,
          0,
          0,
          0.0015146732330322266,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010082721710205078,
          0,
          0,
          0,
          0,
          0.014261007308959961,
          0.0015521049499511719,
          0,
          0,
          0,
          0.013148069381713867,
          0,
          0,
          0,
          0,
          0.014271736145019531,
          0,
          0,
          0,
          0,
          0,
          0.0014650821685791016,
          0,
          0,
          0,
          0,
          0.001010894775390625,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014123916625976562,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014028072357177734,
          0,
          0,
          0,
          0,
          0,
          0.001310586929321289,
          0,
          0,
          0,
          0,
          0.0009942054748535156,
          0,
          0,
          0,
          0,
          0.0010144710540771484,
          0,
          0,
          0,
          0,
          0.01428675651550293,
          0,
          0,
          0,
          0,
          0,
          0.0003955364227294922,
          0,
          0,
          0,
          0,
          0.0013856887817382812,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014211893081665039,
          0,
          0,
          0,
          0,
          0.014157772064208984,
          0,
          0,
          0,
          0,
          0,
          0.0013976097106933594,
          0,
          0,
          0,
          0,
          0.0009963512420654297,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014626502990722656,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001373291015625,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.014282941818237305,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.0014438629150390625,
          0,
          0,
          0,
          0,
          0.0006287097930908203,
          0,
          0,
          0,
          0.014078617095947266,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014182329177856445,
          0,
          0,
          0,
          0,
          0.014258384704589844,
          0,
          0,
          0,
          0,
          0.014244794845581055,
          0,
          0,
          0,
          0,
          0,
          0.0013310909271240234,
          0,
          0,
          0,
          0,
          0.0009996891021728516,
          0,
          0,
          0,
          0,
          0.0009992122650146484,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.014344453811645508,
          0,
          0,
          0,
          0,
          0.014221429824829102,
          0,
          0,
          0,
          0,
          0,
          0.0004782676696777344,
          0,
          0,
          0,
          0,
          0.0009884834289550781,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014213323593139648,
          0,
          0,
          0,
          0,
          0,
          0.0003426074981689453,
          0,
          0,
          0,
          0,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.014219045639038086,
          0,
          0,
          0,
          0,
          0.014249801635742188,
          0,
          0,
          0,
          0,
          0,
          0.0013701915740966797,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.0010118484497070312,
          0,
          0,
          0,
          0,
          0.014321327209472656,
          0,
          0,
          0,
          0,
          0.01422572135925293,
          0,
          0,
          0,
          0,
          0.014128923416137695,
          0,
          0,
          0,
          0,
          0.014183998107910156,
          0,
          0,
          0,
          0,
          0.014178276062011719,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0,
          0.011759281158447266,
          0.001331329345703125,
          0,
          0.0014028549194335938,
          0,
          0,
          0,
          0.0013537406921386719,
          0,
          0,
          0,
          0.0020110607147216797,
          0,
          0,
          0,
          0.014273881912231445,
          0,
          0,
          0,
          0,
          0.0010156631469726562,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014182806015014648,
          0,
          0,
          0,
          0,
          0,
          0.0006742477416992188,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013795852661132812,
          0,
          0,
          0,
          0,
          0.014220714569091797,
          0,
          0,
          0,
          0,
          0,
          0.001354217529296875,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.0020112991333007812,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014258146286010742,
          0,
          0,
          0,
          0,
          0,
          0.0005075931549072266,
          0,
          0,
          0,
          0.014154672622680664,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010170936584472656,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0005202293395996094,
          0,
          0,
          0,
          0,
          0.0004107952117919922,
          0,
          0,
          0,
          0,
          0.0013623237609863281,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.014317989349365234,
          0,
          0,
          0,
          0,
          0.014229536056518555,
          0,
          0,
          0,
          0,
          0,
          0.0014522075653076172,
          0,
          0,
          0,
          0,
          0.0013840198516845703,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0010006427764892578,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014242887496948242,
          0,
          0,
          0,
          0,
          0.014208793640136719,
          0,
          0,
          0,
          0,
          0,
          0.00042366981506347656,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014034032821655273,
          0,
          0,
          0,
          0,
          0,
          0.0013077259063720703,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014299154281616211,
          0,
          0,
          0,
          0,
          0,
          0.001323699951171875,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.014245748519897461,
          0,
          0,
          0,
          0,
          0.014219999313354492,
          0,
          0,
          0,
          0,
          0.0009961128234863281,
          0,
          0,
          0,
          0.014066219329833984,
          0,
          0,
          0,
          0,
          0.014342069625854492,
          0,
          0,
          0,
          0,
          0.014219284057617188,
          0,
          0,
          0,
          0,
          0.014310121536254883,
          0,
          0,
          0,
          0,
          0.014189958572387695,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014265298843383789,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0010101795196533203,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.01432657241821289,
          0,
          0,
          0,
          0,
          0.014137029647827148,
          0,
          0,
          0,
          0,
          0.014282464981079102,
          0.00039839744567871094,
          0,
          0,
          0,
          0,
          0.00044536590576171875,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0,
          0,
          0,
          0,
          0.0013382434844970703,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001600503921508789,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014248371124267578,
          0,
          0,
          0,
          0,
          0.014194726943969727,
          0,
          0,
          0,
          0,
          0.014140844345092773,
          0,
          0,
          0,
          0,
          0.014212369918823242,
          0,
          0,
          0,
          0,
          0.014210224151611328,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01433253288269043,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.014157533645629883,
          0,
          0,
          0,
          0,
          0,
          0.0003287792205810547,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.014345645904541016,
          0,
          0,
          0,
          0,
          0,
          0.00045180320739746094,
          0,
          0,
          0,
          0,
          0.0013463497161865234,
          0,
          0,
          0,
          0,
          0.0010159015655517578,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014223337173461914,
          0,
          0,
          0,
          0,
          0.014215230941772461,
          0,
          0,
          0.007720232009887695,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0020122528076171875,
          0,
          0,
          0,
          0.014171361923217773,
          0,
          0,
          0,
          0.014221668243408203,
          0.0010030269622802734,
          0.0003800392150878906,
          0,
          0.009273767471313477,
          0.0019986629486083984,
          0,
          0,
          0,
          0.001001596450805664,
          0,
          0,
          0,
          0,
          0.0010025501251220703,
          0,
          0,
          0,
          0.0009975433349609375,
          0.0009839534759521484,
          0,
          0,
          0,
          0.00099945068359375,
          0,
          0,
          0,
          0,
          0.0010013580322265625,
          0.00039505958557128906,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.0009999275207519531,
          0.0010006427764892578,
          0,
          0,
          0,
          0.001997232437133789,
          0.0014889240264892578,
          0,
          0,
          0.010178804397583008,
          0,
          0.0013403892517089844,
          0,
          0,
          0.009371757507324219,
          0.00401759147644043,
          0.0019981861114501953,
          0,
          0.008769035339355469,
          0.001999378204345703,
          0,
          0,
          0.0010111331939697266,
          0.0009953975677490234,
          0,
          0,
          0.009385347366333008,
          0.001043558120727539,
          0.001310586929321289,
          0,
          0,
          0.009428977966308594,
          0.0009987354278564453,
          0,
          0,
          0,
          0.001010894775390625,
          0,
          0,
          0,
          0,
          0.0009860992431640625,
          0.00036525726318359375,
          0,
          0,
          0.01030111312866211,
          0.0009658336639404297,
          0.001298666000366211,
          0,
          0,
          0,
          0.0009570121765136719,
          0,
          0,
          0,
          0.0010101795196533203,
          0.0010173320770263672,
          0,
          0,
          0,
          0.0010209083557128906,
          0.0009772777557373047,
          0,
          0,
          0,
          0.0009982585906982422,
          0.0009822845458984375,
          0,
          0,
          0.009753704071044922,
          0.0009553432464599609,
          0.0013804435729980469,
          0,
          0,
          0.0009570121765136719,
          0.0009775161743164062,
          0,
          0,
          0,
          0.0019989013671875,
          0.0013103485107421875,
          0,
          0,
          0.010674715042114258,
          0.000997304916381836,
          0,
          0,
          0,
          0.0017507076263427734,
          0,
          0,
          0,
          0.0008661746978759766,
          0,
          0,
          0,
          0.0009982585906982422,
          0.0009846687316894531,
          0,
          0,
          0.0010154247283935547,
          0.0009784698486328125,
          0,
          0,
          0.009293556213378906,
          0.0010395050048828125,
          0,
          0,
          0,
          0.009283781051635742,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0009758472442626953,
          0,
          0,
          0,
          0.001024007797241211,
          0.0009572505950927734,
          0,
          0,
          0,
          0.0009996891021728516,
          0.0009772777557373047,
          0,
          0,
          0.009293317794799805,
          0.0009560585021972656,
          0,
          0,
          0,
          0.001008749008178711,
          0.0009987354278564453,
          0.0013747215270996094,
          0,
          0,
          0.0010094642639160156,
          0.001001119613647461,
          0,
          0,
          0.0020062923431396484,
          0.00099945068359375,
          0.0013353824615478516,
          0,
          0,
          0.0009984970092773438,
          0.0010023117065429688,
          0,
          0,
          0.008437395095825195,
          0.001035451889038086,
          0,
          0,
          0,
          0.0010356903076171875,
          0.000997304916381836,
          0,
          0,
          0,
          0.0011310577392578125,
          0.0009551048278808594,
          0,
          0,
          0,
          0.0009582042694091797,
          0.0009484291076660156,
          0,
          0,
          0,
          0.0010025501251220703,
          0.0009810924530029297,
          0,
          0,
          0.009419918060302734,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0010612010955810547,
          0.0009751319885253906,
          0,
          0,
          0,
          0.0009586811065673828,
          0.0009565353393554688,
          0,
          0,
          0.009237051010131836,
          0.0009982585906982422,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0010035037994384766,
          0.0009753704071044922,
          0,
          0,
          0.009348869323730469,
          0.0009844303131103516,
          0.001356363296508789,
          0,
          0,
          0.0009970664978027344,
          0.0009832382202148438,
          0,
          0,
          0.009333133697509766,
          0.0009980201721191406,
          0,
          0,
          0,
          0.0009984970092773438,
          0.0009903907775878906,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0.0009756088256835938,
          0.0010170936584472656,
          0,
          0,
          0.0009799003601074219,
          0,
          0,
          0,
          0.0009772777557373047,
          0.0016307830810546875,
          0.0012066364288330078,
          0.000997781753540039,
          0.0013947486877441406,
          0,
          0.0013086795806884766,
          0,
          0,
          0.0009987354278564453,
          0.0009768009185791016,
          0,
          0,
          0.009300470352172852,
          0.0009765625,
          0.0003886222839355469,
          0,
          0,
          0.01024627685546875,
          0.0009992122650146484,
          0.0009984970092773438,
          0,
          0,
          0.009278297424316406,
          0.0009663105010986328,
          0,
          0,
          0,
          0.0010488033294677734,
          0,
          0,
          0,
          0,
          0.0010399818420410156,
          0.000982522964477539,
          0,
          0.003596782684326172,
          0.0008633136749267578,
          0.010932445526123047,
          0.0007014274597167969,
          0,
          0.0020012855529785156,
          0,
          0,
          0.0009968280792236328,
          0.0005135536193847656,
          0,
          0.009627342224121094,
          0.0010004043579101562,
          0,
          0,
          0,
          0.0010180473327636719,
          0,
          0,
          0,
          0,
          0.0009560585021972656,
          0.0009784698486328125,
          0,
          0,
          0.009267807006835938,
          0,
          0.0009768009185791016,
          0,
          0,
          0.009385824203491211,
          0.00099945068359375,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0.00035834312438964844,
          0,
          0,
          0.0009987354278564453,
          0.0014843940734863281,
          0,
          0,
          0.010405540466308594,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0009989738464355469,
          0.0009756088256835938,
          0,
          0,
          0.009365081787109375,
          0.0009524822235107422,
          0,
          0,
          0.011121034622192383,
          0,
          0,
          0,
          0.014556646347045898,
          0.0009818077087402344,
          0,
          0,
          0.0010111331939697266,
          0.0010116100311279297,
          0,
          0,
          0,
          0.0009534358978271484,
          0.0014176368713378906,
          0,
          0.002004861831665039,
          0.001009225845336914,
          0.0009751319885253906,
          0,
          0,
          0.010258674621582031,
          0.0009775161743164062,
          0.0009777545928955078,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0.0010077953338623047,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0.0009729862213134766,
          0,
          0,
          0,
          0.0010523796081542969,
          0.0009765625,
          0,
          0,
          0.009364843368530273,
          0.0010020732879638672,
          0.0009796619415283203,
          0,
          0,
          0.00973963737487793,
          0.0009989738464355469,
          0.0013055801391601562,
          0,
          0,
          0.0011131763458251953,
          0,
          0,
          0,
          0,
          0.0009548664093017578,
          0.0009989738464355469,
          0,
          0,
          0.009272575378417969,
          0.000997781753540039,
          0,
          0,
          0,
          0.0010080337524414062,
          0,
          0,
          0,
          0,
          0.0009777545928955078,
          0.0009999275207519531,
          0,
          0,
          0.0009996891021728516,
          0.0009999275207519531,
          0,
          0,
          0.0009999275207519531,
          0.00099945068359375,
          0.0020079612731933594,
          0.0020062923431396484,
          0.0010802745819091797,
          0.0009992122650146484,
          0.0021653175354003906,
          0,
          0.001009225845336914,
          0.0019996166229248047,
          0,
          0,
          0.001008749008178711,
          0.002009868621826172,
          0,
          0,
          0.0009982585906982422,
          0,
          0,
          0,
          0.0010116100311279297,
          0.0009729862213134766,
          0,
          0,
          0,
          0.00098419189453125,
          0.00045299530029296875,
          0,
          0,
          0.0010111331939697266,
          0.0009996891021728516,
          0,
          0,
          0,
          0.0019989013671875,
          0.0020220279693603516,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0.009133338928222656,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009524822235107422,
          0.0014913082122802734,
          0,
          0,
          0.010149717330932617,
          0.001001119613647461,
          0,
          0,
          0,
          0.0010113716125488281,
          0,
          0,
          0,
          0,
          0.0009832382202148438,
          0.0009992122650146484,
          0,
          0,
          0.009206533432006836,
          0.0009527206420898438,
          0,
          0,
          0,
          0.009328603744506836,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010089874267578125,
          0.0010006427764892578,
          0,
          0,
          0,
          0.0009608268737792969,
          0.001401662826538086,
          0,
          0,
          0,
          0.00099945068359375,
          0.0006871223449707031,
          0,
          0,
          0,
          0.0009818077087402344,
          0,
          0,
          0,
          0.0009968280792236328,
          0.000995635986328125,
          0,
          0,
          0.009782791137695312,
          0.0010008811950683594,
          0.0004870891571044922,
          0,
          0,
          0.001009225845336914,
          0.0009508132934570312,
          0,
          0,
          0,
          0.0010216236114501953,
          0.0009524822235107422,
          0,
          0,
          0.0010104179382324219,
          0.0009996891021728516,
          0.0014090538024902344,
          0,
          0,
          0.0009982585906982422,
          0,
          0,
          0,
          0.0010094642639160156,
          0.0009818077087402344,
          0,
          0,
          0,
          0.0009810924530029297,
          0.0005087852478027344,
          0,
          0,
          0.011275529861450195,
          0.00103759765625,
          0,
          0,
          0,
          0.0009992122650146484,
          0.0009737014770507812,
          0,
          0,
          0.009125947952270508,
          0,
          0.0004992485046386719,
          0,
          0,
          0.0010106563568115234,
          0.0010020732879638672,
          0,
          0,
          0,
          0.0009491443634033203,
          0,
          0,
          0,
          0,
          0.0010175704956054688,
          0.001470804214477539,
          0,
          0,
          0.0010542869567871094,
          0.0009667873382568359,
          0,
          0,
          0,
          0.0010018348693847656,
          0.0004417896270751953,
          0.0020046234130859375,
          0,
          0.0009987354278564453,
          0.0011181831359863281,
          0,
          0,
          0.0010175704956054688,
          0.0008819103240966797,
          0,
          0,
          0.010118961334228516,
          0.0010008811950683594,
          0.00039577484130859375,
          0,
          0,
          0.0009491443634033203,
          0,
          0,
          0,
          0.001009225845336914,
          0.0009996891021728516,
          0,
          0,
          0,
          0.0009713172912597656,
          0,
          0,
          0,
          0,
          0,
          0,
          0.012903690338134766,
          0.001001596450805664,
          0,
          0,
          0,
          0.01270294189453125,
          0.0003581047058105469,
          0,
          0,
          0,
          0.0009746551513671875,
          0,
          0,
          0,
          0.0010004043579101562,
          0,
          0,
          0,
          0.012598037719726562,
          0.0009989738464355469,
          0,
          0,
          0,
          0.00096893310546875,
          0.0010328292846679688,
          0,
          0,
          0.011629819869995117,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01255345344543457,
          0.0008540153503417969,
          0,
          0,
          0,
          0.013892173767089844,
          0,
          0,
          0,
          0,
          0.014034032821655273,
          0,
          0,
          0,
          0,
          0.013918399810791016,
          0,
          0,
          0,
          0,
          0.0140380859375,
          0.000997304916381836,
          0.0009875297546386719,
          0,
          0.0009570121765136719,
          0.0009865760803222656,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0022161006927490234,
          0,
          0,
          0,
          0.0009198188781738281,
          0.0035343170166015625,
          0,
          0,
          0,
          0.0006244182586669922,
          0,
          0,
          0,
          0.013982295989990234,
          0.0007874965667724609,
          0,
          0,
          0,
          0.013918161392211914,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009679794311523438,
          0,
          0,
          0,
          0,
          0.0007452964782714844,
          0,
          0,
          0,
          0.013853073120117188,
          0,
          0,
          0,
          0,
          0.013939619064331055,
          0,
          0,
          0,
          0,
          0.014078378677368164,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0011470317840576172,
          0,
          0,
          0,
          0,
          0.0011522769927978516,
          0,
          0,
          0,
          0,
          0.0011620521545410156,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014064788818359375,
          0,
          0,
          0,
          0,
          0.013947725296020508,
          0.0007262229919433594,
          0,
          0,
          0,
          0.013774633407592773,
          0.0006396770477294922,
          0,
          0,
          0,
          0,
          0.0016222000122070312,
          0,
          0,
          0,
          0,
          0.0004868507385253906,
          0,
          0,
          0,
          0,
          0.0010106563568115234,
          0,
          0,
          0,
          0,
          0.0011563301086425781,
          0,
          0,
          0,
          0,
          0.001142740249633789,
          0,
          0,
          0,
          0,
          0.014001607894897461,
          0,
          0,
          0,
          0,
          0.013990640640258789,
          0,
          0,
          0,
          0,
          0.0020842552185058594,
          0,
          0,
          0,
          0.013555288314819336,
          0,
          0,
          0,
          0,
          0.01400303840637207,
          0,
          0,
          0,
          0,
          0.01405024528503418,
          0,
          0,
          0,
          0,
          0.013669490814208984,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0010406970977783203,
          0,
          0,
          0,
          0,
          0.002009153366088867,
          0,
          0,
          0,
          0,
          0.0009622573852539062,
          0,
          0,
          0,
          0,
          0.0010619163513183594,
          0,
          0,
          0,
          0,
          0.0009677410125732422,
          0,
          0,
          0,
          0,
          0.0009486675262451172,
          0,
          0,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014028310775756836,
          0,
          0,
          0,
          0,
          0.014060497283935547,
          0,
          0,
          0,
          0,
          0.014008283615112305,
          0.0006551742553710938,
          0,
          0,
          0,
          0,
          0.0007028579711914062,
          0,
          0,
          0,
          0,
          0.0006127357482910156,
          0,
          0,
          0,
          0,
          0.0015063285827636719,
          0,
          0,
          0,
          0,
          0.0006101131439208984,
          0,
          0,
          0,
          0,
          0.0010030269622802734,
          0,
          0,
          0,
          0,
          0.0011491775512695312,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01413583755493164,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014068841934204102,
          0,
          0,
          0,
          0,
          0.014034748077392578,
          0,
          0,
          0
         ]
        },
        {
         "line": {
          "color": "green"
         },
         "mode": "lines+markers",
         "name": "im2col",
         "type": "scatter",
         "y": [
          0.0020074844360351562,
          0.0010006427764892578,
          0,
          0,
          0.005683183670043945,
          0.00099945068359375,
          0.0010006427764892578,
          0.0009999275207519531,
          0.0009996891021728516,
          0.0009999275207519531,
          0.0010023117065429688,
          0,
          0.00099945068359375,
          0.0010330677032470703,
          0.0010023117065429688,
          0.0009980201721191406,
          0.0010290145874023438,
          0.0009989738464355469,
          0.0009989738464355469,
          0.00099945068359375,
          0.0010836124420166016,
          0.0012149810791015625,
          0.0010004043579101562,
          0.0010001659393310547,
          0.0011305809020996094,
          0.0010020732879638672,
          0.0013964176177978516,
          0,
          0,
          0,
          0.013180255889892578,
          0,
          0,
          0,
          0,
          0.014160871505737305,
          0.0009310245513916016,
          0,
          0,
          0,
          0,
          0.0005421638488769531,
          0,
          0,
          0,
          0,
          0.00099945068359375,
          0,
          0,
          0,
          0,
          0.002008676528930664,
          0,
          0,
          0,
          0,
          0.0010082721710205078,
          0,
          0,
          0,
          0,
          0.01424264907836914,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.014031648635864258,
          0,
          0,
          0,
          0,
          0.014025449752807617,
          0,
          0,
          0,
          0,
          0,
          0.001481771469116211,
          0,
          0,
          0,
          0,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0.0010075569152832031,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.0010094642639160156,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014358758926391602,
          0,
          0,
          0,
          0,
          0,
          0.0013358592987060547,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0.0010097026824951172,
          0,
          0,
          0,
          0,
          0.014343500137329102,
          0,
          0,
          0,
          0,
          0.014225959777832031,
          0,
          0,
          0,
          0,
          0.013839006423950195,
          0,
          0,
          0,
          0,
          0,
          0.0004444122314453125,
          0,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0010101795196533203,
          0,
          0,
          0,
          0,
          0.014401674270629883,
          0,
          0,
          0,
          0,
          0,
          0.0014786720275878906,
          0,
          0,
          0,
          0,
          0.001016855239868164,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.014303445816040039,
          0,
          0,
          0,
          0,
          0.0010120868682861328,
          0,
          0,
          0,
          0,
          0.014256715774536133,
          0,
          0,
          0,
          0,
          0.014296293258666992,
          0.0004096031188964844,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0010166168212890625,
          0,
          0,
          0,
          0,
          0.014265060424804688,
          0,
          0,
          0,
          0,
          0.014293432235717773,
          0,
          0,
          0,
          0,
          0,
          0.0005095005035400391,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.014361143112182617,
          0,
          0,
          0,
          0,
          0,
          0.0003876686096191406,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.014314413070678711,
          0,
          0,
          0,
          0,
          0.014662504196166992,
          0,
          0,
          0,
          0,
          0,
          0.0014014244079589844,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0.0010118484497070312,
          0,
          0,
          0,
          0,
          0.0006234645843505859,
          0,
          0,
          0,
          0.014107704162597656,
          0,
          0,
          0,
          0,
          0.01407623291015625,
          0,
          0,
          0,
          0,
          0.0010120868682861328,
          0,
          0,
          0,
          0,
          0.0010118484497070312,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.0010159015655517578,
          0,
          0,
          0,
          0,
          0.0010170936584472656,
          0,
          0,
          0,
          0,
          0.014389514923095703,
          0,
          0,
          0,
          0,
          0,
          0.0004553794860839844,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0013287067413330078,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.014223098754882812,
          0,
          0,
          0,
          0,
          0,
          0.0005185604095458984,
          0,
          0,
          0,
          0,
          0.00040793418884277344,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.014350175857543945,
          0,
          0,
          0,
          0,
          0.014227867126464844,
          0,
          0,
          0,
          0,
          0,
          0.0014302730560302734,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014371871948242188,
          0,
          0,
          0,
          0,
          0,
          0.00039696693420410156,
          0,
          0,
          0,
          0,
          0.00145721435546875,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0.0010161399841308594,
          0,
          0,
          0,
          0,
          0.0010149478912353516,
          0,
          0,
          0,
          0,
          0.01427602767944336,
          0,
          0,
          0,
          0,
          0,
          0.0003230571746826172,
          0,
          0,
          0,
          0,
          0.0014159679412841797,
          0,
          0,
          0,
          0.014834403991699219,
          0,
          0.0019979476928710938,
          0.002449512481689453,
          0,
          0.0010411739349365234,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0005183219909667969,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.000993967056274414,
          0,
          0,
          0,
          0,
          0.0010132789611816406,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.01443338394165039,
          0,
          0,
          0,
          0,
          0.013868331909179688,
          0,
          0,
          0,
          0,
          0.015257596969604492,
          0,
          0,
          0,
          0,
          0.0010161399841308594,
          0,
          0,
          0,
          0,
          0.0010144710540771484,
          0,
          0,
          0,
          0,
          0.014390707015991211,
          0,
          0,
          0,
          0,
          0,
          0.00044083595275878906,
          0,
          0,
          0,
          0,
          0.0013990402221679688,
          0,
          0,
          0,
          0,
          0.0009136199951171875,
          0,
          0,
          0,
          0,
          0.0014917850494384766,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.014266490936279297,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.015363216400146484,
          0,
          0,
          0,
          0,
          0.0015544891357421875,
          0,
          0,
          0,
          0,
          0.0005726814270019531,
          0,
          0,
          0,
          0.014169931411743164,
          0,
          0,
          0,
          0,
          0.013900041580200195,
          0,
          0,
          0,
          0,
          0.014250755310058594,
          0,
          0,
          0,
          0,
          0,
          0.0005955696105957031,
          0,
          0,
          0,
          0,
          0.0013632774353027344,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0,
          0,
          0,
          0,
          0.0010173320770263672,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014301300048828125,
          0,
          0,
          0,
          0,
          0.014200925827026367,
          0,
          0,
          0,
          0,
          0,
          0.0004849433898925781,
          0,
          0,
          0,
          0.014173507690429688,
          0.00043010711669921875,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010190010070800781,
          0,
          0,
          0,
          0,
          0.0010111331939697266,
          0,
          0,
          0,
          0,
          0.01431417465209961,
          0,
          0,
          0,
          0,
          0.014181375503540039,
          0,
          0,
          0,
          0,
          0.0010149478912353516,
          0,
          0,
          0,
          0,
          0.014400243759155273,
          0,
          0,
          0,
          0,
          0,
          0.0003955364227294922,
          0,
          0,
          0,
          0,
          0.0014338493347167969,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014304161071777344,
          0,
          0,
          0,
          0,
          0,
          0.0013594627380371094,
          0,
          0,
          0,
          0,
          0.0009963512420654297,
          0,
          0,
          0,
          0,
          0.0010156631469726562,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0005354881286621094,
          0,
          0,
          0,
          0.0010132789611816406,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.0011343955993652344,
          0,
          0,
          0,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0010006427764892578,
          0,
          0,
          0,
          0,
          0.0014524459838867188,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014292478561401367,
          0,
          0,
          0,
          0,
          0,
          0.0004017353057861328,
          0,
          0,
          0,
          0,
          0.0014729499816894531,
          0,
          0,
          0,
          0.014180421829223633,
          0.0004000663757324219,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001619100570678711,
          0,
          0,
          0,
          0.014014959335327148,
          0,
          0,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.001425027847290039,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0.0010216236114501953,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.0010120868682861328,
          0,
          0,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0.013613700866699219,
          0,
          0,
          0,
          0,
          0.001016378402709961,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.014338254928588867,
          0,
          0,
          0,
          0,
          0,
          0.0013859272003173828,
          0,
          0,
          0,
          0,
          0.0013124942779541016,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.014360904693603516,
          0,
          0,
          0,
          0,
          0.014224767684936523,
          0,
          0,
          0,
          0,
          0,
          0.0004942417144775391,
          0,
          0,
          0,
          0,
          0.0014145374298095703,
          0,
          0,
          0,
          0,
          0.0014805793762207031,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.0010149478912353516,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0.0004680156707763672,
          0,
          0,
          0,
          0.00200653076171875,
          0,
          0,
          0,
          0.0010111331939697266,
          0.00101470947265625,
          0,
          0,
          0.0010097026824951172,
          0.00099945068359375,
          0,
          0,
          0.01041412353515625,
          0.0010008811950683594,
          0,
          0,
          0,
          0.0009965896606445312,
          0.0009999275207519531,
          0,
          0,
          0.00928497314453125,
          0.0010006427764892578,
          0.0014238357543945312,
          0,
          0,
          0.0010137557983398438,
          0.0010006427764892578,
          0,
          0,
          0,
          0.002011537551879883,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010275840759277344,
          0.0009975433349609375,
          0,
          0,
          0.009675741195678711,
          0.0009980201721191406,
          0.000997781753540039,
          0,
          0,
          0.009200096130371094,
          0.0010013580322265625,
          0,
          0,
          0,
          0.0010135173797607422,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.003879070281982422,
          0.0020017623901367188,
          0,
          0.0020110607147216797,
          0.0015571117401123047,
          0,
          0.009074211120605469,
          0.0008144378662109375,
          0.0013055801391601562,
          0,
          0,
          0.0010101795196533203,
          0.0009579658508300781,
          0,
          0,
          0,
          0.0010340213775634766,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0010223388671875,
          0,
          0,
          0,
          0.009186267852783203,
          0.0010037422180175781,
          0,
          0,
          0,
          0.0010118484497070312,
          0.0010230541229248047,
          0,
          0,
          0,
          0.0010197162628173828,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010209083557128906,
          0.0010099411010742188,
          0,
          0,
          0.009297370910644531,
          0.0009784698486328125,
          0.0010230541229248047,
          0,
          0,
          0.009252548217773438,
          0.0009982585906982422,
          0.0013852119445800781,
          0,
          0,
          0.0010411739349365234,
          0.0010230541229248047,
          0,
          0,
          0,
          0.001047372817993164,
          0.0013990402221679688,
          0,
          0,
          0.010181665420532227,
          0.0009989738464355469,
          0,
          0,
          0,
          0.00106048583984375,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013782978057861328,
          0,
          0,
          0,
          0.013932228088378906,
          0.002023458480834961,
          0.0010199546813964844,
          0,
          0,
          0.0010225772857666016,
          0.0009975433349609375,
          0,
          0,
          0.001008749008178711,
          0.0009868144989013672,
          0.0014281272888183594,
          0,
          0,
          0.00103759765625,
          0.0009996891021728516,
          0,
          0,
          0,
          0.002000570297241211,
          0.0013229846954345703,
          0,
          0,
          0.010417461395263672,
          0.0010082721710205078,
          0.00099945068359375,
          0,
          0,
          0.010611772537231445,
          0.0009987354278564453,
          0.0003268718719482422,
          0,
          0,
          0.0010106563568115234,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0009984970092773438,
          0.0010106563568115234,
          0,
          0,
          0,
          0.0010001659393310547,
          0.0009989738464355469,
          0.002006053924560547,
          0.0020051002502441406,
          0.001302957534790039,
          0.002000093460083008,
          0.002006053924560547,
          0.002536296844482422,
          0.003776073455810547,
          0.0009999275207519531,
          0.00036454200744628906,
          0,
          0,
          0.0010075569152832031,
          0.002000093460083008,
          0,
          0,
          0,
          0.0009818077087402344,
          0.0014309883117675781,
          0,
          0,
          0,
          0.00087738037109375,
          0.0009989738464355469,
          0,
          0,
          0.009512186050415039,
          0.0009996891021728516,
          0.001374959945678711,
          0,
          0,
          0.010282278060913086,
          0.000997781753540039,
          0.00031065940856933594,
          0,
          0,
          0.0010709762573242188,
          0.001001596450805664,
          0,
          0,
          0,
          0.0010280609130859375,
          0.0009746551513671875,
          0,
          0,
          0,
          0.0019483566284179688,
          0.0013225078582763672,
          0,
          0,
          0.010264158248901367,
          0.0009992122650146484,
          0.00141143798828125,
          0,
          0,
          0.0010111331939697266,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010004043579101562,
          0.0009527206420898438,
          0,
          0,
          0.009244441986083984,
          0.0009951591491699219,
          0.0013043880462646484,
          0,
          0,
          0.0010080337524414062,
          0.0010259151458740234,
          0,
          0,
          0,
          0.0010030269622802734,
          0.0013051033020019531,
          0,
          0,
          0.0010082721710205078,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0009999275207519531,
          0.0013875961303710938,
          0,
          0.009343147277832031,
          0,
          0,
          0,
          0.014590024948120117,
          0.0010344982147216797,
          0,
          0,
          0.009641885757446289,
          0.002017974853515625,
          0,
          0,
          0.009136676788330078,
          0.0010280609130859375,
          0,
          0.0009958744049072266,
          0.0010008811950683594,
          0.001016855239868164,
          0.001024007797241211,
          0,
          0,
          0,
          0.0010023117065429688,
          0.0010018348693847656,
          0,
          0,
          0,
          0.000997781753540039,
          0,
          0,
          0,
          0.001054525375366211,
          0.001001596450805664,
          0.0003352165222167969,
          0,
          0,
          0.001008749008178711,
          0.0009996891021728516,
          0,
          0.0020074844360351562,
          0,
          0.0009586811065673828,
          0.0009558200836181641,
          0,
          0,
          0,
          0.001970052719116211,
          0.0009992122650146484,
          0,
          0.0019989013671875,
          0,
          0.0019383430480957031,
          0.002802610397338867,
          0.007142782211303711,
          0.0020003318786621094,
          0,
          0.010139942169189453,
          0.0010213851928710938,
          0,
          0,
          0.0010838508605957031,
          0.0010194778442382812,
          0,
          0,
          0,
          0.0009784698486328125,
          0.001983165740966797,
          0,
          0,
          0.00931096076965332,
          0.0010204315185546875,
          0.0013561248779296875,
          0,
          0,
          0.0010111331939697266,
          0.0010249614715576172,
          0.00032258033752441406,
          0,
          0,
          0.0010416507720947266,
          0.0010030269622802734,
          0,
          0,
          0,
          0.0010325908660888672,
          0.001024007797241211,
          0,
          0,
          0,
          0.0010027885437011719,
          0,
          0,
          0,
          0.0010614395141601562,
          0.0010008811950683594,
          0,
          0,
          0,
          0.0009989738464355469,
          0.0009982585906982422,
          0,
          0,
          0.0010099411010742188,
          0.001055002212524414,
          0,
          0,
          0.0010106563568115234,
          0,
          0,
          0,
          0.0009982585906982422,
          0.0010025501251220703,
          0,
          0,
          0.0010013580322265625,
          0.0009868144989013672,
          0,
          0,
          0.009285211563110352,
          0,
          0,
          0.005467414855957031,
          0,
          0.000997304916381836,
          0.0013704299926757812,
          0,
          0,
          0.0010106563568115234,
          0.0009996891021728516,
          0.0003097057342529297,
          0,
          0,
          0.0010101795196533203,
          0.0010004043579101562,
          0,
          0,
          0,
          0.00103759765625,
          0.0019648075103759766,
          0,
          0,
          0.010623931884765625,
          0.0010004043579101562,
          0.0010213851928710938,
          0,
          0,
          0.00930929183959961,
          0.0009486675262451172,
          0.00099945068359375,
          0,
          0,
          0.0010101795196533203,
          0.0009982585906982422,
          0,
          0,
          0,
          0.001058340072631836,
          0.0010251998901367188,
          0,
          0,
          0,
          0.000997304916381836,
          0.0009992122650146484,
          0,
          0,
          0,
          0.002002716064453125,
          0.0014598369598388672,
          0,
          0,
          0.0010364055633544922,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0010235309600830078,
          0.0010237693786621094,
          0,
          0,
          0.010153770446777344,
          0.0009992122650146484,
          0.0015723705291748047,
          0,
          0.009086847305297852,
          0.0010004043579101562,
          0.0016236305236816406,
          0.002007722854614258,
          0,
          0.0009989738464355469,
          0.0009989738464355469,
          0,
          0,
          0.0010247230529785156,
          0.0013501644134521484,
          0,
          0,
          0.0020012855529785156,
          0.001024007797241211,
          0,
          0,
          0.001001119613647461,
          0.0013480186462402344,
          0,
          0,
          0.001001119613647461,
          0,
          0,
          0,
          0.0010211467742919922,
          0.001001119613647461,
          0,
          0,
          0.010586261749267578,
          0.0009975433349609375,
          0,
          0,
          0,
          0.0010266304016113281,
          0.0009982585906982422,
          0,
          0,
          0.009842872619628906,
          0.0010457038879394531,
          0.0005402565002441406,
          0,
          0,
          0.001001596450805664,
          0.0009818077087402344,
          0,
          0,
          0.001046895980834961,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0010013580322265625,
          0,
          0,
          0,
          0.001009225845336914,
          0.0009975433349609375,
          0,
          0,
          0,
          0.0009963512420654297,
          0.0009613037109375,
          0,
          0,
          0.01019287109375,
          0.0010480880737304688,
          0.0014345645904541016,
          0,
          0,
          0.0010101795196533203,
          0.0010218620300292969,
          0.0014238357543945312,
          0,
          0,
          0.0010383129119873047,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0010235309600830078,
          0.0009996891021728516,
          0,
          0,
          0.009328603744506836,
          0.0010027885437011719,
          0,
          0,
          0,
          0.0010097026824951172,
          0.0010225772857666016,
          0,
          0,
          0,
          0.0010263919830322266,
          0.0009999275207519531,
          0,
          0,
          0.010509729385375977,
          0.0010023117065429688,
          0.0017514228820800781,
          0,
          0,
          0.001008749008178711,
          0.0010502338409423828,
          0,
          0,
          0,
          0.0010018348693847656,
          0.0014007091522216797,
          0,
          0,
          0.010300159454345703,
          0.0009772777557373047,
          0.0010030269622802734,
          0,
          0,
          0.0010209083557128906,
          0.001024484634399414,
          0,
          0,
          0,
          0.0010018348693847656,
          0.0005474090576171875,
          0,
          0,
          0.0010204315185546875,
          0.0015971660614013672,
          0,
          0,
          0.009991168975830078,
          0.0009982585906982422,
          0,
          0,
          0,
          0.0010180473327636719,
          0.0009610652923583984,
          0,
          0,
          0.010123491287231445,
          0.0010497570037841797,
          0.0014102458953857422,
          0,
          0,
          0.001010894775390625,
          0.0009999275207519531,
          0,
          0,
          0,
          0.0010197162628173828,
          0.0010223388671875,
          0,
          0,
          0,
          0.0009996891021728516,
          0.0009815692901611328,
          0,
          0,
          0.00921177864074707,
          0.0010197162628173828,
          0,
          0,
          0,
          0.001953601837158203,
          0.0010190010070800781,
          0,
          0,
          0.009222269058227539,
          0.00099945068359375,
          0,
          0.0020058155059814453,
          0.002003908157348633,
          0.0010001659393310547,
          0.0019176006317138672,
          0,
          0,
          0.0009810924530029297,
          0.0014314651489257812,
          0,
          0,
          0.0010271072387695312,
          0.0010526180267333984,
          0,
          0,
          0,
          0.0010018348693847656,
          0.001318216323852539,
          0,
          0,
          0.0010004043579101562,
          0.0010013580322265625,
          0,
          0,
          0.010257244110107422,
          0,
          0,
          0,
          0.014845848083496094,
          0,
          0,
          0,
          0.0010104179382324219,
          0.0010249614715576172,
          0,
          0,
          0,
          0.00099945068359375,
          0,
          0,
          0,
          0.01226949691772461,
          0.0010030269622802734,
          0,
          0,
          0,
          0.0010020732879638672,
          0,
          0,
          0,
          0.0010085105895996094,
          0,
          0,
          0,
          0,
          0.0010006427764892578,
          0,
          0,
          0,
          0.001008749008178711,
          0,
          0,
          0,
          0,
          0.0020003318786621094,
          0,
          0,
          0,
          0.001010894775390625,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0010342597961425781,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0010442733764648438,
          0.0010247230529785156,
          0.0010118484497070312,
          0.0009772777557373047,
          0.0009984970092773438,
          0.001005411148071289,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0,
          0.002009153366088867,
          0,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0.0020074844360351562,
          0,
          0,
          0.0006504058837890625,
          0,
          0.002007722854614258,
          0,
          0.006320476531982422,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0010409355163574219,
          0,
          0,
          0,
          0,
          0.0010089874267578125,
          0,
          0,
          0,
          0,
          0.0007262229919433594,
          0,
          0,
          0,
          0.013913631439208984,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.0010466575622558594,
          0,
          0,
          0,
          0,
          0.0011539459228515625,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.00141143798828125,
          0,
          0,
          0,
          0,
          0.0010037422180175781,
          0,
          0,
          0,
          0,
          0.0014500617980957031,
          0,
          0,
          0,
          0,
          0.001142740249633789,
          0,
          0,
          0,
          0,
          0.0011641979217529297,
          0,
          0,
          0,
          0,
          0.0011615753173828125,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013900995254516602,
          0,
          0,
          0,
          0,
          0.013915538787841797,
          0,
          0,
          0,
          0,
          0.01403665542602539,
          0.0005617141723632812,
          0,
          0,
          0,
          0,
          0.001003265380859375,
          0,
          0,
          0,
          0,
          0.0010192394256591797,
          0,
          0,
          0,
          0,
          0.0011539459228515625,
          0,
          0,
          0,
          0,
          0.002007722854614258,
          0,
          0,
          0,
          0,
          0.0009236335754394531,
          0,
          0,
          0,
          0.001008749008178711,
          0,
          0,
          0,
          0,
          0.0010428428649902344,
          0,
          0,
          0,
          0,
          0.0010366439819335938,
          0,
          0,
          0,
          0,
          0.0010075569152832031,
          0,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.0016295909881591797,
          0,
          0,
          0,
          0,
          0.0006692409515380859,
          0,
          0,
          0,
          0,
          0.0006546974182128906,
          0,
          0,
          0,
          0,
          0.0016093254089355469,
          0,
          0,
          0,
          0,
          0.0010471343994140625,
          0,
          0,
          0,
          0.01356649398803711,
          0.0011930465698242188,
          0,
          0,
          0,
          0.013434171676635742,
          0.0011641979217529297,
          0,
          0,
          0,
          0.011228084564208984,
          0,
          0,
          0,
          0,
          0.0010085105895996094,
          0,
          0,
          0,
          0,
          0.001028299331665039,
          0,
          0,
          0,
          0,
          0.0010433197021484375,
          0,
          0,
          0,
          0,
          0.0140533447265625,
          0,
          0,
          0,
          0,
          0.013875484466552734,
          0,
          0,
          0,
          0,
          0.013900995254516602,
          0,
          0,
          0,
          0,
          0.01400303840637207,
          0,
          0,
          0,
          0,
          0.013928413391113281,
          0.000995635986328125,
          0,
          0,
          0,
          0,
          0.0016634464263916016,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009946823120117188,
          0,
          0,
          0,
          0,
          0.0011467933654785156,
          0,
          0,
          0,
          0,
          0.00115966796875,
          0,
          0,
          0,
          0,
          0.0011441707611083984,
          0,
          0,
          0
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "im2col Optimized",
         "type": "scatter",
         "y": [
          0.0009999275207519531,
          0.0008985996246337891,
          0.0040814876556396484,
          0,
          0.0010075569152832031,
          0.0009999275207519531,
          0,
          0.0009992122650146484,
          0.0009992122650146484,
          0.0010006427764892578,
          0.0009984970092773438,
          0.0010013580322265625,
          0.0009989738464355469,
          0.0009672641754150391,
          0.0013091564178466797,
          0.0008730888366699219,
          0.0009996891021728516,
          0.0010006427764892578,
          0.00099945068359375,
          0.0010001659393310547,
          0.001001596450805664,
          0.0009992122650146484,
          0.0009982585906982422,
          0.0010006427764892578,
          0.0008990764617919922,
          0.0011608600616455078,
          0,
          0,
          0,
          0,
          0.0010077953338623047,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013724088668823242,
          0,
          0,
          0,
          0,
          0,
          0.0004582405090332031,
          0,
          0,
          0,
          0,
          0.000537872314453125,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010082721710205078,
          0,
          0,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.0010080337524414062,
          0,
          0,
          0,
          0,
          0.001008749008178711,
          0,
          0,
          0,
          0,
          0.014298677444458008,
          0,
          0,
          0,
          0,
          0.014117002487182617,
          0.0004546642303466797,
          0,
          0,
          0,
          0,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0.000997781753540039,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010085105895996094,
          0,
          0,
          0,
          0,
          0.0010075569152832031,
          0,
          0,
          0,
          0,
          0.013805150985717773,
          0,
          0,
          0,
          0,
          0,
          0.00046133995056152344,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010099411010742188,
          0,
          0,
          0,
          0,
          0.0010085105895996094,
          0,
          0,
          0,
          0,
          0.0010077953338623047,
          0,
          0,
          0,
          0,
          0.014101505279541016,
          0,
          0,
          0,
          0,
          0,
          0.00038695335388183594,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0013685226440429688,
          0,
          0,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0,
          0.001008749008178711,
          0,
          0,
          0,
          0,
          0.014095306396484375,
          0,
          0,
          0,
          0,
          0,
          0.0014636516571044922,
          0,
          0,
          0,
          0,
          0.0009963512420654297,
          0,
          0,
          0,
          0,
          0.0010156631469726562,
          0,
          0,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0,
          0.00101470947265625,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.00037097930908203125,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.001016855239868164,
          0,
          0,
          0,
          0,
          0.001012563705444336,
          0,
          0,
          0,
          0,
          0.014239788055419922,
          0,
          0,
          0,
          0,
          0,
          0.0014200210571289062,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.014181375503540039,
          0,
          0,
          0,
          0,
          0,
          0.0009942054748535156,
          0,
          0,
          0,
          0,
          0.0012981891632080078,
          0,
          0,
          0,
          0,
          0.0010149478912353516,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.014307260513305664,
          0,
          0,
          0,
          0,
          0,
          0.00039649009704589844,
          0,
          0,
          0,
          0,
          0.0014233589172363281,
          0,
          0,
          0,
          0.014289617538452148,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.0010137557983398438,
          0,
          0,
          0,
          0,
          0.0005502700805664062,
          0,
          0,
          0,
          0,
          0.0014271736145019531,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009942054748535156,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.014288187026977539,
          0,
          0,
          0,
          0,
          0.014176607131958008,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0014128684997558594,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010144710540771484,
          0,
          0,
          0,
          0,
          0.014201879501342773,
          0,
          0,
          0,
          0,
          0.014105081558227539,
          0,
          0,
          0,
          0,
          0,
          0.0013835430145263672,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010166168212890625,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014319896697998047,
          0,
          0,
          0,
          0,
          0,
          0.0014007091522216797,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010132789611816406,
          0,
          0,
          0,
          0,
          0.014217138290405273,
          0,
          0,
          0,
          0,
          0.014311790466308594,
          0,
          0,
          0,
          0,
          0,
          0.000335693359375,
          0,
          0,
          0,
          0,
          0.0015082359313964844,
          0,
          0,
          0,
          0,
          0.0015153884887695312,
          0,
          0,
          0,
          0,
          0.00140380859375,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.014261722564697266,
          0,
          0,
          0,
          0,
          0.014327049255371094,
          0,
          0,
          0,
          0.016324281692504883,
          0.0031671524047851562,
          0,
          0.0020008087158203125,
          0,
          0.006682157516479492,
          0.001798391342163086,
          0,
          0,
          0.011260271072387695,
          0,
          0,
          0,
          0.014162302017211914,
          0,
          0,
          0,
          0,
          0.0019834041595458984,
          0,
          0,
          0,
          0.013596057891845703,
          0.00041031837463378906,
          0,
          0,
          0,
          0,
          0.0014081001281738281,
          0,
          0,
          0,
          0,
          0.0013179779052734375,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.0010120868682861328,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0009949207305908203,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010156631469726562,
          0,
          0,
          0,
          0,
          0.014241695404052734,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013780832290649414,
          0,
          0,
          0,
          0,
          0,
          0.0013768672943115234,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0,
          0,
          0,
          0,
          0.0010154247283935547,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009980201721191406,
          0,
          0,
          0,
          0.014194011688232422,
          0,
          0,
          0,
          0,
          0.01407766342163086,
          0,
          0,
          0,
          0,
          0.0011479854583740234,
          0,
          0,
          0,
          0,
          0.0010123252868652344,
          0,
          0,
          0,
          0,
          0.0010128021240234375,
          0,
          0,
          0,
          0,
          0.014382362365722656,
          0,
          0,
          0,
          0,
          0.013978242874145508,
          0,
          0,
          0,
          0,
          0,
          0.00038886070251464844,
          0,
          0,
          0,
          0,
          0.0013592243194580078,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010118484497070312,
          0,
          0,
          0,
          0,
          0.0010116100311279297,
          0,
          0,
          0,
          0,
          0.014251232147216797,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0004870891571044922,
          0,
          0,
          0,
          0,
          0.0013725757598876953,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.0010113716125488281,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.014309406280517578,
          0,
          0,
          0,
          0,
          0.014238357543945312,
          0,
          0,
          0,
          0,
          0,
          0.0014348030090332031,
          0,
          0,
          0,
          0,
          0.0013353824615478516,
          0,
          0,
          0,
          0,
          0.0009982585906982422,
          0,
          0,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0,
          0.0142669677734375,
          0,
          0,
          0,
          0,
          0,
          0.0004200935363769531,
          0,
          0,
          0,
          0,
          0.001384735107421875,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0.014298677444458008,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.0013573169708251953,
          0,
          0,
          0,
          0,
          0.0014863014221191406,
          0,
          0,
          0,
          0,
          0.0013892650604248047,
          0,
          0,
          0,
          0,
          0.0015132427215576172,
          0,
          0,
          0,
          0,
          0.0015609264373779297,
          0,
          0,
          0,
          0,
          0.000457763671875,
          0,
          0,
          0,
          0.014225959777832031,
          0,
          0,
          0,
          0,
          0,
          0.00040721893310546875,
          0,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0,
          0.0010187625885009766,
          0,
          0,
          0,
          0,
          0.001013040542602539,
          0,
          0,
          0,
          0,
          0.014162063598632812,
          0,
          0,
          0,
          0,
          0.014273405075073242,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.014339208602905273,
          0,
          0,
          0,
          0,
          0.014248371124267578,
          0,
          0,
          0,
          0,
          0.001010894775390625,
          0,
          0,
          0,
          0,
          0.0003695487976074219,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0014185905456542969,
          0,
          0,
          0,
          0,
          0.000990152359008789,
          0,
          0,
          0,
          0,
          0.0014605522155761719,
          0,
          0,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.00099945068359375,
          0,
          0,
          0,
          0,
          0.0013670921325683594,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010142326354980469,
          0,
          0,
          0,
          0,
          0.0014941692352294922,
          0,
          0,
          0,
          0,
          0.0004787445068359375,
          0,
          0,
          0,
          0,
          0.000995635986328125,
          0,
          0,
          0,
          0,
          0.0010139942169189453,
          0,
          0,
          0,
          0,
          0.014334917068481445,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0009949207305908203,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.0010151863098144531,
          0,
          0,
          0,
          0,
          0.014245748519897461,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01425313949584961,
          0,
          0,
          0,
          0,
          0,
          0.001401662826538086,
          0,
          0,
          0,
          0,
          0.001425027847290039,
          0,
          0,
          0,
          0,
          0.0014028549194335938,
          0,
          0,
          0,
          0.006516695022583008,
          0.0004436969757080078,
          0,
          0,
          0,
          0,
          0.0003914833068847656,
          0,
          0,
          0,
          0.014276742935180664,
          0,
          0,
          0,
          0,
          0.0004355907440185547,
          0,
          0,
          0,
          0.0009970664978027344,
          0.00098419189453125,
          0,
          0,
          0.000997781753540039,
          0.0013470649719238281,
          0,
          0,
          0.0010113716125488281,
          0,
          0,
          0,
          0.010246753692626953,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0.0009987354278564453,
          0,
          0,
          0,
          0.000997304916381836,
          0.0009987354278564453,
          0,
          0,
          0,
          0,
          0.000997781753540039,
          0,
          0,
          0.009214162826538086,
          0.0009658336639404297,
          0,
          0,
          0,
          0,
          0.0009996891021728516,
          0.0014126300811767578,
          0,
          0,
          0.0010135173797607422,
          0.0009949207305908203,
          0,
          0,
          0,
          0.0009937286376953125,
          0,
          0,
          0,
          0,
          0.0056149959564208984,
          0.0031931400299072266,
          0.0018281936645507812,
          0,
          0.000997781753540039,
          0,
          0,
          0.0010142326354980469,
          0.0011305809020996094,
          0,
          0,
          0,
          0.0009963512420654297,
          0.0010223388671875,
          0,
          0,
          0,
          0,
          0.0013265609741210938,
          0,
          0,
          0.010252237319946289,
          0.0010280609130859375,
          0.00099945068359375,
          0,
          0,
          0.0010089874267578125,
          0.0010459423065185547,
          0,
          0,
          0,
          0.0010552406311035156,
          0.0009961128234863281,
          0,
          0,
          0,
          0,
          0.0010242462158203125,
          0,
          0,
          0.010203361511230469,
          0.0009784698486328125,
          0.0009942054748535156,
          0,
          0,
          0.0010101795196533203,
          0.0010287761688232422,
          0.00041294097900390625,
          0,
          0,
          0.001009225845336914,
          0.0010194778442382812,
          0,
          0,
          0,
          0.0009751319885253906,
          0,
          0,
          0,
          0.008941650390625,
          0,
          0,
          0,
          0,
          0.0010094642639160156,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009679794311523438,
          0,
          0,
          0,
          0.009865283966064453,
          0,
          0,
          0,
          0.0011208057403564453,
          0,
          0,
          0,
          0,
          0.000993490219116211,
          0,
          0,
          0,
          0.002017498016357422,
          0.0013427734375,
          0,
          0,
          0.0009980201721191406,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009946823120117188,
          0.0010175704956054688,
          0,
          0,
          0.010156869888305664,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0.0010089874267578125,
          0,
          0,
          0,
          0.0009999275207519531,
          0.0010254383087158203,
          0,
          0,
          0,
          0.0010390281677246094,
          0.0010230541229248047,
          0,
          0,
          0,
          0,
          0.0009891986846923828,
          0,
          0,
          0.009199857711791992,
          0.0009989738464355469,
          0.0009999275207519531,
          0,
          0,
          0.0010080337524414062,
          0.0009989738464355469,
          0.002007722854614258,
          0,
          0.001009225845336914,
          0.0010001659393310547,
          0.0020072460174560547,
          0,
          0,
          0.0009975433349609375,
          0,
          0,
          0,
          0,
          0.0009982585906982422,
          0,
          0,
          0,
          0.010323524475097656,
          0.0010449886322021484,
          0.0010020732879638672,
          0,
          0,
          0.0010104179382324219,
          0.0010578632354736328,
          0,
          0,
          0,
          0.0009958744049072266,
          0.0010230541229248047,
          0,
          0,
          0,
          0.0009620189666748047,
          0.0014200210571289062,
          0,
          0,
          0.010164499282836914,
          0.0009720325469970703,
          0.0014255046844482422,
          0,
          0,
          0.010343074798583984,
          0,
          0,
          0,
          0,
          0.0010099411010742188,
          0.0010445117950439453,
          0,
          0,
          0,
          0.000997304916381836,
          0.0010001659393310547,
          0,
          0,
          0.01063084602355957,
          0.0009996891021728516,
          0.0014235973358154297,
          0,
          0,
          0.0010104179382324219,
          0.0010042190551757812,
          0,
          0,
          0,
          0.0010001659393310547,
          0.0009925365447998047,
          0,
          0,
          0.009346723556518555,
          0.0010190010070800781,
          0,
          0,
          0,
          0.0010194778442382812,
          0,
          0,
          0,
          0.010243892669677734,
          0.0010495185852050781,
          0,
          0,
          0.0010135173797607422,
          0,
          0,
          0,
          0.0010178089141845703,
          0.0009636878967285156,
          0,
          0,
          0.0010097026824951172,
          0.0009810924530029297,
          0,
          0,
          0.0010089874267578125,
          0.0009713172912597656,
          0.001047372817993164,
          0.0009741783142089844,
          0,
          0.0009932518005371094,
          0.0009975433349609375,
          0,
          0,
          0.008630514144897461,
          0.0010232925415039062,
          0.00037169456481933594,
          0,
          0,
          0.0010502338409423828,
          0.0010225772857666016,
          0,
          0,
          0,
          0.0009529590606689453,
          0,
          0,
          0,
          0,
          0.0009989738464355469,
          0.0009999275207519531,
          0,
          0,
          0.004368782043457031,
          0.001001119613647461,
          0.0014193058013916016,
          0,
          0,
          0.01024770736694336,
          0,
          0.001018524169921875,
          0.006762027740478516,
          0.003000497817993164,
          0,
          0.001998424530029297,
          0,
          0.0009963512420654297,
          0.0016326904296875,
          0,
          0.0009837150573730469,
          0.0009970664978027344,
          0,
          0,
          0,
          0.0012965202331542969,
          0,
          0,
          0,
          0.0010464191436767578,
          0.00034117698669433594,
          0,
          0,
          0.0010097026824951172,
          0.0009987354278564453,
          0,
          0,
          0,
          0.0009965896606445312,
          0,
          0,
          0,
          0,
          0.000993490219116211,
          0,
          0,
          0,
          0,
          0.0009672641754150391,
          0.0009963512420654297,
          0,
          0,
          0.01020193099975586,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009462833404541016,
          0.0013530254364013672,
          0,
          0,
          0.011090755462646484,
          0.0010004043579101562,
          0.00031495094299316406,
          0,
          0,
          0.0010449886322021484,
          0.001608133316040039,
          0,
          0,
          0.0009992122650146484,
          0,
          0,
          0,
          0.0009984970092773438,
          0,
          0,
          0.010604381561279297,
          0.0010008811950683594,
          0.0014526844024658203,
          0,
          0,
          0.0011992454528808594,
          0.0009992122650146484,
          0,
          0,
          0.002512216567993164,
          0.0010790824890136719,
          0,
          0,
          0,
          0.0010211467742919922,
          0.0010209083557128906,
          0,
          0,
          0,
          0.0010423660278320312,
          0.0010228157043457031,
          0,
          0,
          0.010263442993164062,
          0.0009629726409912109,
          0,
          0,
          0,
          0.0010106563568115234,
          0.0010302066802978516,
          0.0003180503845214844,
          0,
          0,
          0.001008749008178711,
          0.0010411739349365234,
          0.0003032684326171875,
          0,
          0,
          0.0009965896606445312,
          0.0010223388671875,
          0,
          0,
          0,
          0.0010008811950683594,
          0.0009975433349609375,
          0,
          0,
          0,
          0.0009999275207519531,
          0,
          0,
          0,
          0.00915074348449707,
          0.0010182857513427734,
          0,
          0,
          0,
          0.001027822494506836,
          0.0013704299926757812,
          0,
          0,
          0.01018524169921875,
          0.0009772777557373047,
          0.0004980564117431641,
          0,
          0,
          0.001010894775390625,
          0.00099945068359375,
          0,
          0,
          0.0010097026824951172,
          0.0010001659393310547,
          0.0020062923431396484,
          0,
          0.005105733871459961,
          0,
          0.00146484375,
          0,
          0,
          0.0009026527404785156,
          0,
          0,
          0,
          0.000997304916381836,
          0.0004012584686279297,
          0,
          0.009149551391601562,
          0.0010004043579101562,
          0,
          0,
          0.00935220718383789,
          0,
          0,
          0,
          0,
          0.0010223388671875,
          0,
          0,
          0,
          0.0010089874267578125,
          0.001003265380859375,
          0,
          0,
          0.010166168212890625,
          0.0009720325469970703,
          0.0013914108276367188,
          0,
          0,
          0.0010113716125488281,
          0.0009517669677734375,
          0,
          0,
          0.008620500564575195,
          0.0010232925415039062,
          0.0014367103576660156,
          0,
          0,
          0.0009860992431640625,
          0.00099945068359375,
          0,
          0,
          0.010222434997558594,
          0.000997781753540039,
          0,
          0,
          0,
          0.0009989738464355469,
          0.0010478496551513672,
          0,
          0,
          0,
          0.0010008811950683594,
          0.0014641284942626953,
          0,
          0,
          0.0010097026824951172,
          0.0009500980377197266,
          0,
          0,
          0,
          0.001046895980834961,
          0.000995635986328125,
          0,
          0,
          0,
          0.0009918212890625,
          0.0010182857513427734,
          0,
          0,
          0.010133743286132812,
          0.0009748935699462891,
          0.0009999275207519531,
          0,
          0,
          0.001008749008178711,
          0.0010216236114501953,
          0,
          0,
          0,
          0.0019989013671875,
          0.0009984970092773438,
          0,
          0,
          0.009022951126098633,
          0.0009708404541015625,
          0,
          0,
          0,
          0.0011188983917236328,
          0.0010197162628173828,
          0,
          0,
          0,
          0.0009982585906982422,
          0.0009865760803222656,
          0,
          0,
          0,
          0.0009989738464355469,
          0,
          0,
          0,
          0.0010106563568115234,
          0.0010178089141845703,
          0.00036835670471191406,
          0,
          0,
          0.000978231430053711,
          0.0009760856628417969,
          0,
          0,
          0.00924229621887207,
          0.0009996891021728516,
          0,
          0,
          0,
          0.000978231430053711,
          0,
          0,
          0,
          0.0010116100311279297,
          0.0010006427764892578,
          0,
          0,
          0,
          0.0009813308715820312,
          0.0015273094177246094,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0009958744049072266,
          0.0010161399841308594,
          0,
          0,
          0,
          0.0009770393371582031,
          0.0013797283172607422,
          0,
          0,
          0.01023101806640625,
          0.0009982585906982422,
          0.0014538764953613281,
          0,
          0,
          0.002007007598876953,
          0.0009770393371582031,
          0,
          0,
          0.009104490280151367,
          0,
          0.0014171600341796875,
          0,
          0,
          0.0010101795196533203,
          0.0009982585906982422,
          0.0020058155059814453,
          0,
          0,
          0.0009996891021728516,
          0.00046539306640625,
          0,
          0.009259700775146484,
          0.001123189926147461,
          0,
          0,
          0,
          0.0009906291961669922,
          0.0009713172912597656,
          0,
          0,
          0.00925135612487793,
          0.0010519027709960938,
          0,
          0,
          0,
          0,
          0.0013985633850097656,
          0,
          0,
          0.0010099411010742188,
          0,
          0,
          0,
          0.0009961128234863281,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0.0010111331939697266,
          0,
          0,
          0,
          0.012723445892333984,
          0.0009732246398925781,
          0,
          0,
          0,
          0.0009996891021728516,
          0,
          0,
          0,
          0.01268148422241211,
          0.0009989738464355469,
          0,
          0,
          0,
          0.001032114028930664,
          0,
          0,
          0,
          0.012717008590698242,
          0.0009970664978027344,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0.0010173320770263672,
          0,
          0,
          0,
          0,
          0.0009741783142089844,
          0,
          0,
          0,
          0,
          0.000997304916381836,
          0,
          0,
          0,
          0,
          0.0009658336639404297,
          0.0010004043579101562,
          0.000995635986328125,
          0.0010013580322265625,
          0,
          0.0009927749633789062,
          0,
          0,
          0,
          0,
          0.0016832351684570312,
          0,
          0,
          0,
          0,
          0.0007927417755126953,
          0,
          0,
          0,
          0,
          0.000652313232421875,
          0,
          0,
          0,
          0,
          0.0007796287536621094,
          0.0020072460174560547,
          0,
          0.0020599365234375,
          0.0036568641662597656,
          0,
          0.002008199691772461,
          0,
          0,
          0.0011200904846191406,
          0,
          0,
          0,
          0,
          0.0009970664978027344,
          0,
          0,
          0,
          0,
          0.001664876937866211,
          0,
          0,
          0,
          0,
          0.0017528533935546875,
          0,
          0,
          0,
          0.013855457305908203,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.0009999275207519531,
          0,
          0,
          0,
          0,
          0.0009865760803222656,
          0,
          0,
          0,
          0,
          0.0014197826385498047,
          0,
          0,
          0,
          0,
          0.0017461776733398438,
          0,
          0,
          0,
          0.013687372207641602,
          0,
          0,
          0,
          0,
          0.014202594757080078,
          0.0006670951843261719,
          0,
          0,
          0,
          0.01374959945678711,
          0,
          0,
          0,
          0,
          0,
          0.0015034675598144531,
          0,
          0,
          0,
          0,
          0.0009968280792236328,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0011534690856933594,
          0,
          0,
          0,
          0,
          0.0011529922485351562,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01392817497253418,
          0.0005469322204589844,
          0,
          0,
          0,
          0.013915777206420898,
          0.0005548000335693359,
          0,
          0,
          0,
          0,
          0.0010104179382324219,
          0,
          0,
          0,
          0,
          0.0012173652648925781,
          0,
          0,
          0,
          0.013427972793579102,
          0,
          0,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0,
          0.0009679794311523438,
          0,
          0,
          0,
          0,
          0.0009722709655761719,
          0,
          0,
          0,
          0,
          0.0010001659393310547,
          0,
          0,
          0,
          0,
          0.0011174678802490234,
          0,
          0,
          0,
          0.013483285903930664,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.01398324966430664,
          0,
          0,
          0,
          0,
          0.014061689376831055,
          0,
          0,
          0,
          0,
          0.013949871063232422,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0011146068572998047,
          0,
          0,
          0,
          0,
          0.001009225845336914,
          0,
          0,
          0,
          0,
          0.0010008811950683594,
          0,
          0,
          0,
          0,
          0.0009806156158447266,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0011610984802246094,
          0,
          0,
          0,
          0,
          0.0011544227600097656,
          0,
          0,
          0,
          0,
          0.0011560916900634766,
          0,
          0,
          0,
          0,
          0.0011570453643798828,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.013539791107177734,
          0,
          0,
          0,
          0,
          0,
          0.0014545917510986328,
          0,
          0,
          0,
          0,
          0.0005700588226318359,
          0,
          0,
          0,
          0,
          0.0014903545379638672,
          0,
          0,
          0,
          0,
          0.0009930133819580078,
          0,
          0,
          0,
          0,
          0.0010056495666503906,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "height": 500,
        "hovermode": "x unified",
        "legend": {
         "x": 0,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Inference Time Comparison (Interactive)"
        },
        "width": 1400,
        "xaxis": {
         "title": {
          "text": "Batch Index"
         }
        },
        "yaxis": {
         "title": {
          "text": "Inference Time (s)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics of Inference Times:\n",
      "           PyTorch       im2col  im2col Optimized\n",
      "count  2000.000000  2000.000000       2000.000000\n",
      "mean      0.001098     0.000940          0.000837\n",
      "std       0.003252     0.002821          0.002657\n",
      "min       0.000000     0.000000          0.000000\n",
      "25%       0.000000     0.000000          0.000000\n",
      "50%       0.000000     0.000000          0.000000\n",
      "75%       0.000856     0.000999          0.000967\n",
      "max       0.014627     0.015363          0.016324\n",
      "\n",
      "Number of contiguous arrays:\n",
      "im2col: 0, im2col_optimized: 0\n",
      "im2col (not contiguous): 0, im2col_optimized (not contiguous): 0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyoff\n",
    "import pandas as pd\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "dict_times[\"ccm\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "dict_pred[\"ccm\"]=[]\n",
    "\n",
    "c_continuity_ckeck = {\n",
    "    'im2col_yes' : 0,\n",
    "    'im2col_no' : 0,\n",
    "    'im2col_optimized_yes' : 0,\n",
    "    'im2col_optimized_no' : 0,\n",
    "}\n",
    "\n",
    "correct = 0\n",
    "skip = True\n",
    "loop = tqdm(range(0, test_labels.shape[0], 5), desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "\n",
    "    ############### CNN nested loops Implementation #####################\n",
    "    # start_time = time.time()\n",
    "    # c1s,mask1s = nested_loop_convolution(c0.astype(np.float32),np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    # c2s,mask2s = nested_loop_convolution(c1s.astype(np.float32),np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    # c3s,mask3s = nested_loop_convolution(c2s.astype(np.float32),np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    # imlps = c3s.reshape(1,-1)\n",
    "    # _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    # predicted2 = np.argmax(res,1)\n",
    "    # end_time = time.time()\n",
    "    # dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    # dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "\n",
    "    ############### CNN im2col Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = im2col_convolution(c0.astype(np.float32),np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2f,mask2f = im2col_convolution(c1f.astype(np.float32),np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3f,mask3f = im2col_convolution(c2f.astype(np.float32),np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted3 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "\n",
    "    ############## CNN optimized im2col Implementation ###########\n",
    "    start_time = time.time()\n",
    "    c1c,mask1c = im2col_optimized(c0.astype(np.float32),np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2c,mask2c = im2col_optimized(c1c.astype(np.float32),np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3c,mask3c = im2col_optimized(c2c.astype(np.float32),np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpc = c3c.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpc,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted4 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"ccm\"].append(end_time-start_time)\n",
    "    dict_pred[\"ccm\"].append(np.array(predicted4))\n",
    "\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    t = int(predicted1[0])\n",
    "    # s = int(predicted2[0])\n",
    "    f = int(predicted3[0])\n",
    "    c = int(predicted4[0])\n",
    "\n",
    "    if t == s and t == f:\n",
    "        correct += 1\n",
    "\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    tat = round(sum(dict_times['ctorch'])/(i+1),10)\n",
    "    # sat = round(sum(dict_times['cslow'])/(i+1),10)\n",
    "    fat = round(sum(dict_times['cfast'])/(i+1),10)\n",
    "    cat = round(sum(dict_times['ccm'])/(i+1),10)\n",
    "\n",
    "    # loop.set_postfix(average_times =f\"t: {tat} s, s: {sat} s, f: {fat} s, c: {cat} s\" , correct_predictions=f\"{(correct/(i+1)*100)}%\")\n",
    "    loop.set_postfix(average_times =f\"pytorch: {tat} s, im2c: {fat} s, im2c_opt: {cat} s\" , correct_predictions=f\"{100*correct/(i+1)}%\")\n",
    "\n",
    "tat = np.mean(dict_times['ctorch'])\n",
    "# sat = np.mean(dict_times['cslow'])\n",
    "fat = np.mean(dict_times['cfast'])\n",
    "cat = np.mean(dict_times['ccm'])\n",
    "\n",
    "# print(f\"Average inference time in seconds:\\nPyTorch:\\t{tat} s,\\nnested_loops:\\t{sat} s,\\nim2col:\\t\\t{fat} s, \\nim2col_optim:\\t{cat} s\")\n",
    "print(f\"Average inference time in seconds:\\nPyTorch:\\t{tat} s,\\nim2col:\\t\\t{fat} s, \\nim2col_optim:\\t{cat} s\")\n",
    "\n",
    "# Plot times altogether\n",
    "\n",
    "# Create interactive traces for each method\n",
    "trace_torch = go.Scatter(\n",
    "    y=dict_times['ctorch'],\n",
    "    mode='lines+markers',\n",
    "    name='PyTorch',\n",
    "    line=dict(color='blue')\n",
    ")\n",
    "# trace_slow = go.Scatter(\n",
    "#     y=dict_times['cslow'],\n",
    "#     mode='lines+markers',\n",
    "#     name='Nested Loops',\n",
    "#     line=dict(color='orange')\n",
    "# )\n",
    "trace_fast = go.Scatter(\n",
    "    y=dict_times['cfast'],\n",
    "    mode='lines+markers',\n",
    "    name='im2col',\n",
    "    line=dict(color='green')\n",
    ")\n",
    "trace_ccm = go.Scatter(\n",
    "    y=dict_times['ccm'],\n",
    "    mode='lines+markers',\n",
    "    name='im2col Optimized',\n",
    "    line=dict(color='red')\n",
    ")\n",
    "\n",
    "data = [trace_torch, trace_fast, trace_ccm]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Inference Time Comparison (Interactive)',\n",
    "    xaxis=dict(title='Batch Index'),\n",
    "    yaxis=dict(title='Inference Time (s)'),\n",
    "    width=1400,\n",
    "    height=500,\n",
    "    legend=dict(x=0, y=1),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "pyoff.iplot(fig)\n",
    "\n",
    "# Also plot some boxplots for better visualization\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.boxplot([dict_times['ctorch'], dict_times['cfast'], dict_times['ccm']],\n",
    "            tick_labels=['PyTorch', 'im2col', 'im2col Optimized'],\n",
    "            showfliers=False)\n",
    "plt.ylabel('Inference Time (s)')\n",
    "plt.title('Boxplot of Inference Times')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "df_times = pd.DataFrame({\n",
    "    'PyTorch': dict_times['ctorch'],\n",
    "    # 'Nested Loops': dict_times['cslow'],\n",
    "    'im2col': dict_times['cfast'],\n",
    "    'im2col Optimized': dict_times['ccm']\n",
    "})\n",
    "print(\"\\nDescriptive Statistics of Inference Times:\")\n",
    "print(df_times.describe())\n",
    "\n",
    "# Print the number of contiguous arrays\n",
    "print(\"\\nNumber of contiguous arrays:\")\n",
    "print(f\"im2col: {c_continuity_ckeck['im2col_yes']}, im2col_optimized: {c_continuity_ckeck['im2col_optimized_yes']}\")\n",
    "print(f\"im2col (not contiguous): {c_continuity_ckeck['im2col_no']}, im2col_optimized (not contiguous): {c_continuity_ckeck['im2col_optimized_no']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291ecb48",
   "metadata": {},
   "source": [
    "## Comparison of the two im2col approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df65d8",
   "metadata": {},
   "source": [
    "### Why is the optimized version faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b80945",
   "metadata": {},
   "source": [
    "The performance difference between `im2col_convolution` and `im2col_optimized` in your inference code is almost certainly down to **memory layout and how NumPy's highly optimized C/Fortran routines (like those for matrix multiplication) can exploit it.**\n",
    "\n",
    "Let's break it down like a detective story, looking at the clues in your code.\n",
    "\n",
    "**The Core Idea of Im2Col:**\n",
    "\n",
    "Both `im2col_convolution` and `im2col_optimized` use the `im2col` technique. The approach of `im2col` is that it transforms the convolution operation into a *single* matrix multiplication. Matrix multiplication is something libraries like NumPy, which often wrap highly optimized BLAS/LAPACK routines, are *exceptionally* good at.\n",
    "\n",
    "So, both functions do this:\n",
    "1.  Take the input image(s).\n",
    "2.  Extract all the regions the kernel would slide over.\n",
    "3.  Flatten each patch into a column (or row, depending on convention), forming a large matrix, `X_col`.\n",
    "4.  Flatten the kernels into rows (or columns). These form another matrix, `W_col`.\n",
    "5.  Perform `X_col @ W_col`.\n",
    "6.  Reshape the result back into the desired output feature map shape.\n",
    "\n",
    "**The core instruction: `np.lib.stride_tricks.sliding_window_view`**\n",
    "\n",
    "Both functions use `np.lib.stride_tricks.sliding_window_view`. This is a very powerful NumPy tool that creates a *view* of the original array. A \"view\" means no data is copied; instead, it's a new array object that looks at the original data with different strides and shape. This is usually great for efficiency.\n",
    "\n",
    "*   It creates an array where each element is a window (patch) from the input.\n",
    "*   For a batch of images `(BS, IC, IH, IW)` and a kernel `(KH, KW)`, `sliding_window_view` with window shape `(1, IC, KH, KW)` will produce something like `(BS, 1, OH_raw, OW_raw, 1, IC, KH, KW)`. `OH_raw` and `OW_raw` are the number of possible patch positions if stride were 1.\n",
    "\n",
    "**Clue for Efficiency Differences: How Stride is Handled and `X_col` is Formed**\n",
    "\n",
    "Here's where the paths diverge, and where the \"optimization\" happens:\n",
    "\n",
    "1.  **`im2col_convolution`:**\n",
    "    ```python\n",
    "    # After padding batch_of_images:\n",
    "    # BS, IC, IH_padded, IW_padded = batch_of_images.shape\n",
    "    \n",
    "    sliding_windows = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images, # This is the padded image\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )[:,:,::stride,::stride] \n",
    "    # The [:,:,::stride,::stride] slices the output window positions\n",
    "\n",
    "    sliding_windows = sliding_windows.reshape((-1, (kernel_height * kernel_width * input_channels)))\n",
    "    # This is X_col\n",
    "    \n",
    "    kernels_reshaped = kernels.reshape((-1, (kernel_height * kernel_width * input_channels))).transpose(1,0)\n",
    "    # This is W_col\n",
    "    \n",
    "    images_dot_kernels = (sliding_windows @ kernels_reshaped)\n",
    "    ```\n",
    "\n",
    "2.  **`im2col_optimized`:**\n",
    "    ```python\n",
    "    # After padding batch_of_images_padded:\n",
    "    # BS, IC, IH_padded, IW_padded = batch_of_images_padded.shape\n",
    "\n",
    "    patches = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )\n",
    "    \n",
    "    patches_strided = patches[:, :, ::stride, ::stride, :, :, :, :] \n",
    "    # Slices the output window positions, same effective result as above for selection\n",
    "\n",
    "    X_col = patches_strided.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * output_height * output_width,\n",
    "        input_channels * kernel_height * kernel_width\n",
    "    )\n",
    "    # This is X_col, but formed via a complex transpose then reshape\n",
    "\n",
    "    W_col = kernels.reshape(kernels_number, -1).T\n",
    "    # This is W_col\n",
    "    \n",
    "    output_col = X_col @ W_col\n",
    "    ```\n",
    "\n",
    "**Memory Contiguity**\n",
    "\n",
    "The key difference lies in the formation of `X_col` (the matrix of unrolled patches).\n",
    "\n",
    "*   **`im2col_convolution`** takes the `sliding_windows` view (which has already been strided) and does a direct `reshape`.\n",
    "*   **`im2col_optimized`** takes the same set of windows, but then performs a very specific `transpose(0, 2, 3, 5, 6, 7, 1, 4)` operation *before* the `reshape`.\n",
    "\n",
    "**This `transpose` is the secret sauce!**\n",
    "\n",
    "Why does this matter? It's all about **C-contiguity (or F-contiguity)** of the arrays in memory.\n",
    "\n",
    "1.  **NumPy's Matrix Multiplication (`@`) is a Speed Demon:** NumPy doesn't do matrix multiplication in pure Python. It calls highly optimized, compiled C or Fortran libraries (like BLAS - Basic Linear Algebra Subprograms, or LAPACK). These libraries are lightning fast *if the data is laid out in memory in a way they expect*.\n",
    "2.  **C-Contiguous Order:** By default, NumPy arrays are C-contiguous (row-major order). This means elements of a row are stored next to each other in memory.\n",
    "3.  **Views and Reshaping:** `sliding_window_view` creates a view. When you then `reshape` this view, NumPy tries to do it without copying data if possible. However, the resulting reshaped array (our `X_col`) might *not* be C-contiguous.\n",
    "    *   If `X_col` is not C-contiguous, the underlying BLAS routines might:\n",
    "        *   **Perform slower:** They might have to do more complex \"hopping\" in memory to get the elements for dot products.\n",
    "        *   **Make an internal copy:** NumPy might decide it's faster to first make a C-contiguous copy of `X_col` and then pass *that* to BLAS. This copying takes time.\n",
    "4.  **The `transpose` in `im2col_optimized`:**\n",
    "    The specific sequence of axes in `transpose(0, 2, 3, 5, 6, 7, 1, 4)` is carefully chosen. Its goal is to reorder the \"logical\" axes of the `patches_strided` view such that when `reshape` is subsequently called, the resulting `X_col` matrix *is* (or is much more likely to be) **C-contiguous**.\n",
    "    *   Think of it like packing a suitcase. The \"normal\" `reshape` might just stuff things in. The `transpose` followed by `reshape` is like carefully folding and arranging your clothes (the data elements of the patches) so they fit perfectly and sequentially into the rows of your `X_col` suitcase.\n",
    "5.  **Benefits of Contiguity for MatMul:**\n",
    "    *   **Cache Locality:** When data is contiguous, CPU caches are used much more effectively. If the next piece of data needed for a calculation is already in a fast cache line, it's a huge gin in performance.\n",
    "    *   **Vectorization (SIMD):** Modern CPUs can perform the same operation on multiple data points simultaneously (Single Instruction, Multiple Data). This works best when data is laid out sequentially.\n",
    "    *   **BLAS Efficiency:** BLAS routines are designed and compiled to scream on contiguous data blocks.\n",
    "\n",
    "**In Essence:**\n",
    "\n",
    "*   **`im2col_convolution`** creates the `X_col` matrix in a way that might be convenient to write but could result in a memory layout that's suboptimal for the highly tuned matrix multiplication routines. NumPy might be doing hidden work (like copying) or the BLAS routine itself runs slower due to memory access patterns.\n",
    "*   **`im2col_optimized`** invests a bit of upfront effort with that specific `transpose` operation to explicitly arrange the data from the view into a C-contiguous (or nearly so) format *before* `reshape`. This allows the subsequent matrix multiplication `@` to run at its full potential without internal data shuffling or inefficient memory access.\n",
    "\n",
    "The difference can be substantial because:\n",
    "*   **Matrix multiplication is the workhorse:** It's where most of the computation in `im2col` happens. Optimizing it has a big impact.\n",
    "*   **Data size:** For images and typical CNNs, `X_col` can be a very large matrix. The overhead of an internal copy or inefficient strided access on a large matrix adds up significantly.\n",
    "*   **Modern CPU architecture:** Caches and vectorization are critical for performance. Contiguous data is king for these features.\n",
    "\n",
    "**An Analogy:**\n",
    "\n",
    "Imagine you need to read a book (perform matrix multiplication).\n",
    "*   **`im2col_convolution`'s `X_col`:** The pages of the book are scattered randomly around a room. You can still read it, but you spend a lot of time finding the next page.\n",
    "*   **`im2col_optimized`'s `X_col`:** The pages are neatly stacked in order. You can fly through reading it. The `transpose` was the step of collecting and ordering those scattered pages.\n",
    "\n",
    "**How to (Potentially) Verify (Beyond Timing):**\n",
    "\n",
    "You could even verify this by checking the flags of the `X_col` arrays in both functions:\n",
    "```python\n",
    "print(\"Normal X_col C-contiguous:\", sliding_windows.flags.c_contiguous) \n",
    "# in im2col_convolution after reshape\n",
    "\n",
    "print(\"Optimized X_col C-contiguous:\", X_col.flags.c_contiguous) \n",
    "# in im2col_optimized after transpose and reshape\n",
    "```\n",
    "You'd likely find that the `X_col` from `im2col_optimized` is `True` for `c_contiguous`, while the other might be `False`, or it might become `True` only after NumPy implicitly makes a copy during the `@` operation.\n",
    "My strong suspicion is that `X_col` from `im2col_optimized` will report `True` for `c_contiguous` more consistently or achieve it without an internal copy, while the one from `im2col_convolution` might be `False` or might have become `True` only after an implicit copy during the `@` operation if NumPy decided it was necessary.\n",
    "\n",
    "This careful attention to memory layout before a computationally intensive, highly optimized operation is a common theme in high-performance NumPy and scientific computing. The \"optimized\" version is essentially giving NumPy's C backend the data in the exact format it loves most!Ah, this is a fantastic question that gets right to the heart of how NumPy achieves its speed and why seemingly similar code can have vastly different performance! The difference between your `im2col_convolution` and `im2col_optimized` (which I'll call `im2col_conv_v1` and `im2col_conv_v2` respectively for clarity based on your notebook's cell IDs) boils down to **how the intermediate `X_col` matrix (the unrolled image patches) is constructed and its resulting memory layout, which profoundly impacts the efficiency of the subsequent matrix multiplication.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27417901",
   "metadata": {},
   "source": [
    "### Both have contiguously-saved variables for the sliding windows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4099d8fc",
   "metadata": {},
   "source": [
    "It's possible that:\n",
    "\n",
    "1.  **NumPy's `reshape` is smart enough (sometimes):** For the specific shapes and strides involved in your `im2col_convolution` after the `sliding_window_view` and the subsequent striding slice `[:,:,::stride,::stride]`, NumPy's `reshape` might *already* be able to produce a C-contiguous view or, if not, it performs an internal copy so quickly that the `flags.c_contiguous` appears `True` on the *resulting* array of the matrix multiplication's left-hand side operand (if the matmul itself triggers a copy).\n",
    "2.  **The `transpose` in `im2col_optimized` does more than just achieve C-contiguity; it achieves a *better* C-contiguity or a layout more amenable to subsequent operations within the BLAS/LAPACK libraries.**\n",
    "3.  **The `sliding_window_view` itself, especially after striding, might already result in a memory layout that, while technically C-contiguous after `reshape`, has strides that are not optimal for the matmul.**\n",
    "4.  **The difference is not *just* about C-contiguity of `X_col` but also about the interplay with `W_col` and how the BLAS routine handles the pair.**\n",
    "\n",
    "Let's dive deeper into potential reasons why `im2col_optimized` is still faster, even if `X_col.flags.c_contiguous` is `True` for both.\n",
    "\n",
    "**Refined Hypotheses (If `X_col` is C-Contiguous in Both):**\n",
    "\n",
    "*   **Optimal Strides for BLAS:**\n",
    "    Even if an array is C-contiguous, its *strides* (the number of bytes to jump to get to the next element along each dimension) can impact performance. The `transpose` operation in `im2col_optimized` might be arranging the data such that the strides of the resulting `X_col` are \"unit strides\" or very simple multiples along the dimension that's most critical for the inner loops of the matrix multiplication.\n",
    "    *   BLAS routines are often highly optimized for specific stride patterns. If one `X_col` has unit stride in its last dimension (elements of a row are truly one after another with no \"gaps\" from the original memory layout perspective), while the other `X_col` is C-contiguous but its rows were formed by \"picking\" elements with larger jumps from the original image's memory, the unit-stride version will likely be faster.\n",
    "    *   The `transpose(0, 2, 3, 5, 6, 7, 1, 4)` in `im2col_optimized` is very specific. It's reordering the dimensions that correspond to (batch, output_height, output_width, input_channels, kernel_height, kernel_width) *before* flattening the last three (`input_channels * kernel_height * kernel_width`) into the columns of `X_col`. This reordering is likely key to ensuring the \"patch elements\" are laid out perfectly sequentially for each row of `X_col`.\n",
    "\n",
    "*   **Implicit Copies or Temporary Arrays:**\n",
    "    *   Even if `X_col.flags.c_contiguous` is `True` *after* the `reshape` in `im2col_convolution`, it's possible that the `@` operator itself, when faced with the strides of that specific `X_col` (derived from the sliced `sliding_window_view`), decides to create a *temporary, better-layout copy* internally before calling the BLAS routine. This copy would be hidden from your `flags` check on `X_col` itself.\n",
    "    *   The `im2col_optimized` version, by explicitly doing the `transpose`, might be creating the *final, optimal layout* directly, avoiding any such intermediate copies within the `@` operator.\n",
    "\n",
    "*   **Interaction with `W_col` Layout:**\n",
    "    Matrix multiplication performance also depends on the layout of `W_col`. While `W_col` is created similarly in both (`kernels.reshape(...).T`), the way its rows/columns interact with `X_col` in the BLAS routines can be affected by `X_col`'s strides. A more \"natural\" layout for `X_col` (unit strides for rows) might lead to more efficient pairing with `W_col`'s elements.\n",
    "\n",
    "*   **The Nature of `sliding_window_view`'s Strides:**\n",
    "    `sliding_window_view` creates a view with potentially very large strides for some of its new \"window\" dimensions, as it's still pointing to the original contiguous image data. When you then slice this view (for the convolution stride `[:,:,::stride,::stride]`) and then `reshape` it:\n",
    "    *   In `im2col_convolution`: The `reshape` might produce a C-contiguous array, but the elements *within each row* of `X_col` (which represent a flattened patch) might still be accessed using the larger, original image strides internally by the view machinery, even if the overall `X_col` array object says it's C-contiguous.\n",
    "    *   In `im2col_optimized`: The `transpose` operation *before* the `reshape` is crucial. It's likely re-ordering the axes such that the dimensions that will form the *rows* of `X_col` (i.e., the elements of a single patch: `input_channels, kernel_height, kernel_width`) are brought together logically. When `reshape` then flattens these, it's more likely to result in an `X_col` where the elements *within each row* are truly adjacent not just from the `X_col` array's perspective, but also reflecting a more compact access pattern from the original data's perspective, or it forces a copy into this ideal layout.\n",
    "\n",
    "**Let's Think Through the `transpose` in `im2col_optimized`:**\n",
    "\n",
    "The `patches_strided` in `im2col_optimized` has a shape like:\n",
    "`(batch_size, 1, output_height, output_width, 1, input_channels, kernel_height, kernel_width)`\n",
    "(The `1`s are from `sliding_window_view`'s own structure when the window itself doesn't have that dimension).\n",
    "\n",
    "The `transpose(0, 2, 3, 5, 6, 7, 1, 4)` reorders these axes to something like:\n",
    "`(batch_size, output_height, output_width, input_channels, kernel_height, kernel_width, 1, 1)`\n",
    "(Ignoring the exact placement of the singleton dimensions for a moment, the key is `(BS, OH, OW, IC, KH, KW)` are brought to the front).\n",
    "\n",
    "Then `reshape(batch_size * output_height * output_width, input_channels * kernel_height * kernel_width)` is applied.\n",
    "This means:\n",
    "*   The first dimension of `X_col` iterates through `batch_size`, then `output_height`, then `output_width`. This is standard for organizing patches.\n",
    "*   The second dimension of `X_col` (the columns, which are the flattened patch elements) iterates through `input_channels`, then `kernel_height`, then `kernel_width`.\n",
    "\n",
    "This reordering by `transpose` ensures that when NumPy flattens to form the rows of `X_col`, it's picking up `input_channels` data, then all `kernel_height` rows for each channel, then all `kernel_width` columns for each row of the kernel. This is precisely the order you'd want for a C-contiguous flattened patch.\n",
    "\n",
    "**The `im2col_convolution` might be subtly different:**\n",
    "The `sliding_windows = np.lib.stride_tricks.sliding_window_view(... )[:,:,::stride,::stride]` results in a shape like:\n",
    "`(BS, 1, OH, OW, 1, IC, KH, KW)` (after the striding slice, OH and OW are the final output patch counts).\n",
    "Then `sliding_windows.reshape((-1, (kernel_height * kernel_width * input_channels)))`.\n",
    "Without the `transpose`, the `reshape` is flattening based on the *existing order of axes in the view*. If this existing order doesn't perfectly align the `(IC, KH, KW)` elements for C-style row-major layout *within the original memory buffer*, the resulting `X_col` might be C-contiguous at its own level but have less optimal internal strides for the matmul.\n",
    "\n",
    "**The \"Aha!\" Moment - It's About Forcing an Optimal Copy or View**\n",
    "\n",
    "Even if both `X_col` report `c_contiguous`, the `transpose` in the optimized version might be doing one of two things:\n",
    "\n",
    "1.  **Forcing an Actual Copy into Optimal Layout:** The `transpose` operation itself might be complex enough that NumPy decides it *must* make a copy of the data. If it does, it will make that copy into the new transposed layout. Then, `reshape` on this *already optimally laid-out copy* will be trivial and preserve that optimal C-contiguity. This explicit copy, done once, might be faster than implicit copies or inefficient access within the matmul.\n",
    "2.  **Creating a View with Optimal Strides:** If a copy isn't made, the `transpose` followed by `reshape` could result in a view whose *strides* are perfectly suited for the matmul (e.g., unit stride for the innermost dimension of the patch), even if the \"normal\" version's `reshape` also yielded a C-contiguous view but with less ideal strides.\n",
    "\n",
    "**What the `transpose` is likely achieving is ensuring that the elements that make up a single row in `X_col` (i.e., a flattened patch) are accessed with unit stride by the matrix multiplication algorithm.** The \"normal\" `im2col_convolution` might create a C-contiguous `X_col`, but the way its rows are \"made\" from the original strided image view might not guarantee this unit-stride property for the patch elements themselves within the matmul's inner loops.\n",
    "\n",
    "**In summary, even if `X_col.flags.c_contiguous` is `True` for both, the `im2col_optimized` is likely faster due to:**\n",
    "\n",
    "*   **More Optimal Strides:** The `transpose` helps create an `X_col` where the elements *within* each row (the flattened patch) are accessed with unit or near-unit strides by the BLAS routines, leading to better cache utilization and SIMD opportunities.\n",
    "*   **Avoiding Hidden Copies:** The explicit `transpose` might force an upfront, optimally laid-out copy, preventing the matrix multiplication from having to do its own (potentially less efficient or repeated) internal copying or from operating on a C-contiguous array with awkward strides.\n",
    "\n",
    "The specific `transpose` permutation is the key. It's not just *any* transpose; it's one that reorganizes the viewed data from `sliding_window_view` into an order that `reshape` can then turn into a \"perfect\" C-contiguous matrix for the subsequent `@` operation, both in terms of the `flags` and the underlying access pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abe784",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f8f80",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a476a",
   "metadata": {},
   "source": [
    "### NumPy Model Training: Weights Initialization\n",
    "\n",
    "For training our NumPy CNNs from scratch, weights and biases are initialized randomly.\n",
    "The shapes are taken from `numpy_weights` (derived from the PyTorch model) to maintain architectural consistency. `np.random.rand()` provides initial values (uniform in [0,1)). While more advanced initializers exist, this suffices for observing basic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "733935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa5546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382d816",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b10d0",
   "metadata": {},
   "source": [
    "### Training the \"Slow\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This tests training the loop-based `nested_loop_convolution` and `Slow_ReLU_Gradient` implementations on a single image.\n",
    "\n",
    "**Per-Epoch Steps:**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> nested_loop_convolution (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> nested_loop_convolution (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> nested_loop_convolution (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa` (probabilities)\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** Gradients are computed using `ReLU_SoftMax_FC_Backward` for MLP, then `Slow_ReLU_Gradient` is called sequentially for conv layers, propagating gradients backward.\n",
    "4.  **Weight Update:** Parameters updated via $W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W_{old}}$.\n",
    "\n",
    "The loss is plotted to observe learning. The padding and stride parameters in `nested_loop_convolution` calls are set to match the PyTorch model architecture, ensuring the flattened features `imlps` have the correct dimension (2048) for the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:30<00:00,  4.54s/it, avgBackward=3.3668 s, avgForward=1.168 s, pendence=[-0.02896908]] \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlsElEQVR4nO3dC4ylZX0/8Gdmdm477MzOLrddF3ClKBSQWEuJl6Z/hKBIEdpGS0Mt1bYaug1Sa4skBUqoXVFjiIYgkhYwCpYmBZtiNYWiVLkLtNQaBCWwlZuyOzN7nev553nPec+cmZ3LuV8/n+Tdc8573pk9775z9nzneX7P83RlMplMAACok+56/UUAAJHwAQDUlfABANSV8AEA1JXwAQDUlfABANSV8AEA1JXwAQDU1ZrQZObm5sKLL74Y1q1bF7q6uhr9cgCAIsQ5S3fv3h02b94curu7Wyt8xOBx1FFHNfplAABl2LFjR9iyZUtrhY/Y4pG++OHh4Ua/HACgCBMTE0njQfo53lLhI+1qicFD+ACA1lJMyYSCUwCgroQPAKCuhA8AoK6EDwCgroQPAKCuhA8AoK6EDwCgroQPAKCuhA8AoK6EDwCgroQPAKCuhA8AoK6abmG5Wnl5/EC4+fvPhdAVwuVnn9DolwMAHatjWj72Ts2EG+//abjt4Rca/VIAoKN1TPjYsLYvud19YCZMz841+uUAQMfqmPAxPNgburqy93ftm2r0ywGAjtUx4aOnuyusH+xN7o/tm270ywGAjtUx4SMaHcp2vezcq+UDABqlo8JHWvexS/gAgIbpqPCxPhc+dqr5AICG6ajwsWFIzQcANFpHhQ81HwDQeB0VPtR8AEDjdVT4GFXzAQAN15HdLlo+AKBxOrLgdJeCUwBomI7sdtHyAQCN05HhY/fkTJiasbgcADRCR4WPuLhcd25xuTFFpwDQEB0VPpLF5dKuF3UfANAQHRU+otG12aJTE40BQGN0XPjYkA631e0CAA3RceEjv7iclg8AaIiOCx/pFOsKTgGgMToufMwvLqfgFAAaoePCx/wsp1o+AKAROi58qPkAgMbquPCh5gMAGqtzaz6EDwBoiM6d50PBKQA0RMfOcLpnciZMzsw2+uUAQMfpuPAxPFC4uJzWDwCot44LH93dXWE0v7icug8AqLeOCx8LJxoTPgCg3jozfOTqPhSdAkD9dWj4MNwWABqlI8NHOtx2TLcLANRdR4YPE40BQON0ZPhIp1jfpeUDAOquI8PH+lzB6U7zfABA3XV2zYduFwCou44MH+b5AIDG6cjwoeYDABqno+f52Ds1a3E5AKizjgwf6wbWhJ7c6nIWlwOA+urI8JFdXC434kXXCwDUVUeGjyi/sq3wAQB11fHhwyynAFBfnRs+hnIr26r5AIC66tjwkU40ptsFAOqrY8NHvttF+ACAuur48LFLzQcA1FXnhg9TrANAQ3Rs+NiQKzg1yRgA1FfHhg81HwDQGB0bPvKjXdR8AEBddWz4WJ9r+dg3NRsOTFtcDgDqpWPDx7DF5QCgITo2fHR1xcXl1H0AQL11bPgoHPGi7gMA6qejw0da96HlAwCaOHzcf//94dxzzw2bN29Oui7uuuuuBc9nMplw5ZVXhk2bNoXBwcFw5plnhmeeeSY0ow258DGm5QMAmjd87N27N5xyyinh+uuvX/L5z3zmM+ELX/hC+NKXvhQefvjhMDQ0FN797neHAwcOhOad5VTBKQDUy5pSv+Dss89OtqXEVo/rrrsu/PVf/3U477zzkn1f+cpXwhFHHJG0kFxwwQWhmaj5AIAWr/l47rnnwssvv5x0taRGRkbCaaedFh588MElv2ZycjJMTEws2OrFaBcAaPHwEYNHFFs6CsXH6XOLbd++PQko6XbUUUeFerGyLQB04GiXyy+/PIyPj+e3HTt21O3vNsU6ALR4+DjyyCOT21deeWXB/vg4fW6x/v7+MDw8vGCrd8HpLgWnANCa4WPr1q1JyLj33nvz+2INRxz18ra3vS00m9G12YJTNR8A0MSjXfbs2ROeffbZBUWmTz75ZNiwYUM4+uijw6WXXhr+9m//Nhx33HFJGLniiiuSOUHOP//80GzSlo/909nF5QZ6exr9kgCg7ZUcPh577LFw+umn5x9//OMfT24vuuiicMstt4S/+qu/SuYC+chHPhLGxsbCO9/5zvCtb30rDAwMhGazrn9NWNPdFWbmMkndx6aRwUa/JABoe12ZODlHE4ndNHHUSyw+rUf9x6mfuif8fPdkuPuSd4YTN4/U/O8DgHZUyud3w0e7NMsU64pOAaA+Oj58rE+LTg23BYC66Pjwkc71YXE5AKiPjg8f84vLCR8AUA8dHz7maz6EDwCoh44PH/M1HwpOAaAeOj58qPkAgPrq+PCh5gMA6qvjw4eaDwCor44PH6O58GGeDwCoD+FjKFtwemB6Luyfmm30ywGAttfx4eOQ/jWht6cruR8XlwMAaqvjw0dXV9d814u6DwCouY4PH1EaPrR8AEDtCR8FdR+7TDQGADUnfBRMNGa4LQDUnvBRONxW+ACAmhM+Cls+1HwAQM0JH8niclo+AKBehI+k5SNbcDqm4BQAak74UPMBAHUlfKj5AIC6Ej60fABAXQkfySRj2fAxOWNxOQCoNeEjhDDU1xP6erL/FDt1vQBATQkf6eJy6RTrul4AoKaEjxx1HwBQH8JHjpVtAaA+hI8ci8sBQH0IHzlpzcdOs5wCQE0JH4u7XbR8AEBNCR85aj4AoD6EjxxTrANAfQgfi2Y53blXzQcA1JLwkbNBzQcA1IXwkbN+bW6G031TIZPJNPrlAEDbEj4W1Xwki8tNW1wOAGpF+MhZGxeXW5NbXE7XCwDUjPBRsLjcfN2HolMAqBXhY4m6j52G2wJAzQgfS9R9jAkfAFAzwseSc30IHwBQK8JHAXN9AEDtCR8FRtV8AEDNCR9LdLvs2me0CwDUivCx1OJyul0AoGaEjwKjuZoPBacAUDvCxxLhI67vAgDUhvBRYHQoXVxu2uJyAFAjwscSNR9TM3Nh35TF5QCgFoSPAoO9PaHf4nIAUFPCx+LF5fLDbYUPAKgF4WOR9fmiU3N9AEAtCB+LbEiLTnW7AEBNCB+LmOsDAGpL+FhEzQcA1JbwsWzNh/ABALUgfCyyIbey7a69Ck4BoBaEj2VWtlXzAQC1IXwsouYDAGpL+FjEaBcAqC3hY5lulzGLywFATQgfi2zItXxMzc6FvRaXA4CqEz4WGezrCQO92X8Ws5wCQPUJH0tQ9wEAtSN8rBA+jHgBgOoTPpZguC0A1I7wseJEY2Y5BYBqEz5WnGJdywcAVJvwsQSLywFAC4WP2dnZcMUVV4StW7eGwcHBcOyxx4ZrrrmmpSbsUvMBALWzptrf8Nprrw033HBDuPXWW8OJJ54YHnvssfChD30ojIyMhEsuuSS0AovLAUALhY8HHnggnHfeeeGcc85JHr/+9a8Pt99+e3jkkUdCq81yukvBKQA0f7fL29/+9nDvvfeGH//4x8nj//qv/wrf+973wtlnn73k8ZOTk2FiYmLB1mjr04JT3S4A0PwtH5/85CeTAHH88ceHnp6epAbkU5/6VLjwwguXPH779u3h6quvDs1a8xFrVbq6uhr9kgCgbVS95eOOO+4IX/va18Jtt90WHn/88aT243Of+1xyu5TLL788jI+P57cdO3aEZpnhdHo2E/ZMzjT65QBAW6l6y8df/uVfJq0fF1xwQfL45JNPDs8//3zSwnHRRRcddHx/f3+yNdvicoO9PWH/9GxS97FuINsNAwA0YcvHvn37Qnf3wm8bu1/m5uZCKxlV9wEArdHyce655yY1HkcffXQy1PaJJ54In//858OHP/zh0EricNsXxw+EncIHADR3+PjiF7+YTDL2p3/6p+HVV18NmzdvDh/96EfDlVdeGVpJvujUXB8A0NzhY926deG6665LtlaWFp2aaAwAqsvaLstQ8wEAtSF8rDLF+q59ZjkFgGoSPpah5gMAakP4WIaaDwCoDeGjiCnWAYDqET5WXVxOzQcAVJPwUUTNR1xcDgCoDuFjlZqPmblM2G1xOQCoGuFjGQO9PWFtX09y34gXAKge4aOI1g91HwBQPcLHCkaHckWnWj4AoGqEjxWY6wMAqk/4WIG5PgCg+oSPomo+hA8AqBbho6huFwWnAFAtwscKNig4BYCqEz5WMJqr+dip2wUAqkb4KKbmQ8sHAFSN8LECk4wBQPUJH0UOtbW4HABUh/CxgvVrswWns3OZMHHA4nIAUA3CxyqLyw1ZXA4Aqkr4WMV6E40BQFUJH6swxToAVJfwUexcH2Y5BYCqED5WsSFXdKrmAwCqQ/hYhZoPAKgu4WMVaj4AoLqEj6JrPoQPAKgG4WMVG/Lruyg4BYBqED5WMZoWnOp2AYCqED6K7HYRPgCgOoSPogtOp8PcnMXlAKBSwkcJi8vttrgcAFRM+FhF/5qCxeV0vQBAxYSPUobbCh8AUDHho5S6D3N9AEDFhI8ijObm+jDRGABUTvgoginWAaB6hI8SRrzE4bYAQGWEj5KmWNfyAQCVEj6KYHE5AKge4aMIaj4AoHqEjyKo+QCA6hE+imCeDwCoHuGjlILTfVMWlwOACgkfRVifCx8xd0wc0PUCAJUQPorQt6Y7HNK/Jrmv7gMAKiN8FGl0KFt0argtAFRG+CiSicYAoDqEj1InGjPXBwBURPgocWXbMeEDACoifJQYPnbuVXAKAJUQPoq0IVdwquYDACojfBRJzQcAVIfwUeJoFzUfAFAZ4aPEWU7N8wEAlRE+Sl1czgynAFAR4aPEGU5jt8usxeUAoGzCR4lDbZPF5fZr/QCAcgkfRert6Q7r8ovLqfsAgHIJH2UMtxU+AKB8wkc5c32Y5RQAyiZ8lGDDWrOcAkClhI8yik51uwBA+YSPEphiHQAqJ3yUM9GYbhcAKJvwUUa3i4JTACif8FGC0VzBqcXlAKB8wkcJ1HwAQJOGj5/97Gfh93//98PGjRvD4OBgOPnkk8Njjz0WWp2aDwCoXHa+8CratWtXeMc73hFOP/308G//9m/hsMMOC88880wYHR0N7VLzMbZ/Ollcrqe7q9EvCQBaTtXDx7XXXhuOOuqocPPNN+f3bd26NbSD9bmaj0xucbm0GwYAaGC3y7/8y7+EX/3VXw3vf//7w+GHHx7e8pa3hJtuumnZ4ycnJ8PExMSCrakXlxvI5jV1HwDQJOHjpz/9abjhhhvCcccdF7797W+Hiy++OFxyySXh1ltvXfL47du3h5GRkfwWW02amboPAKhMVyYTOxGqp6+vL2n5eOCBB/L7Yvh49NFHw4MPPrhky0fcUrHlIwaQ8fHxMDw8HJrN+dd/Pzy5Yyx8+YNvDWedeGSjXw4ANIX4+R0bEYr5/K56y8emTZvCL//yLy/Yd8IJJ4QXXnhhyeP7+/uTF1m4tUTLh24XAChL1cNHHOny9NNPL9j34x//OBxzzDGhnYpOd+0zyykANEX4+PM///Pw0EMPhb/7u78Lzz77bLjtttvCl7/85bBt27bQDjakK9uq+QCA5ggfp556arjzzjvD7bffHk466aRwzTXXhOuuuy5ceOGFoa1mORU+AKA55vmIfvM3fzPZ2pGaDwCojLVdylxcTs0HAJRH+ChzinU1HwBQHuGjzG4XM5wCQHmEjzILTsdzi8sBAKURPkq0fnB+cbkYQACA0ggfJVrT0x2G08Xl1H0AQMmEjzIYbgsA5RM+ymCiMQAon/BRwRTrY1o+AKBkwkcZ1ufCx869Ck4BoFTCRxk2DKWznGr5AIBSCR9lUPMBAOUTPsqg5gMAyid8VFTzIXwAQKmEj4rm+VBwCgClEj4qKDjV8gEApRM+yjCa63aJa7vMzM41+uUAQEsRPsowkltcLrK4HACURvgoc3G5NICY6wMASiN8VFh0apZTACiN8FGm0bWKTgGgHMJHhUWnJhoDgNIIH5VOsS58AEBJhI9KJxrT7QIAJRE+Kux2UXAKAKURPiqc5VTNBwCURviodHE54QMASiJ8lEnNBwCUR/iouOZD+ACAUggfFbZ8TByYsbgcAJRA+ChTXNulqyt7f8zicgBQNOGjTD3dXfOLy+l6AYCiCR8V2KDuAwBKJnxUYYr1Xft0uwBAsYSPKqxsu8tcHwBQNOGjAobbAkDphI8KmGgMAEonfFSh5sMU6wBQPOGjCjUfYwpOAaBowkcF1HwAQOmEj2rUfOh2AYCiCR/VqPnQ8gEARRM+qjDD6e4DM2Ha4nIAUBThowLDhYvLKToFgKIIHxUuLrc+XVxO3QcAFEX4qJC6DwAojfBRpbqPMS0fAFAU4aNC6/Nzfaj5AIBiCB8V2jCk5gMASiF8VEjNBwCURvioUs2Hlg8AKI7wUaX1XXZp+QCAoggf1ep2MckYABRF+KhWwamWDwAoivBRrW4XNR8AUBTho0rhw+JyAFAc4aMKi8t15xaX0/oBAKsTPqqxuFx+xIuiUwBYjfBRBaNrs0WnJhoDgNUJH1WwITfc1uJyALA64aOai8sJHwCwKuGjmlOs63YBgFUJH1VdXE7BKQCsRvio4iynaj4AYHXCRxWo+QCA4gkfVaDmAwCKJ3xUdWVb4QMAViN8VHOeDwWnALAq4aOKM5zunpwJUzMWlwOAlQgfVTA8ML+4nBEvANDg8PHpT386dHV1hUsvvTS0q+7urjBqxAsAND58PProo+HGG28Mb37zm0OnFJ1a2RYAGhQ+9uzZEy688MJw0003hdHR0dApdR+7tHwAQGPCx7Zt28I555wTzjzzzBWPm5ycDBMTEwu2VpTvdjHXBwCsaE2oga9//evh8ccfT7pdVrN9+/Zw9dVXh3YZbmuiMQCoc8vHjh07wsc+9rHwta99LQwMDKx6/OWXXx7Gx8fzW/z6Vq75eGniQJieNdwWAJbTlclkMqGK7rrrrvBbv/VboaenJ79vdnY2GfHS3d2ddLMUPrdY7HYZGRlJgsjw8HBoFTfd/9PwqW/+aEENyKGH9Ge3dfG2L/e476D9/WuW//cAgFZQyud31btdzjjjjPDUU08t2PehD30oHH/88eGyyy5bMXi0stOPPyzc9sgL4YWd+8LsXCbs2jedbM+8umfVr103sCYclg8kBeHkkP6wMRdWkufX9YW1fTXpKQOAuqn6J9m6devCSSedtGDf0NBQ2Lhx40H728kvHb4u3PeJ/xfmkuAxFX6xJ26Tyfbz3ZMLHr9WcH96NhN2H5hJtp/+Yu+qf8/avp4kkGwcyraaxNvkca5VJdm/LnsbW1/W9JhHDoDm4tfoGkw4FoNA3N4U1q14bOzxmtg/E36eCyLJtiio/DzeT/ZNhsmZubBvajbs27k/7Ni5f9XX0tWVHYWzcahvPqAk9/sXBphccDmkf03SPQYALVXzUalWrfmotXiZ9k7NhteSUDKV3L62d2r+ce5+2qoSZ1ot9cr29XQno3bilg0n8X4aVOb3p/vWCSsANEPNB7URP+Rjy0Tcjtk4tOrx2bqTGEhyAaUgnLy2d3GAmQp74qJ4s3Ph5YkDyVaM3p6uXFjJtpzkg0uudSW9n73tD8ODwgoAwkfb6unuyhethlW6f6L9sVVl72QySVoMJDtzISW9H/fHALMzHrNnKmmFifUqr0xMJlsx1sQ1cGIYWTsfVOI2mgsp+dvYVXRI9rZvjZoVgHYjfJAY7OsJW/rWhi2ja4s6/sD07MKQkgsoafdPPsTkHsewMjOXSYpv41as2LWTBJaltlyIKQwtceRQrLsBoHkJH5RloLcnvG79YLIVG1bSbqB4G0PJclv6/FwmhN2TM8kWhzAX2+ITR/nEIJJsQ73ZgJJ/HANLb1gfg0vu8fCA7iCAehI+qFtY2TQymGzFiEOWJw5MJ60nccr69DYW0ibdQLmAkj4X78eRQLHWJTtaqPhp7mN30Po0sCRBZT6wxNskqOQCSzbE9IbhgV4tLABlEj5oSvGDPX7Yxy0cFopuXRlLJnebDyrxNk72lraoJJO/FbSw7Mt1B5UaWGLuGBnMBpY0uGTDSW8SYOb3zbfCxPsxhAF0OuGDthE/2I8cidvqawotDiwxjIzF1pQ0uOzNhZi0hSXZP50cE+tXYpdQOottKQZ7e5KAkgSVfGtKNqDEMJM+jkFlZDB7P+43WRzQToQPOlo5gWVqZi4JIdnwMbXofrZlJT7O7s/uG9s/nXQJ7Z+eDfvHZ8OL48UNZ07FQtoYSNYPZltQklahpOWlN4yk94eygSV7nNACNC/hA0oUh/8ePjyQbMVKZrM9MHNwaMm1psRwknYZjRfcj9PuR+kU/DvC6jPbrhRaYiDJtrBk9yWP07BScJzuIaCWhA+ogziaJv3gP2Zj8V83MzuXhJa0BWV8fy6w7J8O47kgkw0uaQtL9rbS0BIDVgwk84FlPrykQSXtJkrPK25x5JDWFmA1wgc0sfhBns5rUooYWpIWlFwwSVtTsgEmu+X35x6PF3QPxa6lV3dPJlup4iy8hYEkv+UCy/BSzwku0FGED2hD8UM8XeCwnDWE0paUiXyAyQWW/VNJSMmHmVwLTHwcvy6KU/XH7WdjpbW2rBRc4tT8hcElDnXO3l+Tv6+rCFqH8AEsuYbQltHSvnY6dhGlrSgF21L7sttMVYNL7CpKW1DSoBKDyYLwkn+88Ll1A73JBHVAfQgfQFX0ltnaUkxwiXUvscUlTjwXt+xzM7nnppMVnGNXUanT9xeKgSsGlzSYDBe0qqT7YwHv/L75Y+J+XUZQPOEDaOngEmfD3TM1Mx9UCkLJxOIAs+Bx9vbA9NyCVpdSh0Gnhvp6lgwlaWiJrSuL9yWhJtkXu426TfNPxxA+gJafDTf5wB/oLbmrKIotJruTFpVsgMmGlpn58HIgO3ooe//gY+IsuVHsOorbS2WGlzjN/3xQmQ8vhaElDS7Duf3p8+lz/WvUvdAahA+go8VakXJbXdIuoz0xlCwRWuLjfLApeJyEmdxtfBxnzI3T/KeLK1ZyLguDSbZ+pzCoZJ+f35c+n36dFhjqQfgAqLDLKFmQsMTh0ItHGO1eIpykLS1pSImP888X7E8LdmMrTqnrFC3VAnNwOMk+ToqR0/DSv9S+3vz9+O8CyxE+AJpkhNGmkfK+R5ybJW19iXUraSgpDC3Z/em+g+/H59MWmHLWLVqsf033QQEmDSqxGym9v+C5NMjkQkx8HFtzaD/CB0CLi8OEk0nc1vaW/T0KW2CyQWY+lMSAEvftzgWY5H4aaBbti+sXRZMzc2Fyz2T4xZ7yRh+lYvjIB5N8OJkPLvnHyf0YanrCIQXhJT1mbW9PUh9EcxA+AFjQAhPKbIFJZ9fdOzm7oBVmz+R8K0thC0wywii9zd3fnbtNQ0zsSto5U1ktTCqe21B/TxiKrS3J/YUBZcHj3POxVWbxcYJM5YQPAKomzncysra7olaYfIiZmi0IKAcHlvh4by64pKElH2Jy4Sd+j9gtFaUhJ4TKWmNiPW4MIIWBZagvvZ8NN2l4KdxXeOwh+cDT05GjlIQPAJozxAxmZ62tROxOinO5pMEjDSsxoOydWhhgltsfW3LSwt4YZOKkdunQ6kqDTNTb05UNJ7lQkrbOxPtr+xaGlzifzNCiYJM9Jvt4bV8MM80/Ykn4AKBtxQ/hwb6eZDtsXXnDqVcLMoUhpXBfDDF70n25Vpm4b2/umHSCu+nZTH7hx2qII5YODio9+XCztr8nbBoZDNtO/6Wq/H1lvcaG/c0A0KFBprBrae+iAFMYXgoDzb7c4/R+Gmyy9+fDTByxlC5RsJw3HDYkfABAp1lTpa6lwjCzb3o+zCwMLoWhZjaZVK6RhA8AaJMwMxy3geqEmVoyewsAUFfCBwBQV8IHAFBXwgcAUFfCBwBQV8IHAFBXwgcAUFfCBwBQV8IHAFBXwgcAUFfCBwBQV8IHAFBXwgcA0Nmr2mYymeR2YmKi0S8FAChS+rmdfo63VPjYvXt3cnvUUUc1+qUAAGV8jo+MjKx4TFemmIhSR3Nzc+HFF18M69atC11dXVVPZTHU7NixIwwPD4d21knn2mnn61zbVyedr3NtPzFOxOCxefPm0N3d3VotH/EFb9mypaZ/R7z47fwD0Knn2mnn61zbVyedr3NtL6u1eKQUnAIAdSV8AAB11VHho7+/P1x11VXJbbvrpHPttPN1ru2rk87XuXa2pis4BQDaW0e1fAAAjSd8AAB1JXwAAHUlfAAAddV24eP6668Pr3/968PAwEA47bTTwiOPPLLi8f/0T/8Ujj/++OT4k08+OXzzm98MzW779u3h1FNPTWaBPfzww8P5558fnn766RW/5pZbbklmjC3c4jm3gr/5m7856LXHa9Zu1zWKP7uLzzVu27Zta/nrev/994dzzz03mf0wvs677rprwfOx9v3KK68MmzZtCoODg+HMM88MzzzzTNXf881wvtPT0+Gyyy5LfjaHhoaSY/7gD/4gmd252u+FZri2f/iHf3jQ637Pe97Tktd2tXNd6v0bt89+9rMtd11rqa3Cxz/+4z+Gj3/848mQpscffzyccsop4d3vfnd49dVXlzz+gQceCL/3e78X/uiP/ig88cQTyYd43P7nf/4nNLPvfve7yYfRQw89FP793/89+Y/srLPOCnv37l3x6+LMei+99FJ+e/7550OrOPHEExe89u9973vLHtuq1zV69NFHF5xnvL7R+9///pa/rvHnM74n4wfKUj7zmc+EL3zhC+FLX/pSePjhh5MP5fj+PXDgQNXe881yvvv27Ute7xVXXJHc/vM//3PyC8T73ve+qr4XmuXaRjFsFL7u22+/fcXv2azXdrVzLTzHuP3DP/xDEiZ+53d+p+Wua01l2siv/dqvZbZt25Z/PDs7m9m8eXNm+/btSx7/gQ98IHPOOecs2HfaaadlPvrRj2ZayauvvhqHS2e++93vLnvMzTffnBkZGcm0oquuuipzyimnFH18u1zX6GMf+1jm2GOPzczNzbXVdY0/r3feeWf+cTy/I488MvPZz342v29sbCzT39+fuf3226v2nm+W813KI488khz3/PPPV+290CznetFFF2XOO++8kr5PK1zbYq5rPO93vetdKx5zVQtc12prm5aPqamp8IMf/CBpqi1cJyY+fvDBB5f8mri/8PgoJuvljm9W4+Pjye2GDRtWPG7Pnj3hmGOOSRY4Ou+888IPf/jD0Cpi83ts5nzDG94QLrzwwvDCCy8se2y7XNf4M/3Vr341fPjDH15xkcVWvq6p5557Lrz88ssLrltcIyI2tS933cp5zzf7+zhe5/Xr11ftvdBMvvOd7yTdxG9605vCxRdfHF577bVlj22Xa/vKK6+Eu+++O2mFXc0zLXpdy9U24eMXv/hFmJ2dDUccccSC/fFx/E9tKXF/Kcc3o7gK8KWXXhre8Y53hJNOOmnZ4+IbPjb/feMb30g+0OLXvf3tbw//93//F5pd/ACKtQ3f+ta3wg033JB8UP36r/96snpiu17XKPYlj42NJf3l7XhdC6XXppTrVs57vlnFrqVYAxK7C1daeKzU90KziF0uX/nKV8K9994brr322qTr+Oyzz06uXztf21tvvTWpzfvt3/7tFY87rUWvayWablVbShNrP2Itw2r9g29729uSLRU/oE444YRw4403hmuuuSY0s/ifVOrNb35z8kaNv+nfcccdRf1G0ar+/u//Pjn3+NtQO15XsmLN1gc+8IGk4DZ+8LTje+GCCy7I349FtvG1H3vssUlryBlnnBHaVfzFILZirFYEfnaLXtdKtE3Lx6GHHhp6enqSZq5C8fGRRx655NfE/aUc32z+7M/+LPzrv/5ruO+++8KWLVtK+tre3t7wlre8JTz77LOh1cRm6Te+8Y3LvvZWv65RLBq95557wh//8R93xHVNr00p162c93yzBo94vWNxcanLra/2XmhWsWshXr/lXnc7XNv//M//TIqIS30Pt/J17cjw0dfXF9761rcmzXqp2AQdHxf+Zlgo7i88Por/ASx3fLOIvyHF4HHnnXeG//iP/whbt24t+XvEJs2nnnoqGdbYamKNw09+8pNlX3urXtdCN998c9I/fs4553TEdY0/w/FDpfC6TUxMJKNelrtu5bznmzF4xL7+GDQ3btxY9fdCs4rdgrHmY7nX3erXNm25jOcQR8Z0ynUtSaaNfP3rX0+q42+55ZbM//7v/2Y+8pGPZNavX595+eWXk+c/+MEPZj75yU/mj//+97+fWbNmTeZzn/tc5kc/+lFScdzb25t56qmnMs3s4osvTkY4fOc738m89NJL+W3fvn35Yxaf69VXX5359re/nfnJT36S+cEPfpC54IILMgMDA5kf/vCHmWb3F3/xF8m5Pvfcc8k1O/PMMzOHHnpoMsqnna5rYVX/0UcfnbnssssOeq6Vr+vu3bszTzzxRLLF/3o+//nPJ/fT0R2f/vSnk/frN77xjcx///d/J6MEtm7dmtm/f3/+e8RRA1/84heLfs836/lOTU1l3ve+92W2bNmSefLJJxe8jycnJ5c939XeC814rvG5T3ziE5kHH3wwed333HNP5ld+5Vcyxx13XObAgQMtd21X+zmOxsfHM2vXrs3ccMMNS36Pd7XIda2ltgofUbyg8T/uvr6+ZKjWQw89lH/uN37jN5IhX4XuuOOOzBvf+Mbk+BNPPDFz9913Z5pd/IFfaovDLpc710svvTT/73LEEUdk3vve92Yef/zxTCv43d/93cymTZuS1/66170uefzss8+23XVNxTARr+fTTz990HOtfF3vu+++JX9u0/OJw22vuOKK5Dzih84ZZ5xx0L/BMccck4TJYt/zzXq+8UNmufdx/Lrlzne190Iznmv8peiss87KHHbYYckvAfGc/uRP/uSgENEq13a1n+PoxhtvzAwODibDxZdyTItc11rqin+U1lYCAFC+tqn5AABag/ABANSV8AEA1JXwAQDUlfABANSV8AEA1JXwAQDUlfABANSV8AEA1JXwAQDUlfABANSV8AEAhHr6//PEVAen3JijAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToBeTrained = True\n",
    "if ToBeTrained:\n",
    "    avg_loss = []\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    numEpochs = 20\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    loop = tqdm(range(numEpochs))\n",
    "    for i in loop:\n",
    "        c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "        \n",
    "        # Forward\n",
    "        sfts = time.time() # slow forward time start\n",
    "        c1s,mask1s = nested_loop_convolution(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "        c2s,mask2s = nested_loop_convolution(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "        c3s,mask3s = nested_loop_convolution(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "\n",
    "        imlps = c3s.reshape(1,-1)\n",
    "        fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "        sfte = time.time() # slow forward time end\n",
    "        sft = sfte - sfts\n",
    "        forward_time.append(sft)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa,train_labels[0])\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        sbts = time.time() # slow backward time start\n",
    "        dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "        dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "        gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "\n",
    "        gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "        gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "        sbte = time.time() # slow backward time end\n",
    "        sbt = sbte - sbts\n",
    "        backward_time.append(sbt)\n",
    "\n",
    "        # Weights update\n",
    "        w1 -= lr*dL_dw1\n",
    "        b1 -= lr*dL_db1\n",
    "        w2 -= lr*dL_dw2\n",
    "        b2 -= lr*dL_db2\n",
    "        k3 -= lr*gk3\n",
    "        k2 -= lr*gk2\n",
    "        k1 -= lr*gk1\n",
    "        bc3 -= lr*gb3.reshape(-1)\n",
    "        bc2 -= lr*gb2.reshape(-1)\n",
    "        bc1 -= lr*gb1.reshape(-1)\n",
    "        \n",
    "        if len(avg_loss) >= 2:\n",
    "            loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "    plt.plot(avg_loss)\n",
    "    plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef545289",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 3.6265 s\n",
    "- average backward time : 9.8262 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"images\\Slow Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea8a5",
   "metadata": {},
   "source": [
    "### Test for Fast approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1f5ac",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a336a9",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "756e2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffd8c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d6af1",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fff279",
   "metadata": {},
   "source": [
    "### Training the \"Fast\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This tests training using the Im2Col-based `im2col_convolution` and the revised `Fast_ReLU_Gradient` (from cell `c808bdb6`) on a single image.\n",
    "\n",
    "**Per-Epoch Steps (differences from \"Slow\" are conv/grad functions):**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> im2col_convolution (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> im2col_convolution (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> im2col_convolution (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa`\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** `ReLU_SoftMax_FC_Backward` for MLP, then `Fast_ReLU_Gradient` (using `sliding_window_view` for `gi` and `gk`) for conv layers.\n",
    "4.  **Weight Update:** Standard gradient descent.\n",
    "\n",
    "The loss is plotted. Consistent padding/stride ensures correct feature dimensions for the MLP. This setup tests the learning capability and performance of the more optimized NumPy convolution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5629ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m     13\u001b[0m sfts \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# slow forward time start\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m c1s,mask1s \u001b[38;5;241m=\u001b[39m im2col_convolution(c0\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\u001b[43mk1\u001b[49m,bc1,pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     15\u001b[0m c2s,mask2s \u001b[38;5;241m=\u001b[39m im2col_convolution(c1s\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),k2,bc2,pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     16\u001b[0m c3s,mask3s \u001b[38;5;241m=\u001b[39m im2col_convolution(c2s\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),k3,bc3,pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k1' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = im2col_convolution(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "    c2s,mask2s = im2col_convolution(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = im2col_convolution(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = Fast_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "    gi2,gk2,gb2 = Fast_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "    gi1,gk1,gb1 = Fast_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd84d2",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 0.0022 s\n",
    "- average backward time : 0.0097 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"images\\Fast Approach.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4da756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training PyTorch CNN ---\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Time: 0.30s - Training Loss: 2.2490\n",
      "Epoch 1/3 - Test Loss: 2.0493 - Test Accuracy: 29.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Time: 0.08s - Training Loss: 1.3815\n",
      "Epoch 2/3 - Test Loss: 0.7850 - Test Accuracy: 76.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Time: 0.06s - Training Loss: 0.6333\n",
      "Epoch 3/3 - Test Loss: 0.5631 - Test Accuracy: 82.50%\n",
      "PyTorch Training Complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training NumPy CNN: Im2Col Optimized ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Time: 2.06s - Avg Training Loss: 2.3025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Time: 2.05s - Avg Training Loss: 2.3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Time: 2.05s - Avg Training Loss: 2.3024\n",
      "Im2Col Optimized NumPy Training Complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGJCAYAAABsPPK4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABmc0lEQVR4nO3dB3xN9/8/8Ff2QESQ2BWxV+wIEm2toGqU2qtFKWp16aDorhWjVFtaNWuU2psQI2LVDKkYtSJGSCL7/h/vz/938r2JIIice29ez8fjiHvu5577+Zxz7rnv+1nHymAwGEBERERkoaz1zgARERHRi8Rgh4iIiCwagx0iIiKyaAx2iIiIyKIx2CEiIiKLxmCHiIiILBqDHSIiIrJoDHaIiIjIojHYISIiIovGYIcoh1lZWeGLL76AKbpw4YLK32+//ZZt29y5c6fapvzNSS+//LJaSH99+vRB6dKln+m18lmR84foeTDYIZMlX7hykQsNDc22bZ45cwYffvghatSogXz58qFo0aJo3br1Y99DvqQ7dOiAIkWKwN7eHu7u7mjTpg1WrlyJF+3WrVv44IMPUKFCBTg6OsLNzQ0tWrTA2rVrn2u7ixYtwtSpU7Mtn+ZMvoTlPBs6dOgjA7Xly5frFiTI+2uLi4sLvL29MWnSJCQkJDz39o23/bglpwNVUyH7P2/evHpng7KBbXZshMhc/PLLL/j111/xxhtv4N1330V0dDR++ukn1K9fHxs3bkTTpk3TpR87dizGjx+PcuXK4Z133sFLL72kApD169erbSxcuBDdunV7IXkNCwtDkyZNcPPmTfTt2xd16tTB3bt31XtKsPX+++/jhx9+eOZg58SJExg+fHi69VK+Bw8ewM7OLptKAfj7+6ttSqBoyn7++WeMHj0axYoVgylxcHBQ562Q479ixQp17A8ePIglS5Y817b/+OOPdI/nz5+PLVu2PLS+UqVKz71vU1NTn+m1n332GT7++OPnen8iyI1AiUzRvHnz5Ca1hoMHD2bbNkNDQw33799Pty4qKspQuHBhQ8OGDdOtX7ZsmXr/jh07GhITEx/a1saNGw1r1qx56jzINseOHfvYNPJ+VatWNTg7Oxv279+f7rnk5GRD586d1XaWLFlieBatW7c2vPTSSwZL1rhxY7U8ieyHKlWqGGxtbQ1Dhw5N99yOHTvUfpZzQQ+9e/c25MmTJ926lJQUQ506dVS+rly5kq3vN3jwYLXdJ4mNjTXkBpntfzJPbMYis6xWvnTpEl577TX1/+LFi2PmzJnq+ePHj+PVV19Fnjx5VC2F1GAYq1279kPV0gULFoSfnx9Onz6dbv3nn3+umo3mzp2baU2HNCdJHjSRkZF4++234eHhoZqcpLnh999/f6Zyyq93qXmRX7Q+Pj7pnrOxsVG1Ua6urun6/mhNLkuXLsUnn3yimt1kP7z++uu4fPlyWjrpx7Ju3TpcvHgxrZlC60+RWZ+d593nGfvsaM2TmS0Z+9gsWLBAHTMnJyd1LLp06ZKuLJo5c+bAy8tLpatXrx527979VPtbyt+rVy9VA3H16tVn6n+SWd8SeTxkyBAsW7YMlStXVvnz9fVV+0zIcSxbtqw6X6Tssv+fxNraOm0/Sfp58+ap9zly5MhDab/++mt1vly5cgXPSt6ratWqOHTokKqlc3Z2VueXWL16tWoGltowqYGSYzBhwgSkpKQ8dp9p59nEiRPTjp28vm7duqrGKqv7ddWqVSpv8toqVaqo2tmM5LyTWlHZx/I+ss+zux+QHF/tPC1UqBB69Ojx0D6/fv26qqEtUaKEyq80obdt2zbdMZfmdLmuyDZkW56ennjrrbeyLZ+5GZuxyOzIhbRly5bqwvv999+rZh258MmX7aefforu3burPjazZ89WX2Dy5SIXjceRC5FcYDTnzp1T/XvkQiN9e55EmmnkSyE8PFzlRd5PLoBykZemh2HDhj1VGdesWaP+Sv4zkz9/fnWhlGBK3lO+MDVfffWVupB/9NFHKgCTvjnSPHf06FF1AZV9JM13//33H6ZMmaJe86R+Cdm5z2UbGZtJJPCS5grpD2VcDgk433zzTfTr1081502fPl29Xr7YJdgT0iwpTYwNGjRQzXLnz59XAZ4ERyVLlszyPpdySDPOt99+i2nTpiG7SOD1999/Y/DgwerxN998o4JG6Tv2448/qubUO3fuqP0q59v27dufuM1///03LVDv2LGj2rYck5o1a6ZLJ+vkvJTg9HlI060cfwk25YtcAnotcJVzZ+TIkeqv5H3MmDG4d+9elppYJTC+f/++On5yzso+kPNIjuGTmlL37Nmj+s3J/pPPqBwzaVqWoFz2i5DzJCAgQAUW48aNU+exNEsXLlwY2UX2gQQxEqjJsb1x4wYCAwMRHByc7jyVvJ08eVL1DZPATz6b0mQo+dUeN2/eXOVNfuTI6yQQyom+gbmC3lVLRE/TjCXVyrLu66+/Tlt3584dg5OTk8HKyipds86ZM2ey1GQUFBSkXvv555+nrVu9erV67ZQpU7KU16lTp6r0CxYsSNcU5evra8ibN6/h3r17aeuzkqcaNWoY8ufP/9g0kydPVtv6+++/0zW5FC9ePN37/fnnn2p9YGDgE5uxIiIiVFrZ99m1z7V8yd/MPHjwwFC7dm1DsWLFDNeuXVPrLly4YLCxsTF89dVX6dIeP35cNTdp62Ufu7u7q/2VkJCQlm7OnDnqPbPajCX7Q/Tt29fg6OhouHr16iObsWR/ZLbvpMwZL6ny2MHBQe1XzU8//aTWFylSJN1xGj16tFpvnFZrRrl586ZawsPD1XGQ/V69evW0dF27dlX7T5q4NIcPH37oWD5LM5bsQ1k3e/bsh9LHxcU9tO6dd95Rza/x8fGP3GfaeVawYEHD7du3H/rcGTcPP2q/2tvbq/2hOXbsmFo/ffr0tHVt2rRReTFu7jt37pw6h7Ly9fekZizt/JMmZzmPNWvXrlXbHzNmTNrnRR7/8MMPj9zWX3/9le3N9vQ/bMYisyS/9DXyC0hGK0ktg9QCaGSdPCe/Eh9Ffk1JB2OphZBf2hr5ZSqyUqsjpMOyNBt17do1bZ38Mn3vvfcQExODXbt2PVX55Nfuk95be17Lq0ZqVoxfK7/85Zet5NEU9nlG8stcmnWk6U72oZBfs9KhVbYdFRWVtsjz0ll8x44dadX+cgwHDhyYrgO01KhJ7dfTktql5ORkVbuTXaSTuXETjtYsKb/0jY+Ttj7jvouNjVW/9mWRGjxpQpKas7/++ivdMZfmN22/aLU6UpMn7/O8pNlFai8yku0bn7NyjKRJOC4uTtWMPknnzp1RoECBtMfyWpGV80dqK6VZSlO9enU1Wk17rdTibN26Fe3atUvX6Vz2odRSZQft/JNzWJrJNNK0V7FiRdVcrO0nOT+lSU1q8TKj1QDJSMukpKRsyR/9D4MdMyPVmtIvRL6c5QMkH3YZMZSYmPjY10k1sdanQS6a0gSS8WIk1anyIZU2eWlOkCHPcuHXyBdQs2bN1OvloiIX3E2bNiGnyUUlYzW0fLFJW3jGdnhZ/6iLi3yJSHOCXKSl74FxU46UT8hzWSHNMPIlLP0pMhvFIs8/DfkSfNJ7a89nDIokH8Zkn8gFPiv9QV70Ps9I+k9InxNpnpIRccbNiPIDXsqifdFri/Stki8Y4/2ascwSaJYpU+apyymv6dmzp+pHcu3aNWSHUqVKpXusBWEZm9i09Rn3nex7ae6QJSgoSPVZkiYS4/LJ51ICWglwhASKixcvVp/zrAbsjyPNYJmNppNmmfbt26u8y2dGjo80cwlpKn3afaMFPlk5fzK+Vnu99lo5R6R52biJV5PZumehnX8S5GckwY72vASL3333HTZs2KCaALXmYGk+1zRu3FgFptLcJk3qcuzks5EdUwwQgx2TJe3smU3sJgGKXMjkS0IuNNLnQvpJaB0GH0U6z8kHR74oJECRLxJpH9Y6EspfCXQkaNq7d6/qCyLvL+3vGrnQykVVagiks+Irr7yihkBn1jHyRZIOl0+z/v/Xeqcn5ZS+Af/8848KdKSTY8YLldA6kuY0CZLky0IC0EeRvAvp+GoO+zyjkJAQ1ZdJaowGDBiQ7jk5xyWIkg6n2he98SLn/4sifXckyJcvp8w8qmNrxk652bXvJJ3UYsgiNR8SYGa2LamhlNqx+Ph4VcMjNT1a4PG8jGtwNNIXTb6gjx07pvrBSD8zOTbafsvKUPPnOX+e57V6kP5kZ8+eVf16JICV/mjyOdeun9p8Tvv27VP94aSDs/Thkmu31A7T82GwY2aks50ELRKoyC876Ygpc248qRObfJnIrwmpTq9Vqxa+/PJL9QtR+7W/efNmnDp1So1+kQn3pJpXRlXIiBut1kg6ukpTj3TEk1/SMtJD/mqdac2FXISl2n/btm2qg6RcsDMqX768+rUmgVBWLjQyCklqIzJe4LXaM3n+aWijvKTDbGak6UryJkFZxl+pko+MF3/pxGzclKL3jLTS2Via1+Rc00Z1GZNaSMm31GBqX/TGi1YLpO3XjGWWZoCIiIhnypu8twQJElBlVrsjtQfyRZ/R09beZTc5p+W8kM+j1PBILYuM7HlRpElGOi7LjyIJWuWclWNj3CylJ6mdlqBCzv2MMlv3LLTzT+bEykjWZfzcy7k1atQodb2V0ZZybZUJIo3JuS2d86WJTI6j/Kh93vmUiMGORZAaABl5klXSfCMBk3yRaFXp8muiWrVqaaMshFwo5eIpH7bMyBe7NKU8zXubAhkNIcOzZSSM1O48ilQny8Vcah6Mm/M0csHSZjJu1aqVqpKW7WrkNdI8I81jmQVUjyOBgNTYSN+RjLM7y34fNGiQqq6XJsyMJEAybgKTX4vypW3cT0H62mSlmeFFkBoQGdUjF3qpiciseUSOi/xyl2OQ8Ze6PJbjImRIsXypS+2mcVOufAFnFpA8Td8dCZikqSEj+cKSfafVrAnZv8Z9aPQgfVZkkQkIZb/KPra1fXEDbrWaFePjI8dAPlemQKsRk+HpxtMJSKAjzUnZQc4/Cark/DNubpLtSy261JYL6cMkNW4ZzyNpYtReJ5/njOe6/BgQbMp6fhx6bubkgytfqDJfxZPIRUhqZiTYkVoLqXLWvmjki9o40BHaY+N2ZWPynlLrYdxB1dRJ7ZTsB+lvJH2TpCbLmPQ/kEBA6zwpzVjyK0uqmqXzsTaDsjSvaDVDWs2Z1ARIx1hp4pNaFAkypG+FvOfT9puQ4yKvl86tjRo1SjeDsrzn4cOH1S9E+ULLSIJP7TUyDFbeX2p/+vfvn5ZGqsYlMJMhw1JTJwGZNEnmBPlikCHK0qnYuEOtds5JU6l8EUjto8xoLLWP0slU9qHU1khQIftbajSlb46kkz5pMtePHDNJI8H8s/TZyVi7k9k8SbLPZVi/nCvSAV2+yGbNmqVqA+W46F27I/tFZFcT1qPIUH+pxendu7faD1JbKFMKmFIzksynIz9KGjZsqH4gSKA9Y8YM1WwtUzFkhQS9co5l9jmTjsnSbCefNflBI9cIbei5XANGjBih0krzlXyW5VopP2IkCJXzWNJqn2E51+TaJOeVnH/yg0XmfZK+UPJjip6T0cgs0pEMpZUhjtpibW2thqwar7t48WK61/z3338GLy8vw9tvv52l97h7967h7Nmzhl27dqkhmbVq1UobLtm/f39D8+bNH5olVU6R9evXP7SthQsXqiGdW7ZsMeT00PPMhoLK8FiZBfdxw4q118s2H7UYD/vVbNu2zdC2bVs1xFSGrMpsy7L/ZJissRs3bqihy4UKFVLDYqtVq5bpsN+sDD3XREZGGkaOHGkoW7asOh9cXV0NTZs2TRtubkwbJr148WI1jFnyK8PDpfwZz52YmBhDt27d1PbkNdqw4EcNPX+efZ5x6Lk2lDizJeNQ8RUrVhgaNWqU9hmoWLGiGh4dFhaWLt2PP/5o8PT0VPtIZheW6QSeZgZl4/waD1GW4e+ZzaC8efNmNdxYjnOFChXUlAOPGiIt+TWm7eOMw5AfNcz9aWbwlaH7kufy5csbnsWjhp5ndpxFcHCwoX79+uo8k6HvH374oWHTpk0PTTXwqKHnmQ3Fzvj5yOp+FfIe8l4ZP781a9ZUx0qul7/88oth1KhRaoqBJ3nc9UK2pVm6dKl6Dzn/3NzcDN27d1fXZ+NZ2iW/cv7K8ZRpJXx8fNS0EMZTBcgUAqVKlVLbkc/va6+9pmZ9p+fHYMdE3Lp1S11ctaVevXqG7777Lt26pKSktPQyb0S5cuUMPXv2TDe3RlbJnCQSrCxatEg9ljlmvL2906U5f/68+lDLh9CYfJnKxU3mkiDTofetDUh/MhePBOTjx4/XOysmTX68yA8Iyj3YjGUipErUuO+LjH6QtuDMhkhKL30ZCaWNsMo43Dkr/i/QTWsLlmYdaa6R4ZraLLbSzCVVqMajfWQ4q4wQkA5zWns0EZkG6askTTUyfJ7+Pxl+bjyaTDqzy4hSaX6j3IPBjpmRQEeGpUvfEekzI6NaNNqEbJJG2oelo6rcJ0gm2ZL+GdpU5HKbAOn4KhcArS1YnpOgRi6S2vwP0klTpqGXOSKE9BWRC4S0R8sEaFpfHtnOs0zgRkTZQ/pAyWhK+cEi/Zsyu3dXbiV9t6QvnfyVEXPSv0r6xBlPIkqWj8GOmZHaFumULEvG+Ta0joHSoU6GPUrHSSHDL+X+PNJRVXr8a5NayXw6Wi2OjFyQkUXSiU9qeaSTrgQ2Mn+GRiZakxFGEgBp9/kRki6zOYGIKGfI51Q+z9IRVwYsUPrpOqRGWn6cyQ83ub5p02ZQ7mElbVl6Z4KIiIjoReE8O0RERGTRGOwQERGRRWOfHR3JTLgys6dMlqb39P1ERETmRHrhyOSLclf7J41KZrCjIwl0Mt75mIiIiLJO7vOY2Q1yjTHY0ZF2CwE5UDKfTXaQkVgyPboMJZep9C0By2QeWCbzYGllsrTyCJYpa+TejVJhkJXb8TDY0ZHWdCWBTnYGO3LPJ9meJX1IWCbTxzKZB0srk6WVR7BMTycr3UDYQZmIiIgsGoMdIiIismgMdoiIiMiiMdghIiIii8Zgh4iIiCwagx0iIiKyaAx2LEhKqgEHIm7jUJSV+iuPiYiIcjvOs2MhNp64hnFrTuFadDwAG8w/F4qi+R0xtk1lBFQtqnf2iIiIdMOaHQsJdAYtOPx/gc7/XI+OV+vleSIiotyKwY6Zk6YqqdHJrMFKWyfPs0mLiIhyKwY7Zi4k4vZDNTrGJMSR5yUdERFRbsRgx8xF3o/P1nRERESWhsGOmXPP55ildBdvxb3wvBAREZkiBjtmrp6nmxp19aR7vk7echYD5ofiWvSDHMoZERGRaWCwY+ZsrK3U8HKRMeCx+r+lRRUP2FpbYfOpG2g2OQi/BUewwzIREeUaDHYsgMyjM6tHLRTJn75JSx7L+p961sHa9xqhZilXxCQk44s1p9Bh1l6cunpPtzwTERHlFE4qaEEBT7PKRbAvPBKbdx9Acz8f+JZ1VzU/omIRF6wY2AALD1zE9xvDcOzyXbSZsQf9/DwxvEl5ONnb6F0EIiKiF4I1OxZEAhsfTzfULmRQf7VAR2NtbYWevqWxdVRjtKxaRDVl/bTrPJpP3YVdZ2/qlm8iIqIXicFOLuThIs1btfFLrzoolt8Rl28/QO+5IXhv8RHcvJ+gd/aIiIiyFYOdXKxpZQ9sHtkYbzX0hFQC/X3sKppM2oklIZeQyg7MRERkIRjs5HJ5HWwxpk1lrBrcEFWKueBefDI+XnkcXebsR3jkfb2zR0RE9NwY7JBSvYQrVg9uiM9aV4KTnQ1CLtxGy8Ddan6e+KQUvbNHRET0zBjsUBpbG2v08yuDzSP88UqFwkhKMWDatnNoFbgb+/69pXf2iIiIngmDHXpISTdnzO1TFzO61UThfA44HxWLrj/vxwfLjuFObKLe2SMiInoqDHYoU1ZWVnitejFsHdkY3XxKqXXLDv2HJpN34a8j/8FgYAdmIiIyDwx26LHyO9nh6/bVsHygL8q558Xt2ESMWHoMveaG4OKtWL2zR0RE9EQMdihL6pR2w7r3/PB+8/Kwt7XG7nNRaD4lCDN3hCMpJVXv7BERET0Sgx3KMglyhrxaDpuG+6OBV0EkJKfih01haDN9Dw5fuqN39oiIiDLFYIeemmehPFjYzweTOnmjgLMdzly/jzdm7cXnq07gXnyS3tkjIiJKh8EOPXMH5jdql8C2US/jjVolIP2V/9h/EU0n7cKG49fYgZmIiEwGgx16Lm557DHpTW8s6uejanwi7ydg0MLD6D8/FFfuPtA7e0RERAx2KHs0KFsIG4b5YeirZWFnY4WtpyPRbPIu/LonQt1dnYiISC8MdijbONrZYFTzClj/nh/qvFQAcYkpmLD2FNrNDMaJK9F6Z4+IiHIpBjuU7cp55MOf7/iq+XnyOdri+JVovD5jD75cewqxCcl6Z4+IiHIZBjv0QlhbW6mZl7eNaozXqheFtGT9sidCzc2z/cwNvbNHRES5CIMdeqHc8zliRrdamNe3Loq7OqlOy2/9ForBCw8j8l683tkjIqJcgMEO5YhXKrhjy0h/DPAvAxtrK6w7fk3dZ2vB/otIZQdmIiJ6gRjsUI5xtrfFJ60q4e8hDVG9RH7cj0/GZ6tOoNNP+xB2/b7e2SMiIgvFYIdyXJVi+fHXuw0xtk1l5LG3waGLd9B62m78sOkM4pNS9M4eERFZGAY7pAtpyurb0BNbRjZGs8oeSE41YOaOfxEwNQjB4VF6Z4+IiCwIgx3SVTFXJ/zcqw5m96gNDxcHXLgVh+6/HMDIpUdxKyZB7+wREZEFYLBDJiGgahFVy9PL9yVYWQErj1xB08m7sCz0Mu+zRUREz4XBDpkMF0c7jG9bFSsGNUDFIvlwJy4JHyz/B73mhSKSt9kiIqJnxGCHTE6tUgWwZmgjfBRQEY521tgfcQffHbPBjB3/IjE5Ve/sERGRmWGwQybJzsYag172wubhjdGobEEkG6wQuP1ftJq2Gwcv3NY7e0REZEYY7JBJK1XQGXN71ULPsilwy2OH8MgYdJq9D6NXHkd0XJLe2SMiIjPAYIdMnpWVFeoUNmDTe43QuU5JtW5xyCU1A/OaY1fZgZmIiB6LwQ6ZDVdnO3zXsTqWDqiPMoXzIComAUMXH0Hf3w7i8u04vbNHREQmSvdgZ+bMmShdujQcHR3h4+ODkJCQx6ZftmwZKlasqNJXq1YN69evT/e8/MofM2YMihYtCicnJzRt2hTnzp1Ll+b27dvo3r07XFxc4OrqirfffhsxMTGZvl94eDjy5cun0mV09+5dDB48WL2Xg4MDypcv/1B+KPv5lCmIDcP8MLxpOdjbWGNn2E11N/U5Qf8iOYUdmImIyISCnaVLl2LkyJEYO3YsDh8+DG9vb7Ro0QKRkZGZpt+7dy+6du2qgpMjR46gXbt2ajlx4kRamu+//x7Tpk3D7NmzceDAAeTJk0dtMz7+f3fYlkDn5MmT2LJlC9auXYugoCAMGDDgofdLSkpS7+fn5/fQc4mJiWjWrBkuXLiA5cuXIywsDD///DOKFy+ebfuHHs3B1gbDm5bH+mF+8PF0w4OkFHy9/gxenxGMY5fv6p09IiIyIbZ6vvnkyZPRv39/9O3bVz2WAGXdunWYO3cuPv7444fSBwYGIiAgAB988IF6PGHCBBWwzJgxQ71WanWmTp2Kzz77DG3btlVp5s+fDw8PD6xatQpdunTB6dOnsXHjRhw8eBB16tRRaaZPn45WrVph4sSJKFasWNr7yXakFqlJkyYq0DImeZQaIllvZ2en1kkN1eMkJCSoRXPv3r20oEqW7KBtJ7u2ZwoeV6aXCjjgj761sfzwVXy3KQynrt1D+x+D0cOnFEY0LYu8Drqe4o+U246TuWKZTJ+llUewTE+3zaywMujUu1NqRpydnVWtiNTOaHr37q2ah1avXv3Qa0qVKqVqgoYPH562TmqFJJA5duwYzp8/Dy8vL1XrU6NGjbQ0jRs3Vo8lWJIgZdSoUbhz507a88nJyapZTJrI2rdvr9Zt374d/fr1w9GjR7Fy5Ur1npIvjQRHbm5uqgyS18KFC6Nbt2746KOPYGNjk2mZv/jiC4wbN+6h9YsWLVLboedzPwn464I1DkX9/wpLV3sDOnqmopobOzATEVmauLg49b0bHR2tuqU8jm4/e6OiopCSkqJqXYzJ4zNnzmT6muvXr2eaXtZrz2vrHpfG3d093fO2trYqcNHS3Lp1C3369MGCBQseuQMlsJKASJrEpJ+O9O159913VaQpAVhmRo8erYI145qdkiVLonnz5k88UFkl7y+1XdLEptU4mbunKVNnALvDozD279O4fOcBfgmzQbNK7hjzWkUUcXGEqcjtx8lcsEymz9LKI1imrNFaR7LCNOv4dSZNaxIt+vv7PzJNamqqCprmzJmjanJq166NK1eu4IcffnhksCOdmGXJSA58dp/QL2KbestqmV6tVBS+Xu6Ytv0cfg46jy2nI7Hv/G180KICetR/Sd1x3VTk5uNkTlgm02dp5REs0+M9zXZ066BcqFAhFSTcuHEj3Xp5XKRIkUxfI+sfl177+6Q0GTtASzOW9L/R0kiNjfTfkRofWaRDtFSTyf+lGUzICCwZfWXcZFWpUiVVOyRNdKQvJ3sbdbuJte81Qs1SrohJSMbYv0/ijVl7cfpa1n8NEBGR+dMt2LG3t1e1Idu2bUtXWyKPfX19M32NrDdOL6RaTEvv6empAhbjNFLNJaOytDTyV/reHDp0KC2NBDfy3jL0Xezbt0/11dGW8ePHq+Hn8n+tT0/Dhg1V05W8TnP27FkVBEnZyDRULOKC5QMbYELbKsjnYIujl+/itel78M2G03iQmKJ39oiIyNKHnkv/FRmu/fvvv6tRUoMGDUJsbGza6KxevXqpfi6aYcOGqZFUkyZNUv16pMNvaGgohgwZkjbTrnQk/vLLL/H333/j+PHjahsywkrrBC21LzKiS5qqZE6f4OBg9XoZqaWNxJI0VatWTVtkOLm1tbX6f4ECBVQayavUBkmeJMiRUWRff/21mneHTIs0W/X0LY2toxqjZdUiSEk14Kdd59F86i7sOntT7+wREdELpmufnc6dO+PmzZtqEkBp/pERUxLMaB2ML126pIIMTYMGDdTIJRkS/sknn6BcuXJqJJYEIZoPP/xQBUwyb47U4DRq1EhtU0ZbaRYuXKgCHBlSLtt/44031Nw8T0M6Fm/atAkjRoxA9erVVUAkgY+MxiLT5OHiiFk9amPrqRsYs/oELt9+gN5zQ/C6dzF8/lplFM73cH8qIiIyf7p3UJagQ6uZyWjnzp0PrevUqZNaHkVqd6TZSZZHkZFXEjRllYzMkiUjaRLbv39/lrdDpqFpZQ/U9yqIyZvP4re9Efj72FVVwzO6ZUW8WackrE2oAzMREVnA7SKI9CCTDY5pUxmrBjdElWIuiH6QhI9XHkeXOfsRHnlf7+wREVE2YrBDuVr1Eq5YPbghPmtdCU52Ngi5cBstA3dj8paziE9iB2YiIkvAYIdyPVsba/TzK4PNI/zxSoXCSEoxYNq2c2gVuBv7/r2ld/aIiOg5Mdgh+j8l3Zwxt09dzOhWE4XyOuB8VCy6/rwfHyw7hjuxnDuJiMhcMdghytDB/bXqxbBtVGN08yml1i079B+aTt6FVUeuqJvNEhGReWGwQ5SJ/E52+Lp9NSwf6Ity7nlxKzYRw5ceRa+5Ibh4K1bv7BER0VNgsEP0GHVKu2Hde354v3l52NtaY/e5KDSfEoQfd4YjKeV/s2cTEZHpYrBD9AQS5Ax5tRw2DfdHA6+CSEhOxfcbw9Bm+h4cvnRH7+wREdETMNghyiLPQnmwsJ8PJnXyRgFnO5y5fl/dWPTzVSdwLz5J7+wREdEjMNghesoOzG/ULoFto17GG7VKQPor/7H/IppN3oUNx6+xAzMRkQlisEP0DNzy2GPSm95Y1M8HpQs648a9BAxaeBj954fi6t0HemePiIiMMNgheg4NyhbCxuH+GPpqWdjZWGHr6Ug1TP3XPRHq7upERKQ/BjtEz8nRzgajmldQo7bqvFQAcYkpmLD2FNrNDMaJK9F6Z4+IKNdjsEOUTcp75MOf7/jiq/ZVkc/RFsevROP1GXvw5dpTiE1I1jt7RES5FoMdomxkbW2F7j4vYdvIxmhdvSikJeuXPRFqbp7tZ27onT0iolyJwQ7RC+Du4oiZ3WphXp+6KO7qhCt3H+Ct30IxeOFhRN6L1zt7RES5CoMdohfolYru2DLSH/39PGFjbYV1x6+hyeRdWBRyWdX6EBHRi8dgh+gFc7a3xaetK2P14IaoXiI/7scnY+ya05h20gbnbsTonT0iIovHYIcoh1Qtnh9/vdsQY16rjDz2Noi4b4W2s/Zh4qYwxCel6J09IiKLxWCHKAdJU9ZbjTyx4b2GqFogFUkpBszYEY6AqUEIDo/SO3tERBaJwQ6RDormd0S/CqmY0cUbHi4OuHArDt1/OYCRfx7F7dhEvbNHRGRRGOwQ6cTKCmhRxQNbRjZGL9+X1OOVh6+gyaSdWH7oP95ni4gomzDYIdKZi6MdxretihWDGqBikXy4E5eE95cdUzU9EVGxemePiMjsMdghMhG1ShXAmqGN8FFARTjaWWPvv7fQYmoQpm87h8TkVL2zR0RkthjsEJkQOxtrDHrZC5uHN4ZfuUIqyJm05SxaT9uN0Au39c4eEZFZYrBDZIJKFXTG/LfqIbBLDRTMY49zkTHoOHsfRq88jugHSXpnj4jIrDDYITJRVlZWaFujOLaNaow365RQ6xaHXEKTSbuw5thVdmAmIsoiBjtEJs7V2R7fd/TGkgH1UaZwHkTFJGDo4iN467eDuHw7Tu/sERGZPAY7RGaifpmC2DDMD8OalIO9jTV2hN1Ud1P/Oeg8klPYgZmI6FEY7BCZEQdbG4xoVh7rh/mhnqcbHiSl4Kv1p9F2ZjD++e+u3tkjIjJJDHaIzFBZ97xY0r8+vnujGvI72eHk1XtoNzMY49acRExCst7ZIyIyKQx2iMyUtbUVOtctpTowt61RDKkGYF7wBTSbvAtbTt3QO3tERCaDwQ6RmSuU1wGBXWri97fqoaSbE65Fx6P//FAM/OMQrkfH6509IiLdMdghshCNyxdWkxEObOyl7q6+8eR1NJ28C/P3XUCKVPsQEeVSDHaILIiTvQ0+blkRa4c2Qo2Srqr/zpjVJ/HGrL04fe2e3tkjItIFgx0iC1SpqIu6sej4tlWQ18EWRy/fRZvpe/DthjN4kJiid/aIiHIUgx0iCyVNWb18S2PryMYIqFIEyakGzN71L5pP3YWgszf1zh4RUY5hsENk4Yrkd8TsnrXxc686KJrfEZdvP0CvuSEYtuSImo2ZiMjSMdghyiWaVfbAlpGN0bdhaVhbAauPXlX32Vp68BLvs0VEFo3BDlEuIv13xrapglWDG6JyURd1B/WPVhxH5zn7ER4Zo3f2iIheCAY7RLlQ9RKu+HtIQ3zaqhKc7GwQEnEbrQJ3Y8qWs0hIZgdmIrIsDHaIcilbG2v09y+DzSP88UqFwkhMSUXgtnNoGbgb+8/f0jt7RETZhsEOUS5X0s0Zc/vUxYxuNdVszOdvxqLLnP34aPk/uBuXqHf2iIieG4MdIoKVlRVeq15M3Werm08ptW5p6GU1A/Pqo1fYgZmIzBqDHSJKI3dQ/7p9NSwf6Ity7nkRFZOIYUuOqqHql27F6Z09IqJnwmCHiB5Sp7Qb1r3nh/ebl4e9rTV2n4tSkxHO2vkvklJS9c4eEdFTYbBDRJmSIGfIq+Wwabg/GngVRHxSKr7beEbdduLIpTt6Z4+IKMsY7BDRY3kWyoOF/XwwqZM3Cjjb4cz1++gway/GrD6B+/FJemePiMg8gp2ZM2eidOnScHR0hI+PD0JCQh6bftmyZahYsaJKX61aNaxfvz7d89KZcsyYMShatCicnJzQtGlTnDt3Ll2a27dvo3v37nBxcYGrqyvefvttxMRkPqlaeHg48uXLp9I9ypIlS1Qnz3bt2j1V2YnMgZzbb9QugW2jXkaHWsUh/ZXn77uoOjBvPHGNHZiJyKTpHuwsXboUI0eOxNixY3H48GF4e3ujRYsWiIyMzDT93r170bVrVxWcHDlyRAUXspw4cSItzffff49p06Zh9uzZOHDgAPLkyaO2GR8fn5ZGAp2TJ09iy5YtWLt2LYKCgjBgwICH3i8pKUm9n5+f3yPLcOHCBbz//vuPTUNkCdzy2GPymzVUTU/pgs64cS8BAxccRv/5h3At+n+fLyIiU2KrdwYmT56M/v37o2/fvuqxBCjr1q3D3Llz8fHHHz+UPjAwEAEBAfjggw/U4wkTJqiAZcaMGeq18gtz6tSp+Oyzz9C2bVuVZv78+fDw8MCqVavQpUsXnD59Ghs3bsTBgwdRp04dlWb69Olo1aoVJk6ciGLFiqW9n2xHapGaNGmiAq2MUlJSVOA0btw47N69G3fv3n1kWRMSEtSiuXfvXlpAJUt20LaTXdszBSyT6an3Un6sGeyLH3edx8+7L2Dr6RvY+28UWhSzwqsJljM3j7kfp9xQJksrj2CZsuZptqVrsJOYmIhDhw5h9OjRaeusra1Vs9O+ffsyfY2sl5ogY1JrI4GMiIiIwPXr19U2NPnz51fNY/JaCXbkrzRJaYGOkPTy3lIT1L59e7Vu+/btqsns6NGjWLlyZab5GT9+PNzd3VVNkwQ7j/PNN9+ooCijzZs3w9nZGdlJAkBLwzKZnooA3q8GLD1vg4j7Kfjrgg1Cp+xAF68UlMgDi2Huxyk3lMnSyiNYpseLi4szj2AnKipK1YxIrYsxeXzmzJlMXyOBTGbpZb32vLbucWkkQDFma2sLNze3tDS3bt1Cnz59sGDBAtWvJzN79uzBr7/+qoKhrJCgzjhQk5qdkiVLonnz5o98j2eJdOVkatasGezs7GAJWCbT1zfVgEUhF/H9xjBcjrXC5BN26ONbCu+96gVne90rkJ+ZpR0nSyyTpZVHsExZo7WOZIX5XoVeMGla69atG/z9/TN9/v79++jZsyd+/vlnFCpUKEvbdHBwUEtGcuCz+4R+EdvUG8tk2nrULw2b66ewP6E41p+4gV+DL2LjyUh82a4qXqmY/seFubGk42SpZbK08giW6fGeZju6BjsSJNjY2ODGjRvp1svjIkWKZPoaWf+49NpfWSejsYzT1KhRIy1Nxg7QycnJaoSW9nppwvr7779VHx4hfYFSU1NVDdCcOXNQq1Yt1TG5TZs2aduQ54WkCQsLg5eX13PsHSLzk98eCGznjU517uCzVSdw5e4D9P3tIFpXL4qxbSrDPZ+j3lkkolxI19FY9vb2qF27NrZt25YuYJDHvr6+mb5G1hunF1I1pqX39PRUAYtxGqnqkr44Whr5Kx2Jpb+QRoIbeW/p2yOkX480T2mL9M2R4efyf+nTI52Wjx8/ni7N66+/jldeeUX9X5qniHIrqcnZMtIf/f08YW0FrPvnGppM2oWFBy4iNZXD1IkoZ+nejCV9WHr37q06C9erV0+NpIqNjU0bndWrVy8UL15cde4Vw4YNQ+PGjTFp0iS0bt1azW8TGhqqalu0+UCGDx+OL7/8EuXKlVPBz+eff65GWGlz4FSqVEmN6JKmKhnBJW2JQ4YMUZ2XtZFYksaYvId0YK5atWraOuP/C20enozriXIj6avzaevKaFujOEavPI7jV6Lx6V8n8NfhK/i6QzWU98indxaJKJfQPdjp3Lkzbt68qSYBlM7B0tQkw8K1DsaXLl1SQYamQYMGWLRokRoS/sknn6iARkZiGQcYH374oQqYZN4cqcFp1KiR2qZMQqhZuHChCnBkSLls/4033lBz8xBR9qpaPD9WDW6I3/dewMTNYQi9eAetp+3GO/5eGPJqWTja2eidRSKycLoHO0KCDlkys3PnzofWderUSS2PIrU70uwky6PIyCsJmrJKRmbJ8ji//fZblrdHlJvYWFvhrUaeCKhaBGNWn1Tz8szYEY61/1xVd1lvUDZrnfyJiMxyBmUiyj2KuTrh5161MbtHLXi4OODCrTh0++UARv55FLdjLWcyQiIyLQx2iChHSc1rQNWi2DKyMXr5vgQrK2Dl4StoMmknVhz6j/fZIqJsx2CHiHTh4miH8W2rYsWgBqhYJB/uxCVh1LJj6PHrAURExeqdPSKyIAx2iEhXtUoVwJqhjfBRQEU42FojOPwWWkwNwozt55CY/P/nriIieh4MdohId3Y21hj0shc2j/CHX7lCKsiZuPksXpu+G6EXbuudPSIycwx2iMhkvFQwD+a/VQ9TO9dAwTz2OHsjBh1n78Mnfx1H9APLuQM0EeUsBjtEZHIdmNvVLI5toxrjzTol1LpFBy6h6eRdaqg6OzAT0dNisENEJsnV2R7fd/TGkgH1UaZwHty8n4Ahi47grd8O4r87cXpnj4jMCIMdIjJp9csUxIZhfhjWpBzsbayxI+wmmk0Ows9B55Gcwg7MRPRkDHaIyOQ52NpgRLPyWD/MD/U83fAgKQVfrT+NtjOD8c9/d/XOHhGZOAY7RGQ2yrrnxZL+9fHdG9WQ38kOJ6/eQ7uZwRi35iRiEpL1zh4RmSgGO0RkVqytrdC5bilsHdkYbWsUQ6oBmBd8Ac0n78KWUzf0zh4RmSAGO0Rklgrnc0Bgl5r4/a16KOnmhKvR8eg/PxQD/ziE69HxemePiEwIgx0iMmuNyxfG5uGNMbCxl7q7+saT19Uw9fn7LiBFqn2IKNdjsENEZs/J3gYft6yINUMawbukq+q/M2b1Sbwxay9OX7und/aISGcMdojIYlQu5oKVgxpg3OtVkNfBFkcv30Wb6Xvw7YYzeJCYonf2iEgnDHaIyKJIU1bvBqVVB+YWVTyQnGrA7F3/qpuLBp29qXf2iEgHDHaIyCIVye+In3rWwZyetVE0vyMu3Y5Dr7khGLbkCKJiEvTOHhHlIAY7RGTRmlcpgi0jG6NPg9KwsgJWH72KJpN24c+Dl3mfLaJcgsEOEVk86b/zxetVsOrdhqhc1EXdQf3DFf+gy5z9+PdmjN7ZI6IXjMEOEeUaMlLr7yEN8UmrinCys8GBiNtoOXU3pm49i4RkdmAmslTPFOxcvnwZ//33X9rjkJAQDB8+HHPmzMnOvBERZTtbG2sM8PfC5hH+eLlCYSSmpGLq1nNoFbgbB87fSksnc/RIMHQoykr95Zw9RObL9lle1K1bNwwYMAA9e/bE9evX0axZM1SpUgULFy5Uj8eMGZP9OSUiykYl3Zwxr09drP3nGsatOYV/b8ai85z96FynJOp5FsDEzWdxTc3EbIP550JVJ+exbSojoGpRvbNORDlRs3PixAnUq1dP/f/PP/9E1apVsXfvXhXs/Pbbb8+ySSKiHGdlZYU23sWwbWRjdK1XSq1bGnoZo5b983+Bzv/ILSgGLTiMjSeu6ZRbIsrRYCcpKQkODg7q/1u3bsXrr7+u/l+xYkVcu8YLARGZl/zOdvimQzUsGVAfttZWmabRGrGkFohNWkS5INiRJqvZs2dj9+7d2LJlCwICAtT6q1evomDBgtmdRyKiHCEj0WUSwkc+D6gan5CI2zmaLyLSIdj57rvv8NNPP+Hll19G165d4e3trdb//fffac1bRETmJvJ+fLamIyIz7qAsQU5UVBTu3buHAgUKpK2XTsvOzs7ZmT8iohzjns8xW9MRkRnX7Dx48AAJCQlpgc7FixcxdepUhIWFwd3dPbvzSESUI+p5uqlRV5n32vmfVUf/Q2xCcg7lioh0CXbatm2L+fPnq//fvXsXPj4+mDRpEtq1a4dZs2Y9d6aIiPS6iagMLxcZAx7jx0sP/oeWgbtx6CL77hBZbLBz+PBh+Pn5qf8vX74cHh4eqnZHAqBp06Zldx6JiHKMzKMzq0ctdSNRY/J4do9aWNy/Poq7Oqkbi3aavQ8TN4UhMTlVt/wS0QvqsxMXF4d8+fKp/2/evBkdOnSAtbU16tevr4IeIiJzD3iaVS6CfeGR2Lz7AJr7+cC3rLuq+REbhvvhi9UnsfLIFczYEY6dZyMxtXMNlHX//9dFIrKAmp2yZcti1apV6rYRmzZtQvPmzdX6yMhIuLi4ZHceiYhynAQ2Pp5uqF3IoP5qgY5wcbTD5M41MLNbLbg62+HElXtoPW0P5gVHIJVz8BBZRrAjt4N4//33Ubp0aTXU3NfXN62Wp2bNmtmdRyIik9S6elFsGu4P//KFkZCcqiYc7DU3BNeiH+idNSJ63mCnY8eOuHTpEkJDQ1XNjqZJkyaYMmXKs2ySiMgsebg44ve+dTGhbRU42lljT3gUWkwJwppjV/XOGhE9T7AjihQpompxZNZk7Q7oUssjt4wgIspt99jq6Vsa697zg3eJ/LgXn4yhi49g2JIjiI5L0jt7RLneMwU7qampGD9+PPLnz4+XXnpJLa6urpgwYYJ6jogoN/IqnBfLBzXAsCblVB+f1UevIiAwCMHhUXpnjShXe6bRWJ9++il+/fVXfPvtt2jYsKFat2fPHnzxxReIj4/HV199ld35JCIyC3Y21hjRrDxerlAYI/88hoioWHT/5QDeauiJDwMqwNHORu8sEuU6z1Sz8/vvv+OXX37BoEGDUL16dbW8++67+Pnnn/Hbb79lfy6JiMxMzVIFsO69RujuU0o9nhscgTbT9+DElWi9s0aU6zxTsHP79u1M++bIOnmOiIgAZ3tbfNW+Gub1qYtCeR1wLjIG7X8Mxswd4UjhEHUi0w525C7nM2bMeGi9rJNaHiIi+p9XKrpj8wh/tKjigaQUA37YFIbOP+3DpVtxemeNKFd4pj4733//PVq3bo2tW7emzbGzb98+Ncng+vXrszuPRERmzy2PPWb3qI0Vh6/gi79PIvTiHbQMDMKYNpXxZp2SakQXEZlQzU7jxo1x9uxZtG/fXt0IVBa5ZcTJkyfxxx9/ZH8uiYgsgAQ0HWuXwIZhfqhX2g2xiSn4aMVxDPjjEKJiEvTOHpHFeqaaHVGsWLGHRl0dO3ZMjdKaM2dOduSNiMgilXRzxuIB9fHL7vOYuDkMW07dwJFLd/Bth+poWtlD7+wRWZxnnlSQiIienczD805jL6we3AgVPPIhKiYR/eaHYvTKfxCbkKx39ogsCoMdIiIdVS7mgtVDGmKAfxlIt53FIZfRMnA3Dl3kyFai7MJgh4hIZzLR4CetKmFRv/oo7uqES7fj0Gn2PkzcFIbEZM5KT5SjfXakE/LjSEdlIiJ6Nr5eBbFhuB++WH0SK49cwYwd4dh5NhJTO9dAWfd8emePKHfU7Mi9sB63yD2yevXq9dSZmDlzJkqXLg1HR0f4+PggJCTksemXLVumJjCU9NWqVXtouLvBYMCYMWNQtGhRODk5oWnTpjh37ly6NDL5Yffu3eHi4qLu6/X2228jJiYm0/cLDw9Hvnz5VDpjMmO0n58fChQooBZ5nyflnYjocVwc7TC5cw3M7FYLrs52OHHlHlpP24N5wRFI5USERC++ZmfevHnIbkuXLsXIkSMxe/ZsFehMnToVLVq0QFhYGNzd3R9Kv3fvXnTt2hXffPMNXnvtNSxatAjt2rXD4cOHUbVq1bR5gKZNm6Zua+Hp6YnPP/9cbfPUqVMqQBIS6Fy7dg1btmxBUlIS+vbtiwEDBqjtGZPn5P0kqJH3NrZz5071XIMGDdR2v/vuOzRv3lwNwS9evHi27ysiyj1aVy+KOqUL4IPl/yDo7E2MW3MK205H4odO1VE0v5Pe2SPKHUPPs8vkyZPRv39/FWwICXrWrVuHuXPn4uOPP34ofWBgIAICAvDBBx+ox3KndQlYZPZmea3U6kjA9Nlnn6Ft27Yqzfz58+Hh4YFVq1ahS5cuOH36NDZu3IiDBw+iTp06Ks306dPRqlUrTJw4UQ2r18h2pBapSZMmDwU7CxcuTPdY7he2YsUKbNu2LdMaroSEBLVo7t27lxZQyZIdtO1k1/ZMActkHlim7OfmZINfetTAopDL+HbTWewJj0KLKUEY/3pltK5WxCzLlN0srTyCZcqap9mWrsFOYmIiDh06hNGjR6ets7a2Vs1BMiNzZmS91AQZk1obCWREREQErl+/rrahkSY2qTWS10qwI3+lSUoLdISkl/c+cOCAmixRbN++XTWZHT16FCtXrnxieeLi4tTOd3Nzy/R5qY0aN27cQ+s3b94MZ2dnZCcJAC0Ny2QeWKbsVwDAyCrAgnM2uBSbjOF//oM/th9FR89UONuaZ5mym6WVR7BMT/7ONYtgJyoqCikpKarWxZg8PnPmTKavkUAms/SyXnteW/e4NBmbyGxtbVWQoqW5desW+vTpgwULFqh+PVnx0UcfqVoh40DLmAR1xoGa1OyULFlSNX1l9T2eRIItOZmaNWsGOzs7WAKWyTywTC9ej5RUzNp1Hj/uisChKGtcSXTCdx2qooFXQbMt0/OytPIIlilrtNYRs2jGMlXStNatWzf4+/tnKf23336LJUuWqH48Wr+gjBwcHNSSkRz47D6hX8Q29cYymQeW6UXmAxjVohJerVQEI/88hoioWPT+7RDeauiJDwMqqCHs5lam7GJp5REs0+M9zXZ0nWenUKFCsLGxwY0bN9Ktl8dFimTeHi3rH5de+/ukNJGRkemeT05OViO0tDTShCX9d6TGRxYZrRUdHa3+L/2JjEk6CXakOYp3fSeiF61mqQJY914jdPcppR7PDY5Am+l7cOJKtN5ZIzJJugY79vb2qF27turQq0lNTVWPtbupZyTrjdMLqRrT0svoKwlYjNNIVZf0xdHSyF+ZE0j6C2kkuJH3lr49Qvr1SF8dbRk/frwafi7/1/r0aCO/pJO0dHg27gNERPQiOdvb4qv21TCvT10UyuuAc5ExaP9jMGbuCEcKh6gTmVYzlvRh6d27twoU6tWrp0ZSxcbGpo3OklFNMoxbOveKYcOGqbuuT5o0Ca1bt1ZNR6GhoWk3H5W7Cg8fPhxffvklypUrlzb0XPrSyBB1UalSJTWiS5qqZASXtCUOGTJEdV7WRmJJGmPyHtKBWRveLmSoucznI8PVZZ4grb9P3rx51UJE9KK9UtEdm0f4q3tqbTp5Az9sCsOOM5GY/GYNlCqYvQMfiMyV7reL6Ny5s2oGkqChRo0aquZEakm0DsaXLl1S8+FoZE4bCS4kuPH29sby5cvVSCzjIOTDDz/E0KFD1bw5devWVZMFyjaN+9LIsHFtSLkMOW/UqNFT36191qxZakRZx44d1QSG2iLlISLKKW557DG7R21M7OSNvA62CL14By0Dg7D04CU1HQdRbqd7zY6QWhVZMiMdfjPq1KmTWh5Fanek2UmWR5GRVxknEHwcGZkli7ELFy5k+fVERC+SXPc61i4BH083jPrzGEIu3MZHK45j6+lIfNOhmmrqIsqtdK/ZISKi7FPSzRmLB9TH6JYVYWdjhS2nbiBgahC2nko/aIMoN2GwQ0RkYWysrfBOYy+sHtwIFTzyISomEf3mh6p+PbEJyXpnjyjHMdghIrJQlYu5YPWQhhjgXwZWVsDikMtoM3MfIu7rnTOinMVgh4jIgslEg5+0qoRF/eqjuKsTLt95gMATNpi89RwSk1P1zh5RjmCwQ0SUC/h6FcSG4X5o510UBlhh1q4IdJgVjPBIVvOQ5WOwQ0SUS7g42uGHjtXQp3wKXJ3scOLKPbSetgfzgiOQyokIyYIx2CEiymVqFjRg7RBf+JcvjITkVIxbcwq95obgWvQDvbNG9EIw2CEiyoU8XBzxe9+6mNC2ChztrLEnPAotpgRhzbGremeNKNsx2CEiysUTEfb0LY117/nBu0R+3ItPxtDFRzBsyRFExyXpnT2ibMNgh4gol/MqnBfLBzXAsCbl1Bw9q49eRUBgEILDo/TOGlG2YLBDRESws7HGiGblsXygLzwL5cG16Hh0/+UAxq85hfikFL2zR/RcGOwQEVGamqUKYN17jdDdp5R6PDc4Am2m78GJK9F6Z43omTHYISKidJztbfFV+2qY16euuoHoucgYtP8xGDN3hCOFQ9TJDDHYISKiTL1S0R2bR/ijRRUPJKUY8MOmMHT+aR8u3YrTO2tET4XBDhERPZJbHnvM7lEbEzt5I6+DLUIv3kHLwCAsPXgJBgNrecg8MNghIqInDlHvWLsENgzzQ73SbohNTMFHK45jwB+HEBWToHf2iJ6IwQ4REWVJSTdnLB5QH6NbVoSdjRW2nLqBgKlB2Hrqht5ZI3osBjtERJRlMg/PO429sHpwI1TwyIeomET0mx+K0Sv/QWxCst7ZI8oUgx0iInpqlYu5YPWQhhjgXwZWVsDikMtoGbgbhy7e1jtrRA9hsENERM/E0c4Gn7SqhEX96qO4qxMu3Y5Dp9n7MHFTGBKTU/XOHlEaBjtERPRcfL0KYsNwP3SoWRwyDc+MHeHoMCsY4ZH39c4akcJgh4iInpuLox0md66Bmd1qwdXZDieu3EPraXswLzgCqZyIkHTGYIeIiLJN6+pFsWm4P/zLF0ZCcirGrTmFXnNDcC36gd5Zo1yMwQ4REWUrDxdH/N63Lia0rQJHO2vsCY9CiylBWHPsqt5Zo1yKwQ4REb2QiQh7+pbGuvf84F0iP+7FJ2Po4iMYtuQIouOS9M4e5TIMdoiI6IXxKpwXywc1wLAm5dQcPauPXkVAYBCCw6P0zhrlIgx2iIjohbKzscaIZuWxfKAvPAvlwbXoeHT/5QDGrzmF+KQUvbNHuQCDHSIiyhE1SxXAuvcaobtPKfV4bnAE2kzfgxNXovXOGlk4BjtERJRjnO1t8VX7apjXpy4K5XXAucgYtP8xGDN3hCOFQ9TpBWGwQ0REOe6Viu7YPMIfLap4ICnFgB82haHzT/tw6Vac3lkjC8Rgh4iIdOGWxx6ze9TGxE7eyOtgi9CLd9AyMAhLD16CwcBaHso+DHaIiEjXIeoda5fAhmF+qFfaDbGJKfhoxXEM+OMQomIS9M4eWQgGO0REpLuSbs5YPKA+RresCDsbK2w5dQMBU4Ow9dQNvbNGFoDBDhERmQSZh+edxl5YPbgRKnjkQ1RMIvrND8Xolf8gNiFZ7+yRGWOwQ0REJqVyMResHtIQA/zLwMoKWBxyGS0Dd+PQxdt6Z43MFIMdIiIyOY52NvikVSUs6lcfxV2dcOl2HDrN3oeJm8KQmJyqd/bIzDDYISIik+XrVRAbhvuhQ83ikGl4ZuwIR4dZwQiPvK931siMMNghIiKT5uJoh8mda2Bmt1pwdbbDiSv30HraHswLjkAqJyKkLGCwQ0REZqF19aLYNNwf/uULIyE5FePWnELf+YdwlyPU6QkY7BARkdnwcHHE733rYkLbKnC0s8bef2/j22M2WHf8ut5ZIxPGYIeIiMxuIsKevqWx7j0/VC/uggcpVhj+5z8YtuQIouOS9M4emSAGO0REZJa8CufFkv71EFAiVc3Rs/roVQQEBiE4PErvrJGJYbBDRERmy87GGi1LpmJJv7rwLJQH16Lj0f2XAxi/5hTik1L0zh6ZCAY7RERk9mqUdMW69xqhu08p9XhucATaTN+DE1ei9c4amQAGO0REZBGc7W3xVftqmNenLgrldcC5yBi0/zEYM3eEI4VD1HM1BjtERGRRXqnojs0j/NGiigeSUgz4YVMYOv+0D5duxemdNdIJgx0iIrI4bnnsMbtHbUzs5I28DrYIvXgHLQODsPTgJRgMrOXJbRjsEBGRxQ5R71i7BDYM80O90m6ITUzBRyuOY8AfhxAVw5kIcxOTCHZmzpyJ0qVLw9HRET4+PggJCXls+mXLlqFixYoqfbVq1bB+/fp0z0vUPmbMGBQtWhROTk5o2rQpzp07ly7N7du30b17d7i4uMDV1RVvv/02YmJiMn2/8PBw5MuXT6V72rwQEZG+Sro5Y/GA+hjdsiLsbKyw5dQNBEwNwtZTN/TOGuWWYGfp0qUYOXIkxo4di8OHD8Pb2xstWrRAZGRkpun37t2Lrl27quDkyJEjaNeunVpOnDiRlub777/HtGnTMHv2bBw4cAB58uRR24yPj09LI4HOyZMnsWXLFqxduxZBQUEYMGDAQ++XlJSk3s/Pz++Z8kJERPqTeXjeaeyF1YMboYJHPkTFJKLf/FCMXvkPYhOS9c4evWC20NnkyZPRv39/9O3bVz2WAGXdunWYO3cuPv7444fSBwYGIiAgAB988IF6PGHCBBWwzJgxQ71WanWmTp2Kzz77DG3btlVp5s+fDw8PD6xatQpdunTB6dOnsXHjRhw8eBB16tRRaaZPn45WrVph4sSJKFasWNr7yXak5qZJkyYquHmavGSUkJCgFs29e/fSAipZsoO2nezanilgmcwDy2QeLK1MT1uecoWdsOKdepiyLRxz917E4pDL2HMuChM7VkOtUg/X3uvB0o7RiyrT02xL12AnMTERhw4dwujRo9PWWVtbq2anffv2ZfoaWS81Qcak1kYCGREREYHr16+rbWjy58+vmsfktRLsyF9pktICHSHp5b2lJqh9+/Zq3fbt21Uz1dGjR7Fy5cqnzktG33zzDcaNG/fQ+s2bN8PZ2RnZSYIuS8MymQeWyTxYWpmetjzVAQyuZIWF4da4fOcBuvx8AM2KG9CiRCpsdW/zsMxjlN1liouLM49gJyoqCikpKarWxZg8PnPmTKavkUAms/SyXnteW/e4NO7u7umet7W1hZubW1qaW7duoU+fPliwYIHq1/MseclIgjrj4EhqdkqWLInmzZs/8j2eJdKVk6lZs2aws7ODJWCZzAPLZB4srUzPW5634pMwfu0ZrDp2DZuvWOGKIT8mvlENZd3zQi+WdoxeVJm01hGzaMYyVdK01q1bN/j7+2fbNh0cHNSSkRz47D6hX8Q29cYymQeWyTxYWpmetTxudnaY2rUWmlW5hk9XHcfJq/fRbtZ+fNyyInr7loa1tRX0YmnHKLvL9DTb0bWyrlChQrCxscGNG+l7xMvjIkWKZPoaWf+49NrfJ6XJ2AE6OTlZjdDS0kgTlvTfkRofWaQTcnR0tPq/9CfKSl6IiMg8tK5eFJuG+8O/fGEkJKdi3JpT6DU3BNeiH+idNcoGugY79vb2qF27NrZt25a2LjU1VT329fXN9DWy3ji9kKoxLb2np6cKNozTSFWX9MXR0sjfu3fvqv5CGglu5L2lb4/WH0f66mjL+PHj1fBz+b/Wp+dJeSEiIvPh4eKI3/vWxYS2VeBoZ4094VFoMSUIa45d1Ttr9Jx0b8aSPiy9e/dWnYXr1aunRlLFxsamjc7q1asXihcvrjr3imHDhqFx48aYNGkSWrdujSVLliA0NBRz5sxJm0Rq+PDh+PLLL1GuXDkV/Hz++edqhJUMCxeVKlVSo6ikqUpGTUlb4pAhQ1TnZW0klqQxJu8hHZirVq2atu5JeSEiIvMi3yE9fUujQdlCGLn0KI79F42hi49g6+kbGP96VeR3tqxmpdxC9z7nnTt3Vs1FMglgjRo1VM2JDAvXOv5eunQJ165dS0vfoEEDLFq0SAUUMifP8uXL1egn4yDkww8/xNChQ9W8OXXr1lWTBco2ZeI/zcKFC9OGlMuQ80aNGj11kJKVvBARkfnxKpwXywc1wLAm5dQcPauPXkVAYBCCw6P0zhqZY82OkFoVWTKzc+fOh9Z16tRJLY+LzKXZSZZHkZFXEqhklYzMkuVp80JERObJzsYaI5qVx8sVCmPkn8cQERWL7r8cwFsNPfFhQAU42tnonUUyl5odIiIiU1azVAGse68RuvuUUo/nBkegzfQ9OHElWu+sURYx2CEiInoCZ3tbfNW+Gub1qYtCeR1wLjIG7X8Mxswd4UhJ5V3UTR2DHSIioix6paI7No/wR4sqHkhKMeCHTWHo/NM+XLqV9dl8Kecx2CEiInoKbnnsMbtHbUzs5I28DrYIvXgHLQODsPTgJXV/RjI9DHaIiIiekgyE6Vi7BDYM80O90m6ITUzBRyuOY8AfhxAV878bPpNpYLBDRET0jEq6OWPxgPoY3bIi7GyssOXUDQRMDcLWU+ln1yd9MdghIiJ6DjIPzzuNvbB6cCNU8MiHqJhE9JsfitEr/0FsQrLe2SMGO0RERNmjcjEXrB7SEAP8y8DKClgcchktA3fj0MXbemct12OwQ0RElE1kosFPWlXCon71UdzVCZdux6HT7H2YuCkMicmpemcv12KwQ0RElM18vQpiw3A/dKhZHDINz4wd4egwKxjhkff1zlquxGCHiIjoBXBxtMPkzjUws1stuDrb4cSVe2g9bQ/mBUcglRMR5igGO0RERC9Q6+pFsWm4P/zLF0ZCcirGrTmFXnNDcC36gd5ZyzUY7BAREb1gHi6O+L1vXUxoWwWOdtbYEx6FFlOCsObYVb2zlisw2CEiIsqhiQh7+pbGuvf84F0iP+7FJ2Po4iMYtuQIouOS9M6eRWOwQ0RElIO8CufF8kENMKxJOTVHz+qjVxEQGITg8Ci9s2axGOwQERHlMDsba4xoVh7LB/rCs1AeXIuOR/dfDmD8mlOIT0rRO3sWh8EOERGRTmqWKoB17zVCd59S6vHc4Ai0n7Uf/8XqnTPLwmCHiIhIR872tviqfTXM61MXhfI6IPxmLCYft8HsXeeRwiHq2YLBDhERkQl4paI7No/wR7NK7kgxWGHS1nB0/mkfLt2K0ztrZo/BDhERkYlwy2OPmV290d0rBXkcbBB68Q5aBgZh6cFLMBhYy/OsGOwQERGZ2BD1eu4GrB3cAPVKuyE2MQUfrTiOAX8cQlRMgt7ZM0sMdoiIiExQiQJOWDygPka3rAg7GytsOXUDAVODsPXUDb2zZnYY7BAREZkomYfnncZeWD24ESp45ENUTCL6zQ/F6JX/IDYhWe/smQ0GO0RERCaucjEXrB7SEAP8y8DKClgcchktA3fj0MXbemfNLDDYISIiMgOOdjb4pFUlLOpXH8VdnXDpdhw6zd6HiZvCkJicqnf2TBqDHSIiIjPi61UQG4b7oUPN4pBpeGbsCEeHWcEIj7yvd9ZMFoMdIiIiM+PiaIfJnWtgZrdacHW2w4kr99B62h7MC45AKicifAiDHSIiIjPVunpRbBruD//yhZGQnIpxa06h19wQXIt+oHfWTAqDHSIiIjPm4eKI3/vWxYS2VeBoZ4094VFoMSUIa45d1TtrJoPBDhERkQVMRNjTtzTWvecH7xL5cS8+GUMXH8GwJUcQHZeE3I7BDhERkYXwKpwXywc1wLAm5dQcPauPXkVAYBCCw6OQmzHYISIisiB2NtYY0aw8lg/0hWehPLgWHY/uvxzA+DWnEJ+UgtyIwQ4REZEFqlmqANa91wjdfUqpx3ODI9Bm+h6cuBKN3IbBDhERkYVytrfFV+2rYV6fuiiU1wHnImPQ/sdgzNwRjpRcNESdwQ4REZGFe6WiOzaP8EeLKh5ISjHgh01h6PzTPly6FYfcgMEOERFRLuCWxx6ze9TGxE7eyOtgi9CLd9AyMAhLD16CwWDZtTwMdoiIiHLREPWOtUtgwzA/1CvthtjEFHy04jgG/HEIUTEJsFQMdoiIiHKZkm7OWDygPka3rAg7GytsOXUDAVODsPXUDVgiBjtERES5kI21Fd5p7IXVgxuhgkc+RMUkot/8UIxe+Q9iE5JhSRjsEBER5WKVi7lg9ZCGGOBfBlZWwOKQy2gZuBuHLt6GpWCwQ0RElMs52tngk1aVsKhffRR3dcKl23HoNHsfJm4KQ2JyKswdgx0iIiJSfL0KYsNwP3SoWRwyDc+MHeHoMCsY4ZH3Yc4Y7BAREVEaF0c7TO5cAzO71YKrsx1OXLmH1tP2YF5wBFLNdCJCBjtERET0kNbVi2LTcH/4ly+MhORUjFtzCr3mhuBa9AOYGwY7RERElCkPF0f83rcuJrStAkc7a+wJj0KLKUFYc+wqzAmDHSIiInrsRIQ9fUtj3Xt+8C6RH/fikzF08REMW3IE0XFJMAcMdoiIiOiJvArnxfJBDTCsSTk1R8/qo1cREBiE4PAomDoGO0RERJQldjbWGNGsPJYP9IVnoTy4Fh2P7r8cwPg1pxCflAJTpXuwM3PmTJQuXRqOjo7w8fFBSEjIY9MvW7YMFStWVOmrVauG9evXp3tebmY2ZswYFC1aFE5OTmjatCnOnTuXLs3t27fRvXt3uLi4wNXVFW+//TZiYmLSng8LC8Mrr7wCDw8P9T5lypTBZ599hqSk9NV1U6dORYUKFdT7lCxZEiNGjEB8fHy27BciIiJTVbNUAax7rxG6+5RSj+cGR6DN9D04cSUapkjXYGfp0qUYOXIkxo4di8OHD8Pb2xstWrRAZGRkpun37t2Lrl27quDkyJEjaNeunVpOnDiRlub777/HtGnTMHv2bBw4cAB58uRR2zQOQiTQOXnyJLZs2YK1a9ciKCgIAwYMSHvezs4OvXr1wubNm1XgI0HNzz//rPKpWbRoET7++GO17vTp0/j1119VeT755JMXtr+IiIhMhbO9Lb5qXw3z+tRFobwOOBcZg/Y/BmPmjnCkmNgQdVs933zy5Mno378/+vbtqx5LgLJu3TrMnTtXBRIZBQYGIiAgAB988IF6PGHCBBWwzJgxQ71WanUkMJFamLZt26o08+fPVzU0q1atQpcuXVRgsnHjRhw8eBB16tRRaaZPn45WrVph4sSJKFasmKrJkUXz0ksvYefOndi9e3e6wKthw4bo1q2beiy1UxKISYD1KAkJCWrR3Lt3T/2VGqOMtUbPSttOdm3PFLBM5oFlMg+WViZLK485lqmRVwGsG+KLz1afwpbTkfhhUxi2n76B79+oilJuzirw2f/vTRyKskL+c5Go71VY9fl5Xk+zf3QLdhITE3Ho0CGMHj06bZ21tbVqdtq3b1+mr5H1UhNkTGptJJARERERuH79utqGJn/+/Kp5TF4rwY78laYrLdARkl7eWwKV9u3bP/S+4eHhKkDq0KFD2roGDRpgwYIFqtmtXr16OH/+vGpS69mz5yPL/M0332DcuHEPrZcaJGdnZ2QnCQItDctkHlgm82BpZbK08phjmVrnB9y9rLD8gjUOXbqLVoG7UadwKk7escbdRAlubDD/3FG42hvQoXQqvAs+X+1PXFyc6Qc7UVFRSElJUbUuxuTxmTNnMn2NBDKZpZf12vPauselcXd3T/e8ra0t3Nzc0tIYBzTSvCa1MdLMNX78+LTnpEZHytCoUSNVo5ScnIyBAwc+thlLAjvjYE1qdqSvT/PmzVX/oewgka58QJo1a6aa4ywBy2QeWCbzYGllsrTymHuZWgPod+cBPlhxHKEX7yL4hs1DaaITrTDvrA2md/FGiyrpv6+fhtY6YvLNWKZO+uDcv38fx44dU01n0sz14YcfquekWevrr7/Gjz/+qGqOpPZn2LBhqmnt888/z3R7Dg4OaslITubsPqFfxDb1xjKZB5bJPFhamSytPOZcJk93Oyzq74vaX27B/fjkh56X+hyp5/lqQxhaVi/+zE1aT7NvdAt2ChUqBBsbG9y4cSPdenlcpEiRTF8j6x+XXvsr62Q0lnGaGjVqpKXJ2AFaamVkhFbG95VaF1G5cmVVCyW1O6NGjVL5loBGmqz69eun0sjIsNjYWJXm008/Vc1iREREudGhi3cyDXSMAx4Zth4ScVvdfPRF0+0b2d7eHrVr18a2bdvS1qWmpqrHvr6+mb5G1hunF1LVp6X39PRUAYtxGqnmkr44Whr5e/fuXdVfSLN9+3b13lJD8yjyvFQtyl+trTBjQCNBkJBmLSIiotwq8n58tqZ7Xro2Y0n/ld69e6vOwtLJV0ZSSe2INjpLhn8XL15cdewV0kzUuHFjTJo0Ca1bt8aSJUsQGhqKOXPmpE1pPXz4cHz55ZcoV66cCn6kBkZGWMkQdVGpUiU1oktGgckILglghgwZojovSzqxcOFCVT0mtTXS7CTvIf1tOnfunFZt1qZNGzWarGbNmmnNWPJesl4LeoiIiHIj93yO2ZrOrIMdCR5u3rypJgGUzsHS1CSjnrQOxpcuXUpXeyIdhmV+GxlaLh2BJaCRkVhVq1ZNSyN9arTmJKnBkQ7Esk2ZHFAjwYwEOE2aNFHbf+ONN9TcPMYdlr/77jucPXtW1dLI0HNJL5MGaiQPElzJ3ytXrqBw4cIq0Pnqq69yYM8RERGZrnqebiia3xHXo+NVk1VG0kunSH5HlS4n6N5BWYIIWTIjnYAz6tSpk1oeRQIQGTVlPHIqIxl5JUHT44IwWR5HAiKZUNB4okEiIiKC6nQ8tk1lDFpwWAU2xgGP1h1Zns+O+Xaygr1oiYiIKNsFVC2KWT1qqRocY/JY1svzOUX3mh0iIiKyTAFVi6JZ5SLYFx6JzbsPoLmfD3zLuudYjY6GwQ4RERG9MBLY+Hi64dZpg/qb04GOYDMWERERWTQGO0RERGTRGOwQERGRRWOwQ0RERBaNwQ4RERFZNAY7REREZNE49FxH2g1D5Wal2UXu9SU3KZVtavfxMncsk3lgmcyDpZXJ0sojWKas0b47s3LzbQY7Orp//776W7JkSb2zQkREZLbfpfnz539sGitDVkIieiFSU1Nx9epV5MuXT93TK7siXQmeLl++DBcXF1gClsk8sEzmwdLKZGnlESxT1kj4IoFOsWLF0t00PDOs2dGRHJwSJUq8kG3LyWQpHxINy2QeWCbzYGllsrTyCJbpyZ5Uo6NhB2UiIiKyaAx2iIiIyKIx2LEwDg4OGDt2rPprKVgm88AymQdLK5OllUewTNmPHZSJiIjIorFmh4iIiCwagx0iIiKyaAx2iIiIyKIx2CEiIiKLxmDHxAUFBaFNmzZqhkiZZXnVqlVPfM3OnTtRq1Yt1eu9bNmy+O233x5KM3PmTJQuXRqOjo7w8fFBSEgITLE8K1euRLNmzVC4cGE1EZWvry82bdqULs0XX3yhtmW8VKxYETnlacskxydjfmW5fv26SRyjZylTnz59Mi1TlSpVTOI4ffPNN6hbt66ardzd3R3t2rVDWFjYE1+3bNkylUc5BtWqVcP69evTPS/jO8aMGYOiRYvCyckJTZs2xblz52CqZfr555/h5+eHAgUKqEXym/G8yuxYBgQEwFTLJNe3jPmV42XOx+nll1/O9PPUunVrkzhOs2bNQvXq1dMmCJTr8oYNG0z6s8Rgx8TFxsbC29tbffFlRUREhPpAvPLKKzh69CiGDx+Ofv36pQsQli5dipEjR6phgIcPH1bbb9GiBSIjI2Fq5ZEvXQl25INx6NAhVS75Ej5y5Ei6dPKleu3atbRlz549yClPWyaNXPCM8ywXQlM4Rs9SpsDAwHRlkSnh3dzc0KlTJ5M4Trt27cLgwYOxf/9+bNmyRd2UsHnz5qqcj7J371507doVb7/9tjrf5EtKlhMnTqSl+f777zFt2jTMnj0bBw4cQJ48edRxio+PN8kySaAtZdqxYwf27dunpu+X11y5ciVdOvnSND5OixcvRk54ljIJ+cI1zu/FixfTPW9ux0l+5BmXR845Gxubhz5Peh2nEiVK4Ntvv1XX5NDQULz66qto27YtTp48abqfJRl6TuZBDtdff/312DQffvihoUqVKunWde7c2dCiRYu0x/Xq1TMMHjw47XFKSoqhWLFihm+++cZgauXJTOXKlQ3jxo1Lezx27FiDt7e3wRRkpUw7duxQ6e7cufPINKZyjJ71OEl6Kysrw4ULF0zyOEVGRqpy7dq165Fp3nzzTUPr1q3TrfPx8TG888476v+pqamGIkWKGH744Ye05+/evWtwcHAwLF682GCKZcooOTnZkC9fPsPvv/+etq53796Gtm3bGkxBVso0b948Q/78+R/5vCUcpylTpqjjFBMTY5LHSRQoUMDwyy+/GEz1s8SaHQsjv9ak+s+YRMeyXiQmJqpo3DiN3KNLHmtpTP3mqXLjN6k1MCbVndLkUqZMGXTv3h2XLl2CqatRo4aqspWaq+Dg4LT15n6MxK+//qry+9JLL5nkcYqOjlZ/M55HT/NZklpUaXo0TiP36ZEmRz2OU1bKlFFcXJyqacj4GqkBkprGChUqYNCgQbh16xb0kNUyxcTEqHNNaqoy1jBYwnGSz1OXLl1UbYepHaeUlBQsWbJE1VRJc5apfpYY7FgYOWE8PDzSrZPHcsfZBw8eICoqSp2cmaXJ2GfEFE2cOFFd2N588820dfKBkHb7jRs3qrZk+eBIvwQJikyRBDhSVbtixQq1yAVa2uiluUqY+zG6evWqar+X5lNjpnKcJGCW5t2GDRuiatWqT/1Z0o6B9tcUjlNWy5TRRx99pIJP4y8ZaRqZP38+tm3bhu+++041w7Rs2VKdk6ZYJvminzt3LlavXo0FCxao1zVo0AD//fefRRwn6VMlzT0ZP096H6fjx48jb968qm/owIED8ddff6Fy5com+1niXc/JbCxatAjjxo1TFzXj/i3yAddIpzn5UpVfeX/++adqIzY1cnGWRSMX5n///RdTpkzBH3/8AXP3+++/w9XVVbXJGzOV4yT9J+TLIyf7dZlimaTPhfwil9oB4w69UoOgkY6kcqy8vLxUuiZNmsDUyiS1CcY1CvJ5qlSpEn766SdMmDAB5n6cpFZHjkO9evXSrdf7OFWoUEH1C5WaquXLl6N3794q4HpUwKM31uxYmCJFiuDGjRvp1slj6cAnPdwLFSqkOrpllkZea6rkoiy/bOSLMWN1aEbyRVu+fHmEh4fDXMiFTMuvuR4jIV185Fd2z549YW9vb3LHaciQIVi7dq3qoCudLJ/ls6QdA+2v3sfpacpkXEMqwc7mzZvVl+TjSJOjnJOmepwysrOzQ82aNdPya87HSZqG5NqXlR8DOX2c7O3t1Wjf2rVrqxFnMqBBBiqY6meJwY6FkV84Uq1pTEYAaL985ASVk9M4jVStyuNHtbfqTUYY9O3bV/01Hnr5KNLMJTUl0lxkLuQXkpZfczxGGvllJxfbrFycc/I4SRAmXzZS1b59+3Z4eno+92dJtiEXYuM00lwsI0ly4jg9S5m0US9S4yHNiXXq1HliemkOkr4gpnqcMpJmHGli0fJrrsdJG66dkJCAHj16mNRxyoxcoySvJvtZypZuzvTC3L9/33DkyBG1yOGaPHmy+v/FixfV8x9//LGhZ8+eaenPnz9vcHZ2NnzwwQeG06dPG2bOnGmwsbExbNy4MS3NkiVLVC/33377zXDq1CnDgAEDDK6urobr16+bXHkWLlxosLW1VeW4du1a2iI99TWjRo0y7Ny50xAREWEIDg42NG3a1FCoUCE16iEnPG2ZZGTFqlWrDOfOnTMcP37cMGzYMIO1tbVh69atJnGMnqVMmh49eqhRFpnR8zgNGjRIjdiR9zc+j+Li4tLSSHmkXBrJo5x7EydOVJ8lGU1mZ2enjpnm22+/Vcdl9erVhn/++UeNjvH09DQ8ePDAJMsk+bW3tzcsX7483WvkeAv5+/777xv27dunjpOck7Vq1TKUK1fOEB8fb5JlkpGZmzZtMvz777+GQ4cOGbp06WJwdHQ0nDx50myPk6ZRo0ZqNG1Geh+njz/+WI0mk/eW/SmPZfTl5s2bTfazxGDHxGnDlDMuMuxQyN/GjRs/9JoaNWqoi1qZMmXU0MyMpk+fbihVqpRKI8Oc9+/fb5Llkf8/Lr2Qi0HRokVVWYoXL64eh4eH50h5nqVM3333ncHLy0tdkN3c3Awvv/yyYfv27SZzjJ6lTEICUCcnJ8OcOXMy3aaexymzsshi/NmQ8hifV+LPP/80lC9fXuVZpnRYt25duudlyOznn39u8PDwUMFpkyZNDGFhYSZbppdeeinT18iXj5Av4ObNmxsKFy6svowkff/+/XMsyH6WMg0fPjztcyLHoVWrVobDhw+b9XESZ86cUem0AMKY3sfprbfeUu8p+1zyIPvTOJ+m+Fmykn+yp46IiIiIyPSwzw4RERFZNAY7REREZNEY7BAREZFFY7BDREREFo3BDhEREVk0BjtERERk0RjsEBERkUVjsENEREQWjcEOEVE2s7KywqpVq/TOBhH9HwY7RGRR+vTpo4KNjEtAQIDeWSMindjq9cZERC+KBDbz5s1Lt87BwUG3/BCRvlizQ0QWRwKbIkWKpFsKFCignpNanlmzZqFly5ZwcnJCmTJlsHz58nSvP378OF599VX1fMGCBTFgwADExMSkSzN37lxUqVJFvVfRokUxZMiQdM9HRUWhffv2cHZ2Rrly5fD333/nQMmJKDMMdogo1/n888/xxhtv4NixY+jevTu6dOmC06dPq+diY2PRokULFRwdPHgQy5Ytw9atW9MFMxIsDR48WAVBEhhJIFO2bNl07zFu3Di8+eab+Oeff9CqVSv1Prdv387xshKR3D+eiMiC9O7d22BjY2PIkydPuuWrr75Sz8tlb+DAgele4+PjYxg0aJD6/5w5cwwFChQwxMTEpD2/bt06g7W1teH69evqcbFixQyffvrpI/Mg7/HZZ5+lPZZtyboNGzZke3mJ6MnYZ4eILM4rr7yial+Mubm5pf3f19c33XPy+OjRo+r/UsPj7e2NPHnypD3fsGFDpKamIiwsTDWDXb16FU2aNHlsHqpXr572f9mWi4sLIiMjn7tsRPT0GOwQkcWR4CJjs1J2kX48WWFnZ5fusQRJEjARUc5jnx0iynX279//0ONKlSqp/8tf6csjfXc0wcHBsLa2RoUKFZAvXz6ULl0a27Zty/F8E9GzYc0OEVmchIQEXL9+Pd06W1tbFCpUSP1fOh3XqVMHjRo1wsKFCxESEoJff/1VPScdiceOHYvevXvjiy++wM2bNzF06FD07NkTHh4eKo2sHzhwINzd3dWorvv376uASNIRkelhsENEFmfjxo1qOLgxqZU5c+ZM2kipJUuW4N1331XpFi9ejMqVK6vnZKj4pk2bMGzYMNStW1c9lpFbkydPTtuWBELx8fGYMmUK3n//fRVEdezYMYdLSURZZSW9lLOcmojIzEnfmb/++gvt2rXTOytElEPYZ4eIiIgsGoMdIiIismjss0NEuQpb7olyH9bsEBERkUVjsENEREQWjcEOERERWTQGO0RERGTRGOwQERGRRWOwQ0RERBaNwQ4RERFZNAY7REREBEv2/wCco3yN4cbAWQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All training demonstrations complete (or skipped if dependencies missing).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "import math\n",
    "\n",
    "# Ensure all helper functions from your notebook are defined before this script.\n",
    "# For example:\n",
    "# def load_mnist_images(filename): ...\n",
    "# def load_mnist_labels(filename): ...\n",
    "# class SimpleCNN(nn.Module): ...\n",
    "# def nested_loop_convolution(...): ...\n",
    "# def nested_loop_gradient(...): ...\n",
    "# etc.\n",
    "\n",
    "# --- PyTorch CNNDataset (from notebook cell 5ff84faf, potentially commented out) ---\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, digits, labels, transform=None):\n",
    "        assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "        self.digits = digits\n",
    "        self.labels = labels\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.digits)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digits[idx]\n",
    "        label = self.labels[idx]\n",
    "        # PyTorch Conv2d expects (B, C, H, W)\n",
    "        # MNIST images are (H, W), need to add channel dim\n",
    "        digit = digit.unsqueeze(0) # (H, W) -> (1, H, W)\n",
    "        return digit, label\n",
    "\n",
    "# --- Weight Initialization for NumPy models ---\n",
    "def initialize_numpy_weights(shapes_source_dict):\n",
    "    \"\"\"Initializes NumPy weights with random values based on shapes from a source dictionary.\"\"\"\n",
    "    np_weights = {}\n",
    "    # Small random weights for stability, biases to zero\n",
    "    np_weights['k1'] = np.random.randn(*shapes_source_dict['k1'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b_conv1'] = np.zeros(shapes_source_dict['b_conv1'].shape, dtype=np.float32)\n",
    "    np_weights['k2'] = np.random.randn(*shapes_source_dict['k2'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b_conv2'] = np.zeros(shapes_source_dict['b_conv2'].shape, dtype=np.float32)\n",
    "    np_weights['k3'] = np.random.randn(*shapes_source_dict['k3'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b_conv3'] = np.zeros(shapes_source_dict['b_conv3'].shape, dtype=np.float32)\n",
    "    np_weights['w1'] = np.random.randn(*shapes_source_dict['w1'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b1'] = np.zeros(shapes_source_dict['b1'].shape, dtype=np.float32)\n",
    "    np_weights['w2'] = np.random.randn(*shapes_source_dict['w2'].shape).astype(np.float32) * 0.01\n",
    "    np_weights['b2'] = np.zeros(shapes_source_dict['b2'].shape, dtype=np.float32)\n",
    "    return np_weights\n",
    "\n",
    "# --- Training Functions ---\n",
    "\n",
    "def train_pytorch_cnn(train_images_np, train_labels_np, test_images_np, test_labels_np, num_epochs=5, batch_size=128, learning_rate=0.001):\n",
    "    print(\"\\n--- Training PyTorch CNN ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data\n",
    "    tri = torch.from_numpy(train_images_np).float() / 255.0\n",
    "    trl = torch.from_numpy(train_labels_np).float() # Already one-hot\n",
    "    tsi = torch.from_numpy(test_images_np).float() / 255.0\n",
    "    tsl = torch.from_numpy(test_labels_np).float()   # Already one-hot\n",
    "\n",
    "    train_dataset = CNNDataset(tri, trl)\n",
    "    test_dataset = CNNDataset(tsi, tsl)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SimpleCNN(num_classes=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        test_loss_val = 0.0 # Renamed to avoid conflict with outer scope 'loss'\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                current_test_loss = criterion(outputs, labels) # Renamed\n",
    "                test_loss_val += current_test_loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, true_labels_indices = torch.max(labels, 1)\n",
    "                total += true_labels_indices.size(0)\n",
    "                correct += (predicted == true_labels_indices).sum().item()\n",
    "        \n",
    "        avg_test_loss = test_loss_val / len(test_loader.dataset)\n",
    "        accuracy = 100 * correct / total\n",
    "        test_accuracies.append(accuracy)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    print(\"PyTorch Training Complete.\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, marker='o')\n",
    "    plt.title('PyTorch Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), test_accuracies, marker='o', color='r')\n",
    "    plt.title('PyTorch Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_numpy_cnn(train_images_np, train_labels_np,\n",
    "                      conv_forward_fn, conv_backward_fn,\n",
    "                      model_name, initial_np_weights,\n",
    "                      num_epochs=5, batch_size=64, learning_rate=0.001):\n",
    "    print(f\"\\n--- Training NumPy CNN: {model_name} ---\")\n",
    "\n",
    "    k1, b_conv1 = np.copy(initial_np_weights['k1']), np.copy(initial_np_weights['b_conv1'])\n",
    "    k2, b_conv2 = np.copy(initial_np_weights['k2']), np.copy(initial_np_weights['b_conv2'])\n",
    "    k3, b_conv3 = np.copy(initial_np_weights['k3']), np.copy(initial_np_weights['b_conv3'])\n",
    "    w1, b1_fc = np.copy(initial_np_weights['w1']), np.copy(initial_np_weights['b1']) # Renamed b1 to b1_fc\n",
    "    w2, b2_fc = np.copy(initial_np_weights['w2']), np.copy(initial_np_weights['b2']) # Renamed b2 to b2_fc\n",
    "\n",
    "\n",
    "    num_samples = train_images_np.shape[0]\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_loss = 0.0\n",
    "        \n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        shuffled_images = train_images_np[permutation] / 255.0\n",
    "        shuffled_labels = train_labels_np[permutation]\n",
    "\n",
    "        progress_bar = tqdm(range(0, num_samples, batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        num_batches_processed = 0\n",
    "        for i in progress_bar:\n",
    "            num_batches_processed += 1\n",
    "            batch_images = shuffled_images[i:i+batch_size]\n",
    "            batch_labels = shuffled_labels[i:i+batch_size]\n",
    "            \n",
    "            c0 = batch_images.reshape(-1, 1, 28, 28).astype(np.float32)\n",
    "            current_bs = c0.shape[0]\n",
    "\n",
    "            # --- Forward Pass ---\n",
    "            # Conv Layer Parameters: (kernel, bias, padding, stride)\n",
    "            # Conv1: k=2, s=2, p=0. Output: (B, 32, 14, 14)\n",
    "            c1, mask1 = conv_forward_fn(c0, k1, b_conv1, padding=0, stride=2, applyReLU=True)\n",
    "            # Conv2: k=2, s=2, p=1. Output: (B, 64, 8, 8)\n",
    "            c2, mask2 = conv_forward_fn(c1, k2, b_conv2, padding=1, stride=2, applyReLU=True)\n",
    "            # Conv3: k=2, s=2, p=0. Output: (B, 128, 4, 4)\n",
    "            c3, mask3 = conv_forward_fn(c2, k3, b_conv3, padding=0, stride=2, applyReLU=True)\n",
    "            \n",
    "            input_to_mlp = c3.reshape(current_bs, -1) # (B, 2048)\n",
    "            fl, fa, sl, sa = ReLU_SoftMax_FullyConnected(input_to_mlp, w1, b1_fc, w2, b2_fc)\n",
    "            \n",
    "            batch_loss_val = 0 # Renamed\n",
    "            for j_idx in range(current_bs): # Renamed loop var\n",
    "                batch_loss_val += crossEntropy(sa[j_idx], batch_labels[j_idx])\n",
    "            batch_loss_val /= current_bs\n",
    "            current_epoch_loss += batch_loss_val\n",
    "\n",
    "            # --- Backward Pass ---\n",
    "            dL_i_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2 = \\\n",
    "                ReLU_SoftMax_FC_Backward(current_bs, sa, batch_labels, w1, w2, fa, fl, input_to_mlp)\n",
    "\n",
    "            dL_dA3 = dL_i_mlp.reshape(c3.shape) \n",
    "\n",
    "            # Backward pass for conv layers\n",
    "            # Signature for im2col_gradient_: (original_forward_input, d_image, kernels, mask, padding, stride)\n",
    "            # Signature for others: (batch_of_images (input), d_image, kernels, mask, padding, stride)\n",
    "            if conv_backward_fn.__name__ == 'im2col_gradient_':\n",
    "                 gi3, gk3, gb3 = conv_backward_fn(c2, dL_dA3, k3, mask3, padding=0, stride=2)\n",
    "                 gi2, gk2, gb2 = conv_backward_fn(c1, gi3, k2, mask2, padding=1, stride=2)\n",
    "                 gi1, gk1, gb1 = conv_backward_fn(c0, gi2, k1, mask1, padding=0, stride=2)\n",
    "            else:\n",
    "                 gi3, gk3, gb3 = conv_backward_fn(c2, dL_dA3, k3, mask3, padding=0, stride=2)\n",
    "                 gi2, gk2, gb2 = conv_backward_fn(c1, gi3, k2, mask2, padding=1, stride=2)\n",
    "                 gi1, gk1, gb1 = conv_backward_fn(c0, gi2, k1, mask1, padding=0, stride=2)\n",
    "            \n",
    "            # For nested_loop_gradient and im2col_gradient, bias gradients (gb1, gb2, gb3) might be\n",
    "            # (batch_size, num_filters). Sum over batch axis if so.\n",
    "            # im2col_gradient_ returns bias grads already summed over batch, shape (num_filters,).\n",
    "            if conv_backward_fn.__name__ != 'im2col_gradient_':\n",
    "                 if gb3.ndim > 1 and gb3.shape[0] == current_bs : gb3 = np.sum(gb3, axis=0)\n",
    "                 if gb2.ndim > 1 and gb2.shape[0] == current_bs : gb2 = np.sum(gb2, axis=0)\n",
    "                 if gb1.ndim > 1 and gb1.shape[0] == current_bs : gb1 = np.sum(gb1, axis=0)\n",
    "\n",
    "            # Normalize gradients by batch size\n",
    "            dL_dw1 /= current_bs; dL_db1 /= current_bs\n",
    "            dL_dw2 /= current_bs; dL_db2 /= current_bs\n",
    "            gk1 /= current_bs; gb1 /= current_bs\n",
    "            gk2 /= current_bs; gb2 /= current_bs\n",
    "            gk3 /= current_bs; gb3 /= current_bs\n",
    "            \n",
    "            # --- Weight Update (Simple SGD) ---\n",
    "            w1  -= learning_rate * dL_dw1\n",
    "            b1_fc  -= learning_rate * dL_db1.reshape(b1_fc.shape) # FC biases are (1, out_features)\n",
    "            w2  -= learning_rate * dL_dw2\n",
    "            b2_fc  -= learning_rate * dL_db2.reshape(b2_fc.shape) # FC biases are (1, out_features)\n",
    "            \n",
    "            k3  -= learning_rate * gk3\n",
    "            b_conv3 -= learning_rate * gb3.reshape(b_conv3.shape) # Conv biases are (out_channels,)\n",
    "            k2  -= learning_rate * gk2\n",
    "            b_conv2 -= learning_rate * gb2.reshape(b_conv2.shape)\n",
    "            k1  -= learning_rate * gk1\n",
    "            b_conv1 -= learning_rate * gb1.reshape(b_conv1.shape)\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{batch_loss_val:.4f}\")\n",
    "\n",
    "        avg_epoch_loss = current_epoch_loss / num_batches_processed if num_batches_processed > 0 else 0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s - Avg Training Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "    print(f\"{model_name} NumPy Training Complete.\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "    plt.title(f'{model_name} NumPy Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    trained_weights = {'k1': k1, 'b_conv1': b_conv1, 'k2': k2, 'b_conv2': b_conv2, 'k3': k3, 'b_conv3': b_conv3,\n",
    "                       'w1': w1, 'b1': b1_fc, 'w2': w2, 'b2': b2_fc}\n",
    "    return trained_weights\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    # Load MNIST data\n",
    "    try:\n",
    "        raw_train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "        raw_train_labels_int = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "        raw_test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "        raw_test_labels_int = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "    except FileNotFoundError:\n",
    "        print(\"MNIST data files not found. Please download them from http://yann.lecun.com/exdb/mnist/ and place in 'MNIST/' directory.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading MNIST data: {e}\")\n",
    "        print(\"Ensure all helper functions (load_mnist_images, etc.) from your notebook are defined.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    train_labels_one_hot = np.zeros((raw_train_labels_int.shape[0], 10), dtype=np.float32)\n",
    "    for i_idx in range(len(raw_train_labels_int)): # Renamed loop var\n",
    "        train_labels_one_hot[i_idx][raw_train_labels_int[i_idx]] = 1\n",
    "\n",
    "    test_labels_one_hot = np.zeros((raw_test_labels_int.shape[0], 10), dtype=np.float32)\n",
    "    for i_idx in range(len(raw_test_labels_int)): # Renamed loop var\n",
    "        test_labels_one_hot[i_idx][raw_test_labels_int[i_idx]] = 1\n",
    "        \n",
    "    # Use a subset for faster demonstration\n",
    "    num_train_samples = 1280 # Approx 20 batches of 64, or 10 batches of 128\n",
    "    num_test_samples = 200\n",
    "    \n",
    "    train_images_subset = raw_train_images[:num_train_samples]\n",
    "    train_labels_subset = train_labels_one_hot[:num_train_samples]\n",
    "    test_images_subset = raw_test_images[:num_test_samples]\n",
    "    test_labels_subset = test_labels_one_hot[:num_test_samples]\n",
    "\n",
    "    EPOCHS = 3\n",
    "    BATCH_SIZE_PYTORCH = 64 \n",
    "    BATCH_SIZE_NUMPY = 32 \n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    # 1. Train PyTorch CNN\n",
    "    # Make sure SimpleCNN is defined\n",
    "    if 'SimpleCNN' in globals():\n",
    "        trained_pytorch_model = train_pytorch_cnn(\n",
    "            train_images_subset, train_labels_subset,\n",
    "            test_images_subset, test_labels_subset,\n",
    "            num_epochs=EPOCHS, batch_size=BATCH_SIZE_PYTORCH, learning_rate=LEARNING_RATE\n",
    "        )\n",
    "    else:\n",
    "        print(\"SimpleCNN class not defined. Skipping PyTorch training.\")\n",
    "\n",
    "    # Prepare for NumPy models - Get weight shapes\n",
    "    shapes_source = {}\n",
    "    try:\n",
    "        # Attempt to load pre-trained model for shapes, or use a dummy instance\n",
    "        temp_model = SimpleCNN(num_classes=10)\n",
    "        try:\n",
    "            temp_model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu')))\n",
    "        except FileNotFoundError:\n",
    "            print(\"'simple_cnn_mnist.pth' not found for shapes, using uninitialized SimpleCNN shapes.\")\n",
    "        \n",
    "        shapes_source['k1'] = temp_model.conv1.weight.data.numpy()\n",
    "        shapes_source['b_conv1'] = temp_model.conv1.bias.data.numpy()\n",
    "        shapes_source['k2'] = temp_model.conv2.weight.data.numpy()\n",
    "        shapes_source['b_conv2'] = temp_model.conv2.bias.data.numpy()\n",
    "        shapes_source['k3'] = temp_model.conv3.weight.data.numpy()\n",
    "        shapes_source['b_conv3'] = temp_model.conv3.bias.data.numpy()\n",
    "        shapes_source['w1'] = temp_model.fc1.weight.data.numpy().T \n",
    "        shapes_source['b1'] = temp_model.fc1.bias.data.numpy().reshape(1, -1)\n",
    "        shapes_source['w2'] = temp_model.fc2.weight.data.numpy().T\n",
    "        shapes_source['b2'] = temp_model.fc2.bias.data.numpy().reshape(1, -1)\n",
    "        \n",
    "        initial_w = initialize_numpy_weights(shapes_source)\n",
    "        numpy_training_possible = True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not initialize shapes for NumPy models: {e}. Skipping NumPy training.\")\n",
    "        numpy_training_possible = False\n",
    "\n",
    "    if numpy_training_possible:\n",
    "        # Check if all NumPy functions are defined\n",
    "        numpy_fns_defined = all(fn_name in globals() for fn_name in [\n",
    "            'nested_loop_convolution', 'nested_loop_gradient',\n",
    "            'im2col_convolution', 'im2col_gradient',\n",
    "            'im2col_optimized', 'im2col_gradient_',\n",
    "            'ReLU_SoftMax_FullyConnected', 'ReLU_SoftMax_FC_Backward', 'crossEntropy', 'dilate'\n",
    "        ])\n",
    "\n",
    "        if not numpy_fns_defined:\n",
    "            print(\"One or more required NumPy helper functions are not defined. Skipping NumPy training.\")\n",
    "        else:\n",
    "            # 2. Train Nested-Loops NumPy CNN (Can be very slow)\n",
    "            ########## WARNING: Nested Loops NumPy training can be extremely slow!!!\n",
    "            # trained_weights_nested_loops = train_numpy_cnn(\n",
    "            #     train_images_subset, train_labels_subset,\n",
    "            #     conv_forward_fn=nested_loop_convolution,\n",
    "            #     conv_backward_fn=nested_loop_gradient,\n",
    "            #     model_name=\"Nested Loops\",\n",
    "            #     initial_np_weights=initial_w,\n",
    "            #     num_epochs=EPOCHS, batch_size=BATCH_SIZE_NUMPY, learning_rate=LEARNING_RATE\n",
    "            # )\n",
    "\n",
    "            # 3. Train Im2Col NumPy CNN\n",
    "            # trained_weights_im2col = train_numpy_cnn(\n",
    "            #     train_images_subset, train_labels_subset,\n",
    "            #     conv_forward_fn=im2col_convolution,\n",
    "            #     conv_backward_fn=im2col_gradient,\n",
    "            #     model_name=\"Im2Col\",\n",
    "            #     initial_np_weights=initial_w,\n",
    "            #     num_epochs=EPOCHS, batch_size=BATCH_SIZE_NUMPY, learning_rate=LEARNING_RATE\n",
    "            # )\n",
    "\n",
    "            # 4. Train Im2Col Optimized NumPy CNN\n",
    "            trained_weights_im2col_opt = train_numpy_cnn(\n",
    "                train_images_subset, train_labels_subset,\n",
    "                conv_forward_fn=im2col_optimized,\n",
    "                conv_backward_fn=im2col_gradient_optimized,\n",
    "                model_name=\"Im2Col Optimized\",\n",
    "                initial_np_weights=initial_w,\n",
    "                num_epochs=EPOCHS, batch_size=BATCH_SIZE_NUMPY, learning_rate=LEARNING_RATE\n",
    "            )\n",
    "\n",
    "    print(\"\\nAll training demonstrations complete (or skipped if dependencies missing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468c7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
