{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106183a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05471163",
   "metadata": {},
   "source": [
    "This section handles the loading and initial preparation of the MNIST dataset. MNIST contains 28x28 pixel grayscale images of handwritten digits (0-9).\n",
    "\n",
    "**Key Operations:**\n",
    "\n",
    "1.  **Data Loading (`load_mnist_images`, `load_mnist_labels`):**\n",
    "    *   These functions read the MNIST dataset from its specific binary file format.\n",
    "    *   Image data is reshaped to `(num_images, rows, cols)`.\n",
    "\n",
    "2.  **One-Hot Encoding Labels:**\n",
    "    *   For multi-class classification with a softmax output and categorical cross-entropy loss, integer labels (e.g., digit `5`) are converted into a one-hot vector format (e.g., `[0,0,0,0,0,1,0,0,0,0]` for 10 classes).\n",
    "    *   This represents the true label as a probability distribution where the correct class has a probability of 1.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"images/mnist_digits.png\", style=\"border-radius:20px;\", width=\"50%\">\n",
    "    <figcaption>Samples from the MNIST dataset</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9edcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "#-------------- Data Extraction ---------------------------\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097cf66",
   "metadata": {},
   "source": [
    "## PyTorch CNN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8be5b",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is defined using PyTorch's `nn.Module` to serve as a reference and source of pre-trained weights.\n",
    "\n",
    "**Architecture (defined as `SimpleCNN` class):**\n",
    "\n",
    "1.  **Conv1 + ReLU1:** `nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 1, 28, 28)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1 = \\lfloor \\frac{(28 - 2 + 0)}{2} \\rfloor + 1 = 14$\n",
    "    *   Output: `(B, 32, 14, 14)`\n",
    "\n",
    "2.  **Conv2 + ReLU2:** `nn.Conv2d(32, 64, 2, 2, 1)`\n",
    "    *   Input: `(B, 32, 14, 14)`\n",
    "    *   Padded input dimension: $14 + 2*1 = 16$\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(16 - 2 + 0)}{2} \\rfloor + 1 = 8$\n",
    "    *   Output: `(B, 64, 8, 8)`\n",
    "\n",
    "3.  **Conv3 + ReLU3:** `nn.Conv2d(64, 128, 2, 2, 0)`\n",
    "    *   Input: `(B, 64, 8, 8)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(8 - 2 + 0)}{2} \\rfloor + 1 = 4$\n",
    "    *   Output: `(B, 128, 4, 4)`\n",
    "\n",
    "4.  **Flatten:** `nn.Flatten()`\n",
    "    *   Input: `(B, 128, 4, 4)`\n",
    "    *   Output: `(B, 128 * 4 * 4)` which is `(B, 2048)`\n",
    "\n",
    "5.  **FC1 + ReLU4:** `nn.Linear(in_features=2048, out_features=250)`\n",
    "    *   Input: `(B, 2048)`\n",
    "    *   Operation: $Y = XW + b$\n",
    "    *   Output: `(B, 250)`\n",
    "\n",
    "6.  **FC2:** `nn.Linear(in_features=250, out_features=10)` (Output layer)\n",
    "    *   Input: `(B, 250)`    \n",
    "    *   Operation: $Y = XW + b$\n",
    "    *   Output: `(B, 10)` (logits for 10 classes)\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"images/cnn.png\", style=\"border-radius:20px;\", height=300>\n",
    "    <figcaption>CNN Architecture (B: Batch size)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dabdea",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff84faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# No bias version of the model\n",
    "class SimpleCNN_no_bias(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN_no_bias, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1, bias=False)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0, bias=False)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# # 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8a955",
   "metadata": {},
   "source": [
    "### Extracting Pre-trained Weights from PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee8995",
   "metadata": {},
   "source": [
    "This section loads weights from a pre-trained PyTorch model (`simple_cnn_mnist.pth`) and converts them into NumPy arrays. These NumPy weights will be used for our custom CNN implementations to ensure consistency for inference comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e62c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 2, 2), NumPy Shape=(64, 32, 2, 2)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 2, 2), NumPy Shape=(128, 64, 2, 2)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # A good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_1 = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pytorch_weights_of_kernels_in_layer_1\n",
    "\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pytorch_weights_of_kernels_in_layer_1.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_2 = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pytorch_weights_of_kernels_in_layer_2\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pytorch_weights_of_kernels_in_layer_2.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_3 = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pytorch_weights_of_kernels_in_layer_3\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pytorch_weights_of_kernels_in_layer_3.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pytorch_fc1_layer_weights = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pytorch_fc1_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pytorch_fc1_layer_biases = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pytorch_fc1_layer_biases.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pytorch_fc1_layer_weights.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pytorch_fc1_layer_biases.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pytorch_fc2_layer_weights = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pytorch_fc2_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pytorch_fc2_layer_biases = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pytorch_fc2_layer_biases.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pytorch_fc2_layer_weights.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pytorch_fc2_layer_biases.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06822b",
   "metadata": {},
   "source": [
    "## CNN - NumPy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c17c8",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ca069",
   "metadata": {},
   "source": [
    "Zero-padding adds a border of zeros around an input image or feature map before convolution. For example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\quad \\xrightarrow{\\textcolor{lightgreen}{\\textnormal{zero padding}}} \\quad\n",
    "\\begin{bmatrix}\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 1 & 2 & 3 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 4 & 5 & 6 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 7 & 8 & 9 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It's important for:\n",
    "\n",
    "1.  **Controlling Output Spatial Dimensions:** Padding can be used to maintain or control the reduction in height/width of feature maps. The output dimension (e.g., height $O_H$) is given by:\n",
    "    $$ O_H = \\left\\lfloor \\frac{I_H - K_H + 2P_H}{S_H} \\right\\rfloor + 1 $$\n",
    "    where $I_H$ is input height, $K_H$ kernel height, $P_H$ padding on one side of height, and $S_H$ stride.\n",
    "2.  **Improving Feature Extraction at Borders:** Allows the kernel to process edge pixels more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b70273b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "image_3_by_3 = np.arange(1,37).reshape(2,2,3,3)\n",
    "padded_image_3_by_3 = np.pad(image_3_by_3,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(image_3_by_3)\n",
    "print(padded_image_3_by_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00035f0",
   "metadata": {},
   "source": [
    "### Matrix Dilatation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6aa4a",
   "metadata": {},
   "source": [
    "**Relevance in Backpropagation for $\\frac{\\partial L}{\\partial X}$:**\n",
    "\n",
    "This dilation operation is a critical step when computing the gradient of the loss with respect to the input of a convolutional layer ($\\frac{\\partial L}{\\partial X}$), especially if the forward pass utilized a stride $S > 1$. Here is why:\n",
    "* When a forward convolution uses a stride $S > 1$, it effectively downsamples the input, resulting in an output feature map $Z$ with smaller spatial dimensions.\n",
    "* To calculate $\\frac{\\partial L}{\\partial X}$, we need to use the gradient flowing back from the subsequent layer, $\\frac{\\partial L}{\\partial Z}$ (where $Z$ is the output of the strided convolution). **Since the original input $X$ has larger spatial dimensions than $Z$, the gradient $\\frac{\\partial L}{\\partial Z}$ must be \"upsampled\" or \"spread out\" before it can be convolved with the kernel weights to produce a gradient of the correct shape for $X$.**\n",
    "\n",
    "**Dilation Step:** This upsampling is achieved by inserting $S-1$ rows and columns of zeros between the elements of $\\frac{\\partial L}{\\partial Z}$.\n",
    "\n",
    "After $\\frac{\\partial L}{\\partial Z}$ is dilated to form $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}$, it is then typically padded (with $K-1$ zeros where $K$ is the kernel dimension, adjusted for any original padding) and subsequently convolved with the 180-degree rotated (or flipped) kernel ($W_{rot180}$). This entire sequence of operations (padding the dilated output gradient and convolving it with the flipped kernel) is what yields $\\frac{\\partial L}{\\partial X}$ and is often referred to as a \"full convolution\" in this context (see \"A guide to convolution arithmetic for deep learning\" by Dumoulin and Visin, or the provided articles by Mayank Kaushik).\n",
    "\n",
    "The image below illustrates the concept of dilating an output gradient tensor. This dilation is a preparatory step before the gradient is used in further convolution operations during backpropagation for layers that had striding in their forward pass.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*luRORFyTmj9mJ7rVhzlbZA.png\" height=250, style=\"border-radius:20px;\">\n",
    "</figure>\n",
    "\n",
    "Illustrative example of dilating an output gradient tensor (like $\\frac{\\partial L}{\\partial Z}$) by inserting $S-1$ (stride minus one) zeros. In this case, $S=2$, so one row and one column of zeros is inserted between elements.\n",
    "<div style=\"text-align:center;\"><p><i><a href=\"https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710\">(Source: Backpropagation for Convolution with Strides, Mayank Kaushik)</a></i><p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd18f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilatation in case of original stride = 3\n",
      "Original Image Shape: (1, 1, 2, 2)\n",
      "Dilated Image Shape: (1, 1, 4, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dilate(tensor, stride):\n",
    "    if stride == 1:\n",
    "        return tensor\n",
    "\n",
    "    batch_size, num_channels, height, width = tensor.shape\n",
    "    \n",
    "    dilated_height = height + (height - 1) * (stride - 1)\n",
    "    dilated_width = width + (width - 1) * (stride - 1)\n",
    "    \n",
    "    dilated_tensor = np.zeros((batch_size, num_channels, dilated_height, dilated_width)).astype(tensor.dtype)\n",
    "    dilated_tensor[:, :, ::stride, ::stride] = tensor\n",
    "    return dilated_tensor\n",
    "\n",
    "# Example usage of dilate function\n",
    "example_image = np.arange(5, 9).reshape(1, 1, 2, 2)\n",
    "dilated_image = dilate(example_image, 3)\n",
    "print(\"Dilatation in case of original stride = 3\")\n",
    "print(\"Original Image Shape:\", example_image.shape)\n",
    "print(\"Dilated Image Shape:\", dilated_image.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 2), gridspec_kw={'width_ratios': [0.3, 1]})\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(example_image[0, 0], cmap='gray', vmin=0, vmax=8)\n",
    "plt.xticks([0, 1], [1, 2])\n",
    "plt.yticks([0, 1], [1, 2])\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(dilated_image[0, 0], cmap='gray', vmin=0, vmax=8)\n",
    "plt.xticks([0, 1, 2, 3], [1, 2, 3, 4])\n",
    "plt.yticks([0, 1, 2, 3], [1, 2, 3, 4])\n",
    "plt.title('Dilated Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa841de1",
   "metadata": {},
   "source": [
    "### Benchmark network (debug and testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c783ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class TesterCNN(nn.Module):\n",
    "    def __init__(self, kernels: torch.Tensor, biases: torch.Tensor = None, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernels.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(biases is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernels)\n",
    "            if biases is not None:\n",
    "                self.conv.bias.copy_(biases)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if biases is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817601c0",
   "metadata": {},
   "source": [
    "## Nested-Loops Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4351c2f",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26d187",
   "metadata": {},
   "source": [
    "`nested_loop_convolution` implements a 2D convolution followed by ReLU activation using explicit nested loops. This is computationally highly inefficient.\n",
    "\n",
    "**Inputs:**\n",
    "    *   `batch_of_images`: size `(N, C_in, H_in, W_in)`.\n",
    "    *   `kernels`: size `(C_out, C_in, K_H, K_W)`.\n",
    "    *   `biases`: Per-filter biases `(C_out,)`.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Padding & Output Size:** Input `batch_of_images` is padded to better extract information from tha borders of the images. Output dimensions $(O_H, O_W)$ are calculated using the formula in the Padding section.\n",
    "2.  **Convolution:** The convolution is computed explicitly, iterating over each element of the patches from the input tensor and multiplying them by the \"sliding\" kernels. For each output element $(n, f, y_{out}, x_{out})$:\n",
    "    $$ \\text{Output}(n, f, y_{out}, x_{out}) = \\left( \\sum_{c=0}^{C_{in}-1} \\sum_{k_y=0}^{K_H-1} \\sum_{k_x=0}^{K_W-1} \\text{Img}_{pad}(n, c, y_{out}S + k_y, x_{out}S + k_x) \\cdot \\text{Ker}(f, c, k_y, k_x) \\right) + \\text{Bias}(f) $$\n",
    "3.  **ReLU Activation:** If `applyReLU=True`: $\\text{ActivatedOutput} = \\max(0, \\text{Output})$. A binary `mask` (1 where Output > 0, else 0) is also returned for backpropagation.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/refs/heads/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Convolution of a 7x7 image (blue) with a 3x3 kernel (gray), resulting in a 5x5 output (green). <br><i><a href\"https://arxiv.org/pdf/1603.07285\">(Source: A guide to convolution arithmetic for deep learning - Dumoulin & Visin)</a></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51071b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------example_image--------\n",
      "[[[[1. 2. 3.]\n",
      "   [4. 5. 6.]\n",
      "   [7. 8. 9.]]]]\n",
      "-------example_kernel-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Nested-Loop approach-------\n",
      "[[[[  5.  19.]\n",
      "   [ 37.  78.]]\n",
      "\n",
      "  [[ 10.  40.]\n",
      "   [ 82. 191.]]]]\n",
      "-------PyTorch model-------\n",
      "tensor([[[[  5.,  19.],\n",
      "          [ 37.,  78.]],\n",
      "\n",
      "         [[ 10.,  40.],\n",
      "          [ 82., 191.]]]])\n"
     ]
    }
   ],
   "source": [
    "def nested_loop_convolution(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        output_channels, input_channels, kernel_width, kernel_height = kernels.shape\n",
    "        kernel_channels = output_channels\n",
    "    else: # Backward case\n",
    "        input_channels, output_channels, kernel_width, kernel_height = kernels.shape\n",
    "        kernel_channels = input_channels\n",
    "\n",
    "    # biases has shape (output_channels, 1, 1). It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # The number of channels taken in input by the kernel 'input_channels' must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    batch_size, channels, image_height, image_width  = batch_of_images.shape\n",
    "    output_image_height = int(((image_height - kernel_height) / stride) + 1) # new image height # Padding is already added\n",
    "    output_image_width = int(((image_width - kernel_width) / stride) + 1) # new image width\n",
    "    output = np.zeros((batch_size, output_channels, output_image_height, output_image_width)).astype(np.float32) # new image\n",
    "\n",
    "    if input_channels != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({input_channels}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for single_image in range(batch_size):\n",
    "        for single_kernel_channel in range(kernel_channels):\n",
    "            for output_row_idx in range(output_image_height): # which cycles row by row of the new image\n",
    "                for output_col_idx in range(output_image_width): # which cycles column by column of the new image\n",
    "                    output_cell_accumulator = 0.0  # sum for the current output cell (accumulates the convolution result)\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(input_channels): # channels == input_channels\n",
    "                        for kernel_row_idx in range(kernel_height):\n",
    "                            # Position of the kernel over the input image: row\n",
    "                            input_row_idx = (output_row_idx * stride) + kernel_row_idx\n",
    "                            for kernel_col_idx in range(kernel_width):\n",
    "                                # Position of the kernel over the input image: column\n",
    "                                input_col_idx = (output_col_idx * stride) + kernel_col_idx\n",
    "\n",
    "                                # Check if the position is inside the image\n",
    "                                if 0 <= input_row_idx < image_height and 0 <= input_col_idx < image_width:\n",
    "                                    input_element = batch_of_images[single_image, channel, input_row_idx, input_col_idx]\n",
    "                                    kernel_element = kernels[single_kernel_channel, channel, kernel_row_idx, kernel_col_idx]\n",
    "                                    # Compute the convolution sum (to be done for each element of the sliding kernel over the image)\n",
    "                                    output_cell_accumulator += (input_element * kernel_element).astype(np.float32)\n",
    "\n",
    "                    # Assign the result to the output image\n",
    "                    output[single_image, single_kernel_channel, output_row_idx, output_col_idx] = output_cell_accumulator\n",
    "\n",
    "    if biases.all() != 0:\n",
    "        biases = biases.reshape(biases.shape[0],1,1)\n",
    "\n",
    "        if biases.shape[0] != output_channels:\n",
    "            raise ValueError(f\"biases dimension ({biases.shape[0]}) doesn't match kernel's number of channels ({output_channels})\")\n",
    "        \n",
    "        output = output + biases\n",
    "\n",
    "    output = output.astype(np.float32)\n",
    "\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0, output)\n",
    "        mask = output.copy()\n",
    "        mask[mask > 0] = 1\n",
    "\n",
    "        return output, mask\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "example_image = np.arange(1,3*3+1).reshape(1,1,3,3).astype(np.float32)\n",
    "print(\"-------example_image--------\")\n",
    "print(example_image)\n",
    "\n",
    "example_kernel = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------example_kernel-------\")\n",
    "print(example_kernel)\n",
    "\n",
    "example_biases = np.array([1,2]).reshape(2,1,1)\n",
    "outpout, mask = nested_loop_convolution(example_image, example_kernel, biases=example_biases, padding=1, stride=2)\n",
    "print(\"-------Nested-Loop approach-------\")\n",
    "print(outpout)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "example_kernel = torch.from_numpy(example_kernel).float()\n",
    "example_biases = torch.from_numpy(np.array([1,2])).float()\n",
    "testerCNNmodel = TesterCNN(kernels=example_kernel,biases=example_biases, stride=2, padding=1)\n",
    "\n",
    "x = torch.from_numpy(example_image)\n",
    "y = testerCNNmodel(x)\n",
    "print(\"-------PyTorch model-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bad0a3",
   "metadata": {},
   "source": [
    "### Nested-Loops Approach: Convolutional Layer - Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff4adf",
   "metadata": {},
   "source": [
    "We now need to implement the backward pass of a convolutional layer, calculating the gradients of the loss function with respect to the layer's weights (or kernels) ($W^{(i)}$), biases ($b^{(i)}$), and the layer's input (images) ($X^{(i)}$).\n",
    "\n",
    "**Notation:**\n",
    "*   $L$: The Loss function.\n",
    "*   $X^{(i)}$: The input to convolutional layer $i$.\n",
    "*   $W^{(i)}$: The weights (kernels) of layer $i$.\n",
    "*   $b^{(i)}$: The biases of layer $i$.\n",
    "*   $Z^{(i)}$: The pre-activation output of layer $i$ ($Z^{(i)} = \\text{conv}(X^{(i)}, W^{(i)}) + b^{(i)}$).\n",
    "*   $A^{(i)}$: The activated output of layer $i$ (e.g., $A^{(i)} = \\text{ReLU}(Z^{(i)})$).\n",
    "*   $\\frac{\\partial L}{\\partial A^{(i)}}$: The gradient of the loss with respect to the activated output $A^{(i)}$ of the current layer (propagated from the next layer).\n",
    "*   $\\frac{\\partial L}{\\partial Z^{(i)}}$: The gradient of the loss with respect to the pre-activation output $Z^{(i)}$ of the current layer. This is often denoted as $\\delta^{(i)}$ in your original text after passing through the ReLU derivative.\n",
    "*   $\\text{mask}^{(i)}$: The binary mask derived from the ReLU activation in the forward pass (1 if $Z^{(i)} > 0$, else 0).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Gradient through ReLU Activation (Backward ReLU):**\n",
    "    The derivative of ReLU is 1 for positive inputs and 0 otherwise. This is efficiently implemented by element-wise multiplying the incoming gradient $\\frac{\\partial L}{\\partial A^{(i)}}$ with the `mask` computed during the forward pass.\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial Z^{(i)}} = \\frac{\\partial L}{\\partial A^{(i)}} \\odot \\text{mask}^{(i)}\n",
    "    $$\n",
    "    where $\\odot$ denotes the Hadamard (element-wise) product. Henceforth, for brevity, we will denote $\\frac{\\partial L}{\\partial Z^{(i)}}$ as $\\delta_Z^{(i)}$.\n",
    "\n",
    "2.  **Gradient with respect to Weights ($W^{(i)}$):**\n",
    "    To calculate $\\frac{\\partial L}{\\partial W^{(i)}}$, we need to correlate the layer's input $X^{(i)}$ with the pre-activation output gradient $\\delta_Z^{(i)}$. Each weight $W_{f,c,k_y,k_x}$ contributes to multiple elements of the output $Z^{(i)}$. Its gradient is the sum of all these contributions. Mathematically, this is equivalent to a convolution between the input $X^{(i)}$ (appropriately padded as in the forward pass) and the gradient $\\delta_Z^{(i)}$ (which is treated as the \"kernel\" for this convolution).\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(X^{(i)}_{padded}, \\delta_Z^{(i)})\n",
    "    $$\n",
    "    This convolution must always have a stride = 1.\n",
    "* **Dilation of $\\delta_Z^{(i)}$ (if forward stride > 1):** If the forward pass used a stride $S > 1$, $\\delta_Z^{(i)}$ must be dilated by inserting $S-1$ zeros between its elements *before* this convolution for $\\frac{\\partial L}{\\partial W^{(i)}}$. **This technique allows the use of a standard convolution**, where the \"kernel\" ($\\delta_Z^{(i)}$) slides over the \"image\" ($X^{(i)}$), and makes the output's size equal to the one of the original kernel.\n",
    "    \n",
    "    (If $X^{(i)}$ is $(N, C_{in}, H_{in}, W_{in})$ and $\\delta_Z^{(i)}$ is $(N, C_{out}, H_{out}, W_{out})$, the result $\\frac{\\partial L}{\\partial W^{(i)}}$ will have shape $(C_{out}, C_{in}, K_H, K_W)$, as the original kernel.)\n",
    "\n",
    "3.  **Gradient with respect to Input ($X^{(i)}$):**\n",
    "    To propagate the gradient to the previous layer, we also need to calculate $\\frac{\\partial L}{\\partial X^{(i)}}$. This operation is known as a \"Full Convolution\". It involves convolving the gradient $\\delta_Z^{(i)}$ with the kernels $W^{(i)}$ rotated by 180 degrees (or flipped both horizontally and vertically) ($W^{(i)}_{rot180}$).\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial X^{(i)}} = \\text{FullConvolution}(\\delta_Z^{(i)}, W^{(i)}_{rot180})\n",
    "    $$\n",
    "    ***Note:*** *The name \"Full Convolution\" refers to the fact that the first term of the operation is padded as much as possible (a larger padding would make some patches contain only zeros, so it would not make sense).*\n",
    "    \n",
    "    Full Convolution is implemented as follows:\n",
    "    *   **Dilation of $\\delta_Z^{(i)}$:** As for the gradient w.r.t. the weights, if the stride $S$ of the forward pass was greater than 1, $\\delta_Z^{(i)}$ is \"dilated\". Let's call this $\\delta_{Z,dilated}^{(i)}$. If $S=1$, $\\delta_{Z,dilated}^{(i)} = \\delta_Z^{(i)}$.\n",
    "    *   **Padding of $\\delta_{Z,dilated}^{(i)}$:** The dilated gradient is padded. If $P_{fwd}$ was the forward pass padding and $K$ the kernel dimension, the padding here is $K-1-P_{fwd}$ on each side.\n",
    "    *   **Convolution:** The padded $\\delta_{Z,dilated}^{(i)}$ is convolved with $W^{(i)}_{rot180}$. The stride of this convolution is always 1.\n",
    "    The result $\\frac{\\partial L}{\\partial X^{(i)}}$ will have the same spatial dimensions as the original input $X^{(i)}$.\n",
    "\n",
    "4.  **Gradient with respect to Biases ($b^{(i)}$):**\n",
    "    Since the bias $b_c^{(i)}$ (corresponding to filter $c$) is added to all elements of channel $c$ of the pre-activation output $Z^{(i)}$, its gradient is simply the sum of all elements of $\\delta_Z^{(i)}$ (the gradient *before* dilation) over that channel, across the spatial dimensions (height and width) and across all examples in the batch.\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n} \\sum_{h,w} (\\delta_Z^{(i)})_{n,c,h,w}\n",
    "    $$\n",
    "---\n",
    "<div style=\"text-align:center;\">\n",
    "<b>Visual example of forward and backward pass, with padding = 1 and striding = 2:</b>\n",
    "<div style=\"display:flex; flex-direction:row; justify-content:space-between; align-items:flex-start; margin-top:15px;\">\n",
    "<figure style=\"text-align:center; width:50%; flex-grow:1; margin-top:0px;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/padding_strides.gif\", style=\"border-radius:20px; height:200px;\"/>\n",
    "    <figcaption> The input image (blue) is 5x5, and the kernel (gray) is 3x3. The input has padding = 1 a stride = 2 is used. As we can see, the output is 3x3.</figcaption>\n",
    "</figure>\n",
    "<figure style=\"text-align:center; width:50%; flex-grow:1; margin-top:0px;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif\", style=\"border-radius:20px; height:200px;\"/>\n",
    "    <figcaption>Corresponding backward phase: the output gradient of the previous convolution is the 3x3 blue input. It is padded and dilated by stride-1=2-1=1 and convolved with the rotated kernel (gray) to produce the input gradient (5x5 green output).</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<figcaption style=\"text-align:center;\"><i><a href\"https://arxiv.org/pdf/1603.07285\">(Source: A guide to convolution arithmetic for deep learning - Dumoulin & Visin)</a></i>\n",
    "</fgcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1928d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_loop_gradient(batch_of_images, d_output_activated, kernels, mask, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the convolution layer using nested loops.\n",
    "    It takes the original input image batch, the gradient of the loss w.r.t. the activated output of this layer,\n",
    "    the kernels, forward pass padding, and forward pass stride.\n",
    "    The mask is needed to perform the ReLU backward operation.\n",
    "    Returns:\n",
    "        gradient_wrt_input (dL/dX_current_layer)\n",
    "        gradient_wrt_kernel (dL/dW_current_layer)\n",
    "        gradient_wrt_bias (dL/db_current_layer)\n",
    "    \"\"\"\n",
    "    batch_size, num_input_channels_img, img_height_orig, img_width_orig = batch_of_images.shape\n",
    "    num_output_channels_k, num_input_channels_k, kernel_height, kernel_width = kernels.shape\n",
    "\n",
    "    if num_input_channels_img != num_input_channels_k:\n",
    "        raise ValueError(\"Kernel input channels and image input channels must match.\")\n",
    "\n",
    "    # Backward ReLU: dL/dZ = dL/dA * mask\n",
    "    # d_output_activated is dL/dA (gradient from next layer w.r.t. this layer's activated output)\n",
    "    dLdZ = np.multiply(d_output_activated, mask) # Gradient w.r.t. pre-activation output\n",
    "\n",
    "    # --- Gradient w.r.t. Biases (dL/db) ---\n",
    "    # Sum dLdZ over batch, height, and width dimensions for each output channel\n",
    "    gradient_wrt_bias = np.sum(dLdZ, axis=(0, 2, 3)) # Shape: (num_output_channels_k,)\n",
    "\n",
    "    # --- Gradient w.r.t. Input (dL/dX) ---\n",
    "    # This is a \"full convolution\" of dLdZ_dilated with kernels_rotated_180\n",
    "    dLdZ_dilated = dilate(dLdZ, stride)\n",
    "    \n",
    "    # Padding for the \"full convolution\" to get dL/dX\n",
    "    # Output size O = (I - K + 2P_conv) / S_conv + 1. We want O = img_height_orig\n",
    "    # Here, S_conv = 1. K_eff = kernel_height. I_eff = dLdZ_dilated_height\n",
    "    # So, img_height_orig = dLdZ_dilated_height - kernel_height + 2P_conv + 1\n",
    "    # P_conv_total = img_height_orig - dLdZ_dilated_height + kernel_height - 1\n",
    "    # The provided formula in texts for dL/dX padding is K_fwd - 1 - P_fwd (on each side)\n",
    "    pad_h_dx = kernel_height - 1 - padding\n",
    "    pad_w_dx = kernel_width - 1 - padding\n",
    "\n",
    "    dLdZ_dilated_padded = np.pad(dLdZ_dilated,\n",
    "                                 ((0,0), (0,0), (pad_h_dx, pad_h_dx), (pad_w_dx, pad_w_dx)),\n",
    "                                 mode='constant', constant_values=0)\n",
    "\n",
    "    kernels_rot180 = np.rot90(kernels, 2, axes=(-2, -1)) # Rotate each kernel by 180 deg\n",
    "\n",
    "    gradient_wrt_input = np.zeros_like(batch_of_images, dtype=np.float32)\n",
    "    \n",
    "    # Convolution: dLdZ_dilated_padded with kernels_rot180 (stride 1)\n",
    "    # kernels_rot180 shape: (C_out_k, C_in_k, KH, KW)\n",
    "    # For dL/dX, the \"output channels\" of this conv are C_in_k (original input channels)\n",
    "    # and \"input channels\" are C_out_k (output channels of dLdZ)\n",
    "    \n",
    "    # Iterate over batch\n",
    "    for n in range(batch_size):\n",
    "        # Iterate over output channels of dL/dX (which are input channels of original image)\n",
    "        for c_in_orig in range(num_input_channels_img):\n",
    "            # Iterate over spatial dimensions of dL/dX (original image dimensions)\n",
    "            for r_out_dx in range(img_height_orig):\n",
    "                for c_out_dx in range(img_width_orig):\n",
    "                    current_sum_dx = 0.0\n",
    "                    # Iterate over \"input channels\" for this conv (which are output channels of dLdZ)\n",
    "                    for c_out_fwd in range(num_output_channels_k):\n",
    "                        # Iterate over kernel spatial dimensions\n",
    "                        for kr in range(kernel_height):\n",
    "                            for kc in range(kernel_width):\n",
    "                                r_in_conv = r_out_dx + kr # Stride is 1 for this conv\n",
    "                                c_in_conv = c_out_dx + kc\n",
    "                                \n",
    "                                if 0 <= r_in_conv < dLdZ_dilated_padded.shape[2] and \\\n",
    "                                   0 <= c_in_conv < dLdZ_dilated_padded.shape[3]:\n",
    "                                    val_dLdZ = dLdZ_dilated_padded[n, c_out_fwd, r_in_conv, c_in_conv]\n",
    "                                    # kernel_rot180[c_out_k_idx_as_input_channel, c_in_k_idx_as_output_channel, kr, kc]\n",
    "                                    # For dL/dX: kernel_rot180 \"output channels\" map to C_in_orig\n",
    "                                    # \"input channels\" map to C_out_fwd (dLdZ channels)\n",
    "                                    # kernels_rot180 is (orig_C_out, orig_C_in, KH, KW)\n",
    "                                    # We need kernel_rot180[c_out_fwd, c_in_orig, kr, kc] if thinking about it as transposed conv\n",
    "                                    # Or, if thinking of it as a standard conv:\n",
    "                                    # output_channel_of_conv = c_in_orig\n",
    "                                    # input_channel_of_conv = c_out_fwd\n",
    "                                    # So kernel is kernel_flipped[c_in_orig, c_out_fwd, kr, kc]\n",
    "                                    # Let's use the (C_out_orig, C_in_orig, KH, KW) indexing for kernels_rot180,\n",
    "                                    # and map it to the conv:\n",
    "                                    # kernels_rot180_for_conv_output_c_in_orig_input_c_out_fwd[kr,kc]\n",
    "                                    # = kernels_rot180[c_out_fwd, c_in_orig, kr, kc] (after transposing kernel channels)\n",
    "                                    # This is a bit confusing with nested loops. Let's use the common perspective:\n",
    "                                    # dL/dX[n, c_in, y, x] = sum_{c_out, kh, kw} dL/dZ_dilated_padded[n, c_out, y+kh, x+kw] * W_rot[c_out, c_in, kh, kw]\n",
    "                                    # Or, W_rot has shape (C_in_new, C_out_new, KH, KW)\n",
    "                                    # where C_in_new = C_out_orig, C_out_new = C_in_orig\n",
    "                                    # So kernel is kernels_rot180_conv[c_in_orig, c_out_fwd, kr, kc]\n",
    "                                    # which means we need to permute axes of kernels_rot180 (C_out_k, C_in_k, KH, KW)\n",
    "                                    # to (C_in_k, C_out_k, KH, KW) to treat it as a standard conv kernel\n",
    "                                    # kernels_rot180_permuted = kernels_rot180.transpose(1,0,2,3)\n",
    "                                    # val_kernel = kernels_rot180_permuted[c_in_orig, c_out_fwd, kr, kc]\n",
    "\n",
    "                                    # Simpler: use original kernel indexing and match terms\n",
    "                                    # W_rot180[num_output_channels_k, num_input_channels_k, KH, KW]\n",
    "                                    # dL/dX_c_in[y,x] = sum_{c_out} (dL/dZ_c_out_dilated_padded * W_rot[c_out, c_in, :, :])\n",
    "                                    val_kernel = kernels_rot180[c_out_fwd, c_in_orig, kr, kc]\n",
    "                                    current_sum_dx += val_dLdZ * val_kernel\n",
    "                    gradient_wrt_input[n, c_in_orig, r_out_dx, c_out_dx] = current_sum_dx\n",
    "\n",
    "    # --- Gradient w.r.t. Kernel (dL/dW) ---\n",
    "    # This is a convolution of batch_of_images_padded_fwd with dLdZ (potentially dilated for dW, or use strided access)\n",
    "    # Output should have shape of kernels: (num_output_channels_k, num_input_channels_k, kernel_height, kernel_width)\n",
    "    # dL/dW[c_out, c_in, kr, kc] = sum_{n, r_out, c_out} dL/dZ[n, c_out, r_out, c_out] * X_padded_fwd[n, c_in, r_out*S + kr, c_out*S + kc]\n",
    "    \n",
    "    batch_of_images_padded_fwd = np.pad(batch_of_images,\n",
    "                                     ((0,0), (0,0), (padding,padding), (padding,padding)),\n",
    "                                     mode='constant', constant_values=0)\n",
    "\n",
    "    gradient_wrt_kernel = np.zeros_like(kernels, dtype=np.float32)\n",
    "    \n",
    "    # Iterate over output channels of kernel (filters)\n",
    "    for f_out in range(num_output_channels_k):\n",
    "        # Iterate over input channels of kernel\n",
    "        for c_in_k_idx in range(num_input_channels_k):\n",
    "            # Iterate over kernel spatial dimensions (this defines the dW element we are computing)\n",
    "            for kr_idx in range(kernel_height):\n",
    "                for kc_idx in range(kernel_width):\n",
    "                    current_sum_dw = 0.0\n",
    "                    # Sum over batch\n",
    "                    for n in range(batch_size):\n",
    "                        # Sum over spatial dimensions of dLdZ\n",
    "                        for r_dLdZ in range(dLdZ.shape[2]): # dLdZ height\n",
    "                            for c_dLdZ in range(dLdZ.shape[3]): # dLdZ width\n",
    "                                # Corresponding input patch element from batch_of_images_padded_fwd\n",
    "                                r_in_img = r_dLdZ * stride + kr_idx\n",
    "                                c_in_img = c_dLdZ * stride + kc_idx\n",
    "                                \n",
    "                                if 0 <= r_in_img < batch_of_images_padded_fwd.shape[2] and \\\n",
    "                                   0 <= c_in_img < batch_of_images_padded_fwd.shape[3]:\n",
    "                                    val_img = batch_of_images_padded_fwd[n, c_in_k_idx, r_in_img, c_in_img]\n",
    "                                    val_dLdZ_elem = dLdZ[n, f_out, r_dLdZ, c_dLdZ]\n",
    "                                    current_sum_dw += val_dLdZ_elem * val_img\n",
    "                    gradient_wrt_kernel[f_out, c_in_k_idx, kr_idx, kc_idx] = current_sum_dw\n",
    "    \n",
    "    return gradient_wrt_input, gradient_wrt_kernel, gradient_wrt_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2fbc0",
   "metadata": {},
   "source": [
    "## Im2Col Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae10455",
   "metadata": {},
   "source": [
    "### Im2Col approach: Convolutional Layer - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c44f",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "    <style>\n",
    "    td      {vertical-align: middle;}\n",
    "    body    {text-align: center}\n",
    "    table, th, tr, td {border-collapse: collapse}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "<p><code>im2col_convolution</code> implements 2D convolution more efficiently using an Im2Col approach using<code>sliding_window_view</code> and optimized matrix multiplication.</p>\n",
    "\n",
    "<p><strong>Im2Col Core Idea:</strong></p>\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"width: 65%; padding: 25px 25px 0 25px;\">\n",
    "                <ol><li><strong>Input Patch Extraction:</strong></li><br>\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        <code>window_m = ... .reshape((-1,(kw*kh*nc)))</code>: Flattens each extracted patch into a row vector of size <code>kw*kh*nc</code>. <code>window_m</code> (our $X_{col}$) thus has shape <code>(N_patches, patch_size)</code>.\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        <code>kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)</code>: Flattens each filter <code>(kc, ac, kw, kh)</code> -> <code>(kc, ac*kw*kh)</code>. This is $W_{col}$, shape <code>(patch_size, C_out)</code>.\n",
    "                    </li>\n",
    "                </ul>\n",
    "                </ol>\n",
    "                <p>\n",
    "                    Following the example in the animation below, we have a 4x4 RGB image (so <strong>3x4x4</strong>) that has to be convoluted by a <strong>2x2</strong> kernel (represented as the red outlines sliding over the image). The kernel will slide <strong>9</strong> times over the image and, for each slide, the number of multiplications and values to be summed is 4 (elements of the kernel) times 3 (channels of both the image and the kernel) that equals <strong>12</strong>. Hence, as we can see in the animation below, the flattened patches are vectors of <strong>12</strong> elements, and in total they are <strong>9</strong>, as the slides performed by the kernel.\n",
    "                </p>\n",
    "            </td>\n",
    "            <td>\n",
    "                <figure style=\"text-align:center;\">\n",
    "                    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/1.gif\" height=\"250\" style=\"border-radius:20px; max-width: 100%;\"/>\n",
    "                    <figcaption>Each patch is extracted and flattened into a vector</figcaption>\n",
    "                </figure>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"width: 65%; padding: 25px;\">\n",
    "                <ol start=\"2\"><li><strong>Kernel Reshaping:</strong></li></ol>\n",
    "                <p>\n",
    "                    To perform the convolution as a vector-matrix multiplication (or matrix-matrix multiplication) we then have to flatten the kernel(s) into row vectors.\n",
    "                </p>\n",
    "            </td>\n",
    "            <td>\n",
    "                <figure style=\"text-align:center; margin-top: 0;\">\n",
    "                    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/10.png\" height=\"250\" style=\"border-radius:20px; max-width: 100%;\"/>\n",
    "                    <figcaption>Kernels are flattened into a vector</figcaption>\n",
    "                </figure>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"width: 65%; padding: 25px;\">\n",
    "                <ol start=\"3\"><li><strong>Convolution as Matrix Multiplication:</strong></li></ol>\n",
    "                <p>\n",
    "                    At this point, the convolution boils down to a simple matrix multiplication, with enormous gains in efficiency and simplicity.\n",
    "                </p>\n",
    "            </td>\n",
    "            <td>\n",
    "                <figure style=\"text-align:center; margin-top: 0;\">\n",
    "                    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/blog-deep-learning/cnnumpy-fast/11.gif\" height=\"250\" style=\"border-radius:20px; max-width: 100%;\"/>\n",
    "                    <figcaption>Each element of the output is given by a simple and optimized dot product</figcaption>\n",
    "                </figure>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<figcaption style=\"text-align:center; margin-top:10px;\"><i><a href\"https://hackmd.io/@machine-learning/blog-post-cnnumpy-fast\">(Images source: Convolutional Neural Network with Numpy)</a></i>\n",
    "</fgcaption>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c9deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[  1.   2.   3.   4.   5.]\n",
      "   [  6.   7.   8.   9.  10.]\n",
      "   [ 11.  12.  13.  14.  15.]\n",
      "   [ 16.  17.  18.  19.  20.]\n",
      "   [ 21.  22.  23.  24.  25.]]\n",
      "\n",
      "  [[ 26.  27.  28.  29.  30.]\n",
      "   [ 31.  32.  33.  34.  35.]\n",
      "   [ 36.  37.  38.  39.  40.]\n",
      "   [ 41.  42.  43.  44.  45.]\n",
      "   [ 46.  47.  48.  49.  50.]]\n",
      "\n",
      "  [[ 51.  52.  53.  54.  55.]\n",
      "   [ 56.  57.  58.  59.  60.]\n",
      "   [ 61.  62.  63.  64.  65.]\n",
      "   [ 66.  67.  68.  69.  70.]\n",
      "   [ 71.  72.  73.  74.  75.]]\n",
      "\n",
      "  [[ 76.  77.  78.  79.  80.]\n",
      "   [ 81.  82.  83.  84.  85.]\n",
      "   [ 86.  87.  88.  89.  90.]\n",
      "   [ 91.  92.  93.  94.  95.]\n",
      "   [ 96.  97.  98.  99. 100.]]]]\n",
      "-------ker-------\n",
      "[[[[ 1  2]\n",
      "   [ 3  4]]\n",
      "\n",
      "  [[ 5  6]\n",
      "   [ 7  8]]\n",
      "\n",
      "  [[ 9 10]\n",
      "   [11 12]]\n",
      "\n",
      "  [[13 14]\n",
      "   [15 16]]]\n",
      "\n",
      "\n",
      " [[[17 18]\n",
      "   [19 20]]\n",
      "\n",
      "  [[21 22]\n",
      "   [23 24]]\n",
      "\n",
      "  [[25 26]\n",
      "   [27 28]]\n",
      "\n",
      "  [[29 30]\n",
      "   [31 32]]]]\n",
      "-------Conv 'nested_loop'-------\n",
      "[[[[ 7689.]]\n",
      "\n",
      "  [[18314.]]]]\n",
      "-------Conv 'im2col'-------\n",
      "[[[[ 7689.]]\n",
      "\n",
      "  [[18314.]]]]\n"
     ]
    }
   ],
   "source": [
    "def im2col_convolution(batch_of_images, kernels, biases=np.array(0), padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "\n",
    "    ###### im2col approach steps: ######\n",
    "    # 1. Pad the input image as needed\n",
    "    if padding > 0:\n",
    "        batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    # 2. Extract sliding windows from the input image, considering the kernel size and stride\n",
    "    sliding_windows = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,input_channels,kernel_height,kernel_width))[:,:,::stride,::stride]\n",
    "\n",
    "    # 3. Flatten the sliding windows and the kernel to prepare for matrix multiplication\n",
    "    sliding_windows = sliding_windows.reshape((-1,(kernel_height * kernel_width * input_channels)))\n",
    "\n",
    "    kernels = kernels.reshape((-1,(kernel_height*kernel_width*input_channels))).transpose(1,0)\n",
    "\n",
    "    # Now, the convolution can be performed as a matrix multiplication, where each row of the sliding windows\n",
    "    # corresponds to a flattened patch of the input image, and each column of the kernels corresponds to a flattened kernel (channel-wise)\n",
    "    images_dot_kernels = np.matmul(sliding_windows, kernels).astype(np.float32) # It's called 'images_dot_kernels' because it contains the result of the whole convolution operation\n",
    "\n",
    "    # Compute the output dimensions to reshape the resulting matrix (each row corresponds to a patch)\n",
    "    output_width = int(((image_width - kernel_width) / stride) + 1)\n",
    "    output_height = int(((image_height - kernel_height) / stride) + 1)\n",
    "\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output = images_dot_kernels.reshape(batch_size, output_width, output_height, kernels_number)\n",
    "\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    output = output.transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "    # Add biases if they are provided\n",
    "    if biases.any() != 0:\n",
    "        output = (output + biases.reshape(1,-1,1,1))\n",
    "\n",
    "    # Apply ReLU activation if specified\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0, output)\n",
    "\n",
    "    # Create a mask for the backward operation of ReLU activation\n",
    "    mask = np.copy(output)\n",
    "    mask[mask > 0] = 1\n",
    "\n",
    "    return output, mask\n",
    "\n",
    "img = np.arange(1,4*5*5+1).reshape(1,4,5,5).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,32+1).reshape(2,4,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "\n",
    "s = 4\n",
    "\n",
    "res,mask = nested_loop_convolution(img, ker, bias,padding=0,stride=s)\n",
    "print(\"-------Conv 'nested_loop'-------\")\n",
    "print(res)\n",
    "X_c,mask = im2col_convolution(img, ker, bias,padding=0,stride=s)\n",
    "print(\"-------Conv 'im2col'-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c414872",
   "metadata": {},
   "source": [
    "### Im2Col approach: Convolutional Layer - Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e1403",
   "metadata": {},
   "source": [
    "`im2col_gradient` computes the following gradients using again the im2col approach:\n",
    "* $\\frac{\\partial L}{\\partial X}$ (`gi`): gradient w.r.t. the input images;\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial W}$ (`gk`): gradient w.r.t. the kernels;\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial b}$ (`gb`): gradient w.r.t. the biases.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Backward ReLU:** $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\text{mask}$<br>\n",
    "    \"$\\text{mask}$\" is a matrix (tensor) whose elements are 1 if the corrisponding elements in the output of a given layer were > 0. It enables the flowing of the gradient only through elements that contributed to form the output in the first place.\n",
    "\n",
    "2.  **Gradient w.r.t. Input (`gi`):**\n",
    "    $\\frac{\\partial L}{\\partial X} = \\text{FullConv}(\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}, W_{rot180})$<br>\n",
    "    A full convolution is needed, since we must obtain a result with the same size as the input image. As you remember, the output size is given by this formula: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1$, so we must \"invert\" it. In particular, the output tensor must be:\n",
    "    * dilated by $stride - 1$ and\n",
    "    * padded by:\n",
    "    $(kernel\\_height-1-input\\_padding{\\textnormal{, }}kernel\\_width-1-input\\_padding)$\n",
    "\n",
    "2.  **Gradient w.r.t. Kernel (`gk`):**\n",
    "    This is $\\frac{\\partial L}{\\partial W} = \\text{Conv}(X_{padded}, \\frac{\\partial L}{\\partial Z})$.\n",
    "   \n",
    "\n",
    "3.  **Gradient w.r.t. Bias (`gb`):** $\\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} (\\frac{\\partial L}{\\partial Z})_{n,f,h,w}$. (`gb = d_img.sum((-1,-2))`).\n",
    "\n",
    "**But what if, in the forward phase, the stride was greater than 1?**<br>\n",
    "Dilation was already mentioned, but here we will give you again an explanation, but with a detailed visual example. Consider the case in which the stride was $S= 2$ in the forward pass. The output gradient tensor needs to be dilated (in order to obtain a tensor with the same size as the input image) before being convolved with the kernel gradient, to be effectively useful to backpropagate the loss. A visualization of why dilation enables this fundamental result is in the gif below:\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KOHfsOHX5ujcMfr6Xjy6zQ.png\" height=\"250\", style=\"border-radius:20px 0 0 20px;\"/>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QvTW-pNwJAlbfj1LrZe8JA.gif\" height=\"250\", style=\"border-radius:0 20px 20px 0;\"/>\n",
    "    <figcaption>The backpropagation operation is identical to a stride = 1 convolution of a padded, dilated version of the output gradient tensor with a flipped version of the filter.<br><i><a href=\"https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710\">(Source: Backpropagation for Convolution with Strides, Mayank Kaushik)</a></i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c5583e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_gradient(batch_of_images, d_output_activated, kernels, mask, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the convolution layer using im2col.\n",
    "    \"\"\"\n",
    "    C_out_k, C_in_k, KH, KW = kernels.shape\n",
    "    B, C_in_img, H_in_img_orig, W_in_img_orig = batch_of_images.shape\n",
    "\n",
    "    if C_in_img != C_in_k:\n",
    "        raise ValueError(\"Kernel input channels and image input channels must match.\")\n",
    "\n",
    "    # Backward ReLU: dL/dZ = dL/dA * mask\n",
    "    dLdZ = np.multiply(d_output_activated, mask) # Shape: (B, C_out_k, H_out_fwd, W_out_fwd)\n",
    "\n",
    "    # --- Gradient w.r.t. Biases (dL/db) ---\n",
    "    gradient_wrt_biases = np.sum(dLdZ, axis=(0, 2, 3)) # Shape: (C_out_k,)\n",
    "\n",
    "    # --- Gradient w.r.t. Input (dL/dX) ---\n",
    "    dLdZ_dilated_for_dX = dilate(dLdZ, stride)\n",
    "    \n",
    "    pad_h_dx = KH - 1 - padding\n",
    "    pad_w_dx = KW - 1 - padding\n",
    "    dLdZ_dilated_padded_for_dX = np.pad(dLdZ_dilated_for_dX,\n",
    "                                     ((0,0), (0,0), (pad_h_dx, pad_h_dx), (pad_w_dx, pad_w_dx)),\n",
    "                                     mode='constant', constant_values=0)\n",
    "\n",
    "    kernels_rot180 = np.rot90(kernels, 2, axes=(-2, -1)) # Shape: (C_out_k, C_in_k, KH, KW)\n",
    "    \n",
    "    # For dL/dX, \"output channels\" = C_in_k, \"input channels\" = C_out_k\n",
    "    # Kernels for this conv: (C_in_k, C_out_k, KH, KW)\n",
    "    # This means transposing the first two axes of kernels_rot180\n",
    "    W_col_dX_kernels = kernels_rot180.transpose(1,0,2,3).reshape(C_in_k, -1).T # (C_out_k*KH*KW, C_in_k)\n",
    "\n",
    "    # Im2col for dLdZ_dilated_padded_for_dX (input to this conv)\n",
    "    # Patches are (C_out_k, KH, KW)\n",
    "    # Output of sliding_window_view: (B, 1, H_target, W_target, 1, C_out_k, KH, KW)\n",
    "    # where H_target, W_target are original input image dimensions\n",
    "    H_out_dx = (dLdZ_dilated_padded_for_dX.shape[2] - KH) // 1 + 1 # Stride is 1 for this conv\n",
    "    W_out_dx = (dLdZ_dilated_padded_for_dX.shape[3] - KW) // 1 + 1\n",
    "\n",
    "    # Ensure H_out_dx == H_in_img_orig and W_out_dx == W_in_img_orig\n",
    "    if H_out_dx != H_in_img_orig or W_out_dx != W_in_img_orig:\n",
    "         # This can happen if padding_fwd was 'same' or auto, and K-1-P_fwd leads to slight mismatch\n",
    "         # We'll allow it for now and reshape/crop later if needed, but it's a sign of potential padding issues\n",
    "         pass\n",
    "\n",
    "\n",
    "    view_dX = np.lib.stride_tricks.sliding_window_view(dLdZ_dilated_padded_for_dX, (1, C_out_k, KH, KW))\n",
    "    # view_dX shape: (B, 1_dummy, H_out_dx, W_out_dx, 1_dummy, C_out_k, KH, KW)\n",
    "    X_col_dX = view_dX.reshape(B * H_out_dx * W_out_dx, C_out_k * KH * KW)\n",
    "\n",
    "    gradient_wrt_input_col = np.matmul(X_col_dX, W_col_dX_kernels) # (B*H_out_dx*W_out_dx, C_in_k)\n",
    "    \n",
    "    # Reshape to (B, H_out_dx, W_out_dx, C_in_k) then transpose\n",
    "    gradient_wrt_input_reshaped = gradient_wrt_input_col.reshape(B, H_out_dx, W_out_dx, C_in_k)\n",
    "    gradient_wrt_input = gradient_wrt_input_reshaped.transpose(0,3,1,2) # (B, C_in_k, H_out_dx, W_out_dx)\n",
    "\n",
    "    # Crop if necessary to match original input shape (due to padding effects in \"full\" conv)\n",
    "    if gradient_wrt_input.shape[2] > H_in_img_orig:\n",
    "        gradient_wrt_input = gradient_wrt_input[:,:,:H_in_img_orig,:]\n",
    "    if gradient_wrt_input.shape[3] > W_in_img_orig:\n",
    "        gradient_wrt_input = gradient_wrt_input[:,:,:,:W_in_img_orig]\n",
    "    if gradient_wrt_input.shape != batch_of_images.shape: # Final check if cropping wasn't enough\n",
    "        # This might indicate a more fundamental issue with padding calculations for dL/dX\n",
    "        # For now, we'll force reshape if total elements match, or raise error\n",
    "        if gradient_wrt_input.size == batch_of_images.size:\n",
    "             gradient_wrt_input = gradient_wrt_input.reshape(batch_of_images.shape)\n",
    "        else:\n",
    "            raise ValueError(f\"Shape mismatch for dL/dX: expected {batch_of_images.shape}, got {gradient_wrt_input.shape}\")\n",
    "\n",
    "\n",
    "    # --- Gradient w.r.t. Kernel (dL/dW) ---\n",
    "    # dL/dW = X_col.T @ dLdZ_col\n",
    "    # X_col from original input batch_of_images (padded as in forward pass)\n",
    "    if padding > 0:\n",
    "        X_orig_padded_for_dW = np.pad(batch_of_images, ((0,0),(0,0),(padding,padding),(padding,padding)), mode='constant', constant_values=0)\n",
    "    else:\n",
    "        X_orig_padded_for_dW = batch_of_images\n",
    "        \n",
    "    # Im2col for X_orig_padded_for_dW\n",
    "    # Patches are (C_in_k, KH, KW), strided by `stride` (forward stride)\n",
    "    # Output of sliding_window_view (B, 1, H_out_fwd, W_out_fwd, 1, C_in_k, KH, KW)\n",
    "    H_out_fwd, W_out_fwd = dLdZ.shape[2], dLdZ.shape[3] # Spatial dims of dLdZ\n",
    "\n",
    "    view_X_dW = np.lib.stride_tricks.sliding_window_view(X_orig_padded_for_dW, (1, C_in_k, KH, KW))\n",
    "    # view_X_dW shape: (B, 1_dummy, H_unstrided, W_unstrided, 1_dummy, C_in_k, KH, KW)\n",
    "    X_col_windows_dW = view_X_dW[:, 0, ::stride, ::stride, 0, :, :, :] # Apply stride\n",
    "    # X_col_windows_dW shape: (B, H_out_fwd, W_out_fwd, C_in_k, KH, KW)\n",
    "    X_col_dW = X_col_windows_dW.reshape(B * H_out_fwd * W_out_fwd, C_in_k * KH * KW)\n",
    "\n",
    "    # dLdZ_col from dLdZ (not dilated for dW)\n",
    "    # Reshape dLdZ (B, C_out_k, H_out_fwd, W_out_fwd) to (B*H_out_fwd*W_out_fwd, C_out_k)\n",
    "    dLdZ_col_dW = dLdZ.transpose(0, 2, 3, 1).reshape(-1, C_out_k)\n",
    "    \n",
    "    gradient_wrt_kernels_flat = np.matmul(X_col_dW.T, dLdZ_col_dW) # (C_in_k*KH*KW, C_out_k)\n",
    "    \n",
    "    # Reshape to original kernel shape (C_out_k, C_in_k, KH, KW)\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(C_in_k, KH, KW, C_out_k).transpose(3,0,1,2)\n",
    "\n",
    "    if gradient_wrt_kernels.dtype != np.float32:\n",
    "        gradient_wrt_kernels = gradient_wrt_kernels.astype(np.float32)\n",
    "\n",
    "    return gradient_wrt_input.astype(np.float32), gradient_wrt_kernels, gradient_wrt_biases.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d96bd4",
   "metadata": {},
   "source": [
    "### Optimized versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e444f55",
   "metadata": {},
   "source": [
    "* #### Im2col optimized\n",
    "\n",
    "Optimization changes:\n",
    "1. Memory Layout and Copying (Implicit and Explicit):\n",
    "\n",
    "    * `im2col_convolution` (Forward):\n",
    "\n",
    "        * `sliding_windows = np.lib.stride_tricks.sliding_window_view(...)` creates a view, which is efficient. However, `sliding_windows.reshape((-1,(KH*KW*C_in)))` often creates a copy if the reshape isn`t compatible with the existing strides.\n",
    "\n",
    "        * `kernels.reshape((-1,(KH*KW*C_in))).transpose(1,0)`: The `transpose()` operation almost always creates a copy of the array with a new memory layout (from C-cont*iguous to F-contiguous or vice-versa, or a mix).\n",
    "\n",
    "        * `output = images_*dot_kernels.reshape(...).transpose(...)`: Again, `transpose()` likely creates a copy.\n",
    "\n",
    "    The optimized forward version (im2col_optimized) tries to manage these reshapes and transpositions more carefully. For instance, `W_col = kernels.reshape(kernels_number, -1).T` is a common and efficient way to get the kernel matrix for `X_col @ W_col`. It might also be making better use of views where possible, or ensuring C-contiguity before matmul.\n",
    "\n",
    "    * `im2col_gradient` (Backward):\n",
    "\n",
    "        * dL/dX: Similar to the forward pass, creating X_col_dX from dLdZ_dilated_padded_for_dX and W_col_dX_kernels from kernels_rot180 involves reshapes and transposes that can lead to copies.\n",
    "\n",
    "        * dL/dW: Creating X_col_dW from the padded input and dLdZ_col_dW from dLdZ also involves reshapes (e.g., dLdZ.transpose(0, 2, 3, 1).reshape(-1, C_out_k)) and transpositions (X_col_dW.T) that can create copies and be less cache-friendly for the matmul.\n",
    "\n",
    "        * `im2col_optimized_gradient`: This version is designed to minimize copies.\n",
    "\n",
    "2. NumPy`s Internal Optimizations and BLAS:\n",
    "\n",
    "    * `np.matmul` calls highly optimized BLAS (Basic Linear Algebra Subprograms) libraries (like MKL, OpenBLAS) for the actual matrix multiplication. These libraries are extremely fast.\n",
    "\n",
    "    * The efficiency of BLAS heavily depends on the memory layout of the input matrices. Matrices that are C-contiguous (for the first operand) and F-contiguous (for the second, or C-contiguous if transposed in the call) are generally fastest.\n",
    "\n",
    "    * The optimized versions are often structured to prepare data in these favorable layouts. The unoptimized im2col might be feeding `np.matmul` with arrays that have \"messier\" strides due to multiple prior operations, forcing `np.matmul` to internally create temporary, better-laid-out copies, which adds overhead.\n",
    "\n",
    "3. Looping vs. Vectorization (Subtleties):\n",
    "\n",
    "    While both im2col versions avoid Python loops for the core computation (unlike nested_loop_convolution), the way data is prepared for the final `np.matmul` can still involve less efficient NumPy operations in the unoptimized version compared to the more carefully constructed sequence in the optimized one. For example, multiple small reshape and transpose calls on potentially non-contiguous data can be slower than fewer, larger operations on contiguous blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daa1dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_optimized(batch_of_images, kernels, biases=None, padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "\n",
    "    # 1. Pad the input image\n",
    "    if padding > 0:\n",
    "        # Original used 'batch_of_images' name, implying potential re-binding.\n",
    "        # Using a distinct name for padded version internally for clarity.\n",
    "        batch_of_images_padded = np.pad(batch_of_images, ((0,0),(0,0),(padding,padding),(padding,padding)), mode='constant', constant_values=0)\n",
    "    else:\n",
    "        batch_of_images_padded = batch_of_images\n",
    "\n",
    "    batch_size, input_channels, image_height_padded, image_width_padded = batch_of_images_padded.shape\n",
    "\n",
    "    # 2. Extract sliding windows (im2col)\n",
    "    # Original: view_shape = (1, input_channels, kernel_height, kernel_width)\n",
    "    # Output of view: (B, 1_dummy_ch, H_unstrided, W_unstrided, 1_dummy_item, C_in, KH, KW)\n",
    "    sliding_windows_view = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width) # Item shape for window\n",
    "    )\n",
    "    # Apply stride: (B, 1, H_out, W_out, 1, C_in, KH, KW)\n",
    "    sliding_windows_strided = sliding_windows_view[:, :, ::stride, ::stride, :, :, :, :]\n",
    "    \n",
    "    # 3. Flatten sliding windows (X_col)\n",
    "    # Reshapes (B*1*H_out*W_out*1, KH*KW*C_in) -> (B*H_out*W_out, KH*KW*C_in)\n",
    "    X_col = sliding_windows_strided.reshape((-1, (kernel_height * kernel_width * input_channels)))\n",
    "\n",
    "    # Flatten kernels (W_col)\n",
    "    # Original: kernels.reshape((-1,(KH*KW*C_in))).transpose(1,0) -> (KH*KW*C_in, K_num)\n",
    "    W_col = kernels.reshape(kernels_number, -1).T\n",
    "\n",
    "    # 4. Matrix multiplication\n",
    "    images_dot_kernels = np.matmul(X_col, W_col)\n",
    "    # Original explicitly casted here: .astype(np.float32). If X_col, W_col are already f32, result usually is.\n",
    "    if images_dot_kernels.dtype != np.float32:\n",
    "        images_dot_kernels = images_dot_kernels.astype(np.float32)\n",
    "\n",
    "    # 5. Compute output dimensions (using padded dimensions, as in original)\n",
    "    # Note: Integer division `//` is safer here than relying on `int()`.\n",
    "    output_height = ((image_height_padded - kernel_height) // stride) + 1\n",
    "    output_width = ((image_width_padded - kernel_width) // stride) + 1\n",
    "    \n",
    "    # 6. Reshape output\n",
    "    # Original: images_dot_kernels.reshape(batch_size, output_width, output_height, kernels_number)\n",
    "    # This means (Batch, Width, Height, Channels_out)\n",
    "    output_reshaped = images_dot_kernels.reshape(batch_size, output_height, output_width, kernels_number)\n",
    "    \n",
    "    # 7. Transpose to (Batch, Channels_out, Height, Width)\n",
    "    # Original transpose (0,3,1,2) on (B, W, H, C) makes (B, C, W, H).\n",
    "    # To get (B, C, H, W) from (B, H_orig_idx, W_orig_idx, C), use (0,3,1,2)\n",
    "    output = output_reshaped.transpose(0, 3, 1, 2) # (B, C_out, H_out, W_out)\n",
    "    if output.dtype != np.float32: # Original also casted here.\n",
    "        output = output.astype(np.float32)\n",
    "\n",
    "    # 8. Add biases (replicating original's specific logic)\n",
    "    if biases.any() != 0: # This was the original condition.\n",
    "        bias_to_add = biases\n",
    "        if hasattr(biases, 'dtype') and biases.dtype != np.float32:\n",
    "            bias_to_add = biases.astype(np.float32) # Ensure bias is also f32\n",
    "        output = output + bias_to_add.reshape(1, -1, 1, 1)\n",
    "\n",
    "    # 9. Apply ReLU activation\n",
    "    # The mask should be based on the state *after* potential ReLU.\n",
    "    # The original mask creation was: `mask = np.copy(output); mask[mask > 0] = 1` (output here is post-relu)\n",
    "    # This was not a binary mask if output had negative values after (impossible post-relu) or if relu not applied.\n",
    "    # A standard binary mask is better and what user's provided \"optimized\" functions also used.\n",
    "    \n",
    "    if applyReLU:\n",
    "        output_activated = np.maximum(0, output)\n",
    "        mask = (output_activated > 0).astype(output.dtype) # Binary mask {0, 1}\n",
    "        output = output_activated\n",
    "    else:\n",
    "        mask = np.ones_like(output, dtype=output.dtype) # All pass if no ReLU\n",
    "        # output remains unchanged from bias addition\n",
    "        \n",
    "    return output, mask\n",
    "\n",
    "def im2col_gradient_optimized(\n",
    "    batch_of_images, # Original input to forward pass, unpadded\n",
    "    d_image, # Gradient from next layer (dL/dY_activated)\n",
    "    kernels,            # Kernels used in forward pass (C_out, C_in, KH, KW)\n",
    "    mask,               # ReLU mask from forward pass\n",
    "    padding,        # Padding used in forward pass\n",
    "    stride          # Stride used in forward pass\n",
    "    ):\n",
    "    \n",
    "    C_out_k, C_in_k, KH, KW = kernels.shape\n",
    "    B, C_in_img, H_in_img_orig, W_in_img_orig = batch_of_images.shape\n",
    "\n",
    "    # Backward ReLU: dL/dZ = dL/dOutput_activated * mask\n",
    "    dLdZ = np.multiply(d_image, mask)\n",
    "\n",
    "    ############ Gradient of Input Image (dX) - Faithfully following original structure ############\n",
    "    # 1. Dilate dLdZ (as in original for dX)\n",
    "    dLdZ_dilated_for_dX = dilate(dLdZ, stride)\n",
    "    \n",
    "    # 2. Pad dLdZ_dilated_for_dX (as in original for dX)\n",
    "    # Original padding: KH-1-padding (fwd), KW-1-padding (fwd)\n",
    "    # This padding aims to make the subsequent convolution \"full\" in a way that\n",
    "    # its output size matches the original input image size before forward padding.\n",
    "    pad_h_dx = max(0, KH - 1 - padding)\n",
    "    pad_w_dx = max(0, KW - 1 - padding)\n",
    "    dLdZ_padded_for_dX = np.pad(dLdZ_dilated_for_dX,\n",
    "                                ((0,0),(0,0),(pad_h_dx, pad_h_dx),(pad_w_dx, pad_w_dx)),\n",
    "                                mode='constant', constant_values=0)\n",
    "\n",
    "    # 3. Rotate kernels by 180 degrees\n",
    "    kernels_rot180 = np.rot90(kernels, 2, axes=(-2,-1)) # Shape (C_out_k, C_in_k, KH, KW)\n",
    "\n",
    "    # 4. Create X_col_dX from dLdZ_padded_for_dX\n",
    "    # Window shape for view is (1, C_out_k, KH, KW) as dLdZ has C_out_k channels. Stride is 1.\n",
    "    view_dX = np.lib.stride_tricks.sliding_window_view(dLdZ_padded_for_dX, (1, C_out_k, KH, KW))\n",
    "    X_col_dX = view_dX.reshape((-1, (C_out_k * KH * KW))) # (Num_Patches_dX, C_out_k*KH*KW)\n",
    "    \n",
    "    # 5. Create W_col_dX from rotated kernels\n",
    "    # Original: kernels_rot180.reshape((-1,(KW*KH*C_out_k))).transpose(1,0)\n",
    "    # This reshapes to (C_in_k, KW*KH*C_out_k) then transposes to (KW*KH*C_out_k, C_in_k)\n",
    "    W_col_dX = kernels_rot180.reshape(C_in_k, -1).T # Equivalent\n",
    "\n",
    "    # 6. Calculate gradient columns for dX\n",
    "    gradient_wrt_images_col = np.matmul(X_col_dX, W_col_dX) # (Num_Patches_dX, C_in_k)\n",
    "    if gradient_wrt_images_col.dtype != np.float32:\n",
    "        gradient_wrt_images_col = gradient_wrt_images_col.astype(np.float32)\n",
    "    \n",
    "    # 7. Reshape to original image dimensions\n",
    "    # Original: .transpose(1,0).reshape(batch_of_images.shape)\n",
    "    # This implies Num_Patches_dX * C_in_k == B * C_in_img * H_in_img_orig * W_in_img_orig total elements\n",
    "    # AND C_in_k == C_in_img.\n",
    "    # And Num_Patches_dX should correspond to B * H_in_img_orig * W_in_img_orig.\n",
    "    # A standard reshape would be:\n",
    "    # target_H_dx_out = H_in_img_orig # Output of this 'full' conv should be original input height\n",
    "    # target_W_dx_out = W_in_img_orig # Output of this 'full' conv should be original input width\n",
    "    try:\n",
    "        # Attempt standard reshape first assuming the convolution output matches original input spatial dims\n",
    "        gradient_wrt_images_reshaped = gradient_wrt_images_col.reshape(B, H_in_img_orig, W_in_img_orig, C_in_k)\n",
    "        gradient_wrt_images = gradient_wrt_images_reshaped.transpose(0,3,1,2) # (B, C_in_k, H_orig, W_orig)\n",
    "        # Ensure final shape matches original input image's shape exactly, cropping if necessary due to minor size mismatches.\n",
    "        if gradient_wrt_images.shape != batch_of_images.shape:\n",
    "            gradient_wrt_images = gradient_wrt_images[:, :, :H_in_img_orig, :W_in_img_orig]\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Standard dX reshape failed ({e}). Attempting original's transpose-reshape logic for dX. This may indicate an issue in expected output dimensions of the dX convolution.\")\n",
    "        grad_temp_transposed = gradient_wrt_images_col.transpose(1,0)\n",
    "        gradient_wrt_images = grad_temp_transposed.reshape(batch_of_images.shape)\n",
    "\n",
    "\n",
    "    ############ Gradient of Kernel (dW) - Using Standard Corrected Method ############\n",
    "    # The original dW logic was highly non-standard and likely incorrect.\n",
    "    # Using the standard X_col.T @ dLdZ_col method, which is generally correct\n",
    "    # and was also present in the \"optimized\" versions you were given.\n",
    "    \n",
    "    # 1. Pad original input batch_of_images as it was for the forward pass\n",
    "    if padding > 0:\n",
    "        X_orig_padded_for_dW = np.pad(batch_of_images, ((0,0),(0,0),(padding,padding),(padding,padding)), mode='constant', constant_values=0)\n",
    "    else:\n",
    "        X_orig_padded_for_dW = batch_of_images\n",
    "        \n",
    "    # 2. Create X_col_dW from X_orig_padded_for_dW\n",
    "    # Patches are of size (C_in_img, KH, KW), strided by `stride` (forward stride)\n",
    "    # H_out_fwd, W_out_fwd are spatial dimensions of dLdZ\n",
    "    H_out_fwd, W_out_fwd = dLdZ.shape[2], dLdZ.shape[3]\n",
    "\n",
    "    _view_X_dW = np.lib.stride_tricks.sliding_window_view(X_orig_padded_for_dW, (1, C_in_img, KH, KW))\n",
    "    # Apply forward stride\n",
    "    X_col_windows_dW = _view_X_dW[:, 0, ::stride, ::stride, 0, :, :, :]\n",
    "    # Reshape to (B*H_out_fwd*W_out_fwd, C_in_img*KH*KW)\n",
    "    X_col_dW = X_col_windows_dW.reshape(B * H_out_fwd * W_out_fwd, C_in_img * KH * KW)\n",
    "\n",
    "    # 3. Create dLdZ_col from dLdZ (not dilated for dW)\n",
    "    # Reshape to (B*H_out_fwd*W_out_fwd, C_out_k)\n",
    "    dLdZ_col_dW = dLdZ.transpose(0, 2, 3, 1).reshape(-1, C_out_k)\n",
    "    \n",
    "    # 4. Calculate dW_flat = X_col_dW.T @ dLdZ_col_dW\n",
    "    gradient_wrt_kernels_flat = np.matmul(X_col_dW.T, dLdZ_col_dW) # (C_in_img*KH*KW, C_out_k)\n",
    "    \n",
    "    # 5. Reshape dW_flat to original kernel shape (C_out_k, C_in_img, KH, KW)\n",
    "    # Current shape is (C_in_img*KH*KW, C_out_k). Reshape to (C_in_img, KH, KW, C_out_k) then transpose.\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(C_in_img, KH, KW, C_out_k).transpose(3,0,1,2)\n",
    "    if gradient_wrt_kernels.dtype != np.float32:\n",
    "        gradient_wrt_kernels = gradient_wrt_kernels.astype(np.float32)\n",
    "\n",
    "    ############### Gradient of Bias (db) - Corrected ###############\n",
    "    # Original summed d_image *after* it was potentially dilated for dX.\n",
    "    # Correct approach is to sum dLdZ *before* any dilation.\n",
    "    gradient_wrt_biases = dLdZ.sum(axis=(0,2,3)) # Sum over B, H, W of dLdZ\n",
    "    if gradient_wrt_biases.dtype != np.float32:\n",
    "        gradient_wrt_biases = gradient_wrt_biases.astype(np.float32)\n",
    "    \n",
    "    return gradient_wrt_images, gradient_wrt_kernels, gradient_wrt_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef184b5",
   "metadata": {},
   "source": [
    "Reading the extremely interesting article [Faster Matrix Multiplications in Numpy](https://www.benjaminjohnston.com.au/matmul) by Benjamin Johnson, we extracted fundamental tips and techniques that allowed us to significantly improve the performances of our im2col approach. Following the order of the steps in the article, we now will briefly present them and apply them to our code.\n",
    "\n",
    "1. **Use BLAS directly, instead of relying on BLAS-based operations of NumPy:** *\"BLAS is a high-performance matrix library. Even though NumPy uses BLAS, I've noticed performance can be improved by calling BLAS directly. Perhaps this is simply because using direct calls to BLAS forces you to shape your data ready for use with BLAS.<br>Replace numpy.matmul with `scipy.linalg.blas.sgemm(...)` for float32 matrix-matrix multiplication and `scipy.linalg.blas.sgemv(...)` for float32 matrix-vector multiplication.\"*\n",
    "2. **Use the fastest BLAS possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c4a34",
   "metadata": {},
   "source": [
    "* Direct call to underlying BLAS functions, without NumPy intermediation (works best if the tensors are really large):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3edc08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import sgemm\n",
    "\n",
    "def im2col_optimized_blas(batch_of_images, kernels, biases=None, padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "    if kernel_channels != input_channels:\n",
    "        raise ValueError(f\"Numero di canali del kernel ({kernel_channels}) non corrisponde ai canali dell'input ({input_channels})\")\n",
    "    output_height = (image_height - kernel_height + 2 * padding) // stride + 1\n",
    "    output_width = (image_width - kernel_width + 2 * padding) // stride + 1\n",
    "    \n",
    "    current_dtype = batch_of_images.dtype # Per mantenere la precisione originale finché possibile\n",
    "    if current_dtype != np.float32: # sgemm richiede float32\n",
    "        current_dtype = np.float32\n",
    "\n",
    "    if padding > 0:\n",
    "        batch_of_images_padded = np.pad(batch_of_images.astype(current_dtype, copy=False),\n",
    "                                        ((0, 0), (0, 0), (padding, padding), (padding, padding)),\n",
    "                                        mode='constant')\n",
    "    else:\n",
    "        batch_of_images_padded = batch_of_images.astype(current_dtype, copy=False)\n",
    "\n",
    "    patches = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )\n",
    "    patches_strided = patches[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col_shape = (batch_size * output_height * output_width, input_channels * kernel_height * kernel_width)\n",
    "    X_col_transposed_view = patches_strided.transpose(0, 2, 3, 5, 6, 7, 1, 4)\n",
    "    # Assicura C-contiguità per la matrice 'a' di sgemm\n",
    "    X_col = np.ascontiguousarray(X_col_transposed_view.reshape(X_col_shape), dtype=np.float32)\n",
    "\n",
    "    # Prepara i kernel per sgemm con trans_b=True\n",
    "    # kernels_reshaped avrà forma (OC, patch_size) ed è C-contigua\n",
    "    kernels_reshaped_for_sgemm = np.ascontiguousarray(kernels.reshape(kernels_number, -1), dtype=np.float32)\n",
    "\n",
    "    # sgemm calcola alpha * A @ B oppure A.T @ B ecc.\n",
    "    # Vogliamo X_col @ W_col, dove W_col = kernels.reshape(OC, -1).T\n",
    "    # Quindi, W_col ha forma (patch_size, OC).\n",
    "    # Passando kernels_reshaped_for_sgemm (OC, patch_size) e trans_b=True,\n",
    "    # sgemm calcolerà X_col @ kernels_reshaped_for_sgemm.T\n",
    "    output_col = sgemm(alpha=1.0, a=X_col, b=kernels_reshaped_for_sgemm, trans_b=True)\n",
    "\n",
    "    output = output_col.reshape(batch_size, output_height, output_width, kernels_number).transpose(0, 3, 1, 2)\n",
    "\n",
    "    if biases is not None and biases.any() != 0:\n",
    "        output = output + biases.astype(np.float32, copy=False).reshape(1, -1, 1, 1)\n",
    "\n",
    "    mask_type = output.dtype\n",
    "    if applyReLU:\n",
    "        # np.maximum può cambiare il dtype se uno degli argomenti è 0 (int). Forziamo float32.\n",
    "        output = np.maximum(0, output, dtype=np.float32)\n",
    "        mask = (output > 0).astype(mask_type) # Maschera binaria con il dtype originale (prima di forzare float32)\n",
    "    else:\n",
    "        mask = np.ones_like(output, dtype=mask_type)\n",
    "\n",
    "    return output.astype(np.float32, copy=False), mask.astype(np.float32, copy=False)\n",
    "\n",
    "def im2col_gradient_optimized_blas(original_forward_input, d_image, kernels, mask, padding, stride):\n",
    "    # Assicura che mask sia float32 se usata in moltiplicazioni\n",
    "    gradient_pre_activation = np.multiply(d_image, mask.astype(d_image.dtype, copy=False)) # dL/dZ\n",
    "\n",
    "    kernels_number, input_channels_kernel, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels_orig, input_height, input_width = original_forward_input.shape\n",
    "\n",
    "    gradient_wrt_biases = np.sum(gradient_pre_activation, axis=(0, 2, 3)).astype(np.float32, copy=False)\n",
    "\n",
    "    current_dtype = original_forward_input.dtype\n",
    "    if current_dtype != np.float32:\n",
    "        current_dtype = np.float32\n",
    "\n",
    "    if padding > 0:\n",
    "        X_padded_for_dW = np.pad(original_forward_input.astype(current_dtype, copy=False),\n",
    "                                 ((0,0), (0,0), (padding,padding), (padding,padding)),\n",
    "                                 mode='constant')\n",
    "    else:\n",
    "        X_padded_for_dW = original_forward_input.astype(current_dtype, copy=False)\n",
    "\n",
    "    patches_X_for_dW = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_padded_for_dW,\n",
    "        (1, input_channels_orig, kernel_height, kernel_width)\n",
    "    )[:, :, ::stride, ::stride, :, :, :, :]\n",
    "\n",
    "    X_col_for_dW_shape = (\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        input_channels_orig * kernel_height * kernel_width\n",
    "    )\n",
    "    X_col_for_dW_transposed_view = patches_X_for_dW.transpose(0, 2, 3, 5, 6, 7, 1, 4)\n",
    "    # Assicura C-contiguità per 'a' in sgemm(trans_a=True)\n",
    "    X_col_for_dW = np.ascontiguousarray(X_col_for_dW_transposed_view.reshape(X_col_for_dW_shape), dtype=np.float32)\n",
    "\n",
    "    dLdZ_col_shape = (\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        kernels_number\n",
    "    )\n",
    "    dLdZ_col_transposed_view = gradient_pre_activation.transpose(0, 2, 3, 1)\n",
    "    # Assicura C-contiguità per 'b' in sgemm(trans_a=True)\n",
    "    # (anche se F-contiguità sarebbe ideale per 'b' se 'a' non è trasposta,\n",
    "    # BLAS gestisce C-contigue per entrambe con trans_a=True)\n",
    "    dLdZ_col = np.ascontiguousarray(dLdZ_col_transposed_view.reshape(dLdZ_col_shape), dtype=np.float32)\n",
    "\n",
    "    # Calcola X_col_for_dW.T @ dLdZ_col\n",
    "    gradient_wrt_kernels_flat = sgemm(alpha=1.0, a=X_col_for_dW, b=dLdZ_col, trans_a=True)\n",
    "\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(\n",
    "        input_channels_orig, kernel_height, kernel_width, kernels_number\n",
    "    ).transpose(3, 0, 1, 2).astype(np.float32, copy=False)\n",
    "\n",
    "    # --- Calcolo di gradient_wrt_input (dL/dX) ---\n",
    "    dilated_grad_pre_activation = dilate(gradient_pre_activation.astype(np.float32, copy=False), stride)\n",
    "\n",
    "    # Ottimizzazione per rotazione kernel: W_rot180 = W[:, :, ::-1, ::-1] (flip per KH, KW)\n",
    "    # Poi transpose per scambiare canali input/output per la convoluzione trasposta: (IC, OC, KH, KW)\n",
    "    flipped_kernels_for_dX = kernels.astype(np.float32, copy=False)[:,:,::-1,::-1].transpose(1,0,2,3)\n",
    "    # Assicura che i kernel flippati siano C-contigui per la chiamata ricorsiva a im2col_optimized_v3\n",
    "    flipped_kernels_for_dX = np.ascontiguousarray(flipped_kernels_for_dX)\n",
    "\n",
    "\n",
    "    padding_for_dX_conv = kernel_height - 1 - padding\n",
    "\n",
    "    gradient_wrt_input_raw, _ = im2col_optimized_blas(\n",
    "        dilated_grad_pre_activation,\n",
    "        flipped_kernels_for_dX,\n",
    "        biases=None,\n",
    "        padding=padding_for_dX_conv,\n",
    "        stride=1,\n",
    "        applyReLU=False\n",
    "    )\n",
    "\n",
    "    # Crop per ottenere le dimensioni corrette\n",
    "    gradient_wrt_input = gradient_wrt_input_raw[:, :, :input_height, :input_width].astype(np.float32, copy=False)\n",
    "\n",
    "    return gradient_wrt_input, gradient_wrt_kernels, gradient_wrt_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf7862",
   "metadata": {},
   "source": [
    "## MLP Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249b2ef",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3f21f",
   "metadata": {},
   "source": [
    "`ReLU_SoftMax_FullyConnected` executes the forward pass of a two-layer Multi-Layer Perceptron (one hidden layer, one output layer), typically used for classification after feature extraction by convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68206d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3eaeb8",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7b0a1",
   "metadata": {},
   "source": [
    "`ReLU_SoftMax_FC_Backward` computes gradients for the MLP. Inputs: batch size `bs`, predictions `pred` ($P$), true `labels` ($Y$), weights $W_1, W_2$, hidden activation `fa` ($A_1$), hidden pre-activation `fl` ($Z_1$), MLP input `i_mlp` ($X_{mlp}$).\n",
    "\n",
    "**Gradients (from output layer backwards):**\n",
    "\n",
    "1.  $\\frac{\\partial L}{\\partial Z_2} = P - Y$ (`dL_dz2`)\n",
    "2.  $\\frac{\\partial L}{\\partial W_2} = A_1^T \\frac{\\partial L}{\\partial Z_2}$ (`dL_dw2`)\n",
    "3.  $\\frac{\\partial L}{\\partial b_2} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_2}$ (`dL_db2`)\n",
    "4.  $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} W_2^T$ (`dL_dfa`)\n",
    "5.  $\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\text{ReLU}'(Z_1)$ (`dL_dfl`, where $\\text{ReLU}'(Z_1)$ is 1 if $Z_1 > 0$, else 0)\n",
    "6.  $\\frac{\\partial L}{\\partial W_1} = X_{mlp}^T \\frac{\\partial L}{\\partial Z_1}$ (`dL_dw1`)\n",
    "7.  $\\frac{\\partial L}{\\partial b_1} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_1}$ (`dL_db1`)\n",
    "8.  $\\frac{\\partial L}{\\partial X_{mlp}} = \\frac{\\partial L}{\\partial Z_1} W_1^T$ (`dL_i_mlp`) (gradient to pass to conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10736bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd7105",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf96a8c",
   "metadata": {},
   "source": [
    "`crossEntropy` calculates the Categorical Cross-Entropy loss for a single sample.\n",
    "\n",
    "**Formula:** Given predicted probabilities $P=(p_1, ..., p_K)$ and one-hot true label $Y=(y_1, ..., y_K)$:\n",
    "$$ L(P, Y) = - \\sum_{k=1}^{K} y_k \\log(p_k) $$\n",
    "If class $c$ is the true class ($y_c=1$), $L = - \\log(p_c)$.\n",
    "A small epsilon (`1/100000`) is added to $p$ to prevent $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1291609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_numpy(x):\n",
    "    # x sono i logit, shape (N, C)\n",
    "    max_x = np.max(x, axis=-1, keepdims=True)\n",
    "    stable_x = x - max_x\n",
    "    # Aggiungi epsilon a np.sum per evitare log(0) se tutti gli exp sono 0 (improbabile con max sottratto)\n",
    "    log_sum_exp = np.log(np.sum(np.exp(stable_x), axis=-1, keepdims=True) + 1e-9) \n",
    "    return stable_x - log_sum_exp\n",
    "\n",
    "def nll_loss_numpy(log_probs, true_class_indices_flat):\n",
    "    # log_probs: output di log_softmax_numpy, shape (N, C)\n",
    "    # true_class_indices_flat: indici delle classi vere, shape (N,) es. np.array([idx1, idx2...])\n",
    "    N = log_probs.shape[0]\n",
    "    # Assicurati che true_class_indices_flat sia un array di interi per l'indicizzazione\n",
    "    return -log_probs[np.arange(N), true_class_indices_flat.astype(int)].mean()\n",
    "\n",
    "# def crossEntropy(logits, true_labels):\n",
    "#     return nll_loss_numpy(log_softmax_numpy(logits), true_labels)\n",
    "\n",
    "def crossEntropy(probabilities, true_labels):\n",
    "    return -np.sum(true_labels * np.log(probabilities + 1e-9)) / probabilities.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49079",
   "metadata": {},
   "source": [
    "## Inference: Comparing Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e3c9f",
   "metadata": {},
   "source": [
    "This section compares the inference performance and correctness of three CNN implementations: PyTorch, nested_loops, im2col and im2col_optimized. All use identical pre-trained weights.\n",
    "\n",
    "**Objectives:**\n",
    "1.  **Correctness:** Verify that all three models yield the same predictions.\n",
    "2.  **Performance measurement:** Compare average inference time per image.\n",
    "\n",
    "The loop iterates through test images, runs each model, records predictions and times. This demonstrates the efficiency gains from optimized libraries (PyTorch) and im2col approach over the naive loops one. The `padding` and `stride` parameters are set to match the PyTorch model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ab6c9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 20/20 [00:23<00:00,  1.19s/it, accuracy=100.00 %, im2col=0.0038 s, im2col_blas=0.0147 s, im2col_opt=0.0031 s, nested_loops=2.4757 s, pytorch=0.0045 s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average inference time in seconds:\n",
      "PyTorch:\t0.00214846134185791 s,\n",
      "im2col:\t\t0.0018279910087585448 s, \n",
      "im2col_optim:\t0.0014963626861572265 s, \n",
      "nested_loops:\t1.1760767459869386 s,\n",
      "im2col_blas:\t0.006983041763305664 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "pytorch_model",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0.0010156631469726562,
          0.0019457340240478516,
          0.0025026798248291016,
          0,
          0.0035181045532226562,
          0.002005338668823242,
          0.0009996891021728516,
          0.0020055770874023438,
          0.002997159957885742,
          0.0024080276489257812,
          0.0020041465759277344,
          0.0015039443969726562,
          0.0015239715576171875,
          0.004550933837890625,
          0.003515005111694336,
          0.002472400665283203,
          0.0029959678649902344,
          0.00099945068359375,
          0.0030045509338378906,
          0.0010008811950683594
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "nested_loops",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          1.951812982559204,
          1.1052265167236328,
          1.0736384391784668,
          1.1069872379302979,
          1.0937983989715576,
          1.0883080959320068,
          1.0827431678771973,
          1.0840754508972168,
          1.0873219966888428,
          1.086564064025879,
          1.100074052810669,
          1.1446995735168457,
          1.3711555004119873,
          1.323371171951294,
          1.248857021331787,
          1.1883916854858398,
          1.0887598991394043,
          1.1012423038482666,
          1.0898494720458984,
          1.1046578884124756
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "im2col",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.0010004043579101562,
          0.0028944015502929688,
          0.001965045928955078,
          0.0019998550415039062,
          0.0020051002502441406,
          0.0011293888092041016,
          0.002992391586303711,
          0.002998828887939453,
          0.0009970664978027344,
          0.0020020008087158203,
          0.0015172958374023438,
          0.0010001659393310547,
          0.002007007598876953,
          0.0010008811950683594,
          0.002000093460083008,
          0.0025305747985839844,
          0.001003265380859375,
          0.0010027885437011719,
          0.004513263702392578
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "im2col_optimized",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0.0025107860565185547,
          0.001322031021118164,
          0.001001596450805664,
          0.0009982585906982422,
          0.0010004043579101562,
          0.0010004043579101562,
          0.0010256767272949219,
          0.0021975040435791016,
          0.002000093460083008,
          0.0012919902801513672,
          0,
          0.0010044574737548828,
          0.0045146942138671875,
          0.0010004043579101562,
          0.0010004043579101562,
          0.0020596981048583984,
          0.0010006427764892578,
          0.0009999275207519531,
          0.0009984970092773438,
          0.0029997825622558594
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "im2col_optimized_blas",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0.0045070648193359375,
          0.00943303108215332,
          0.009018659591674805,
          0.019037246704101562,
          0.004002809524536133,
          0.0010004043579101562,
          0.006489992141723633,
          0.004511356353759766,
          0.007017612457275391,
          0.007002592086791992,
          0.008018255233764648,
          0.003512859344482422,
          0.009017705917358398,
          0.008035659790039062,
          0.003513336181640625,
          0.008002758026123047,
          0.0045108795166015625,
          0.006510257720947266,
          0.007999420166015625,
          0.00851893424987793
         ]
        }
       ],
       "layout": {
        "height": 500,
        "hovermode": "x unified",
        "legend": {
         "x": 0,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Inference Time Comparison (Interactive)"
        },
        "width": 1400,
        "xaxis": {
         "title": {
          "text": "Batch Index"
         }
        },
        "yaxis": {
         "title": {
          "text": "Inference Time (s)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics of Inference Times:\n",
      "         PyTorch  Nested Loops     im2col  im2col Optimized  \\\n",
      "count  20.000000     20.000000  20.000000         20.000000   \n",
      "mean    0.002148      1.176077   0.001828          0.001496   \n",
      "std     0.001094      0.200960   0.001020          0.000986   \n",
      "min     0.000000      1.073638   0.000000          0.000000   \n",
      "25%     0.001382      1.088062   0.001002          0.001000   \n",
      "50%     0.002005      1.100658   0.001982          0.001003   \n",
      "75%     0.002996      1.155623   0.002138          0.002015   \n",
      "max     0.004551      1.951813   0.004513          0.004515   \n",
      "\n",
      "       im2col Optimized BLAS  \n",
      "count              20.000000  \n",
      "mean                0.006983  \n",
      "std                 0.003655  \n",
      "min                 0.001000  \n",
      "25%                 0.004510  \n",
      "50%                 0.007010  \n",
      "75%                 0.008156  \n",
      "max                 0.019037  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyoff\n",
    "import pandas as pd\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"pytorch_model\"]=[]\n",
    "dict_times[\"nested_loops\"]=[]\n",
    "dict_times[\"im2col\"]=[]\n",
    "dict_times[\"im2col_optimized\"]=[]\n",
    "dict_times[\"im2col_optimized_blas\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"pytorch_model\"]=[]\n",
    "dict_pred[\"nested_loops\"]=[]\n",
    "dict_pred[\"im2col\"]=[]\n",
    "dict_pred[\"im2col_optimized\"]=[]\n",
    "dict_pred[\"im2col_optimized_blas\"]=[]\n",
    "\n",
    "# torch garbage collection\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "correct = 0\n",
    "count = 0\n",
    "\n",
    "skip = True\n",
    "loop = tqdm(range(0, test_labels.shape[0], 500), desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i].reshape(1,1,28,28)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, pred_pytorch = torch.max(outputs.data, 1)\n",
    "    dict_times[\"pytorch_model\"].append(end_time-start_time)\n",
    "    dict_pred[\"pytorch_model\"].append(np.array(pred_pytorch))\n",
    "\n",
    "    ############### CNN nested loops Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1s,mask1s = nested_loop_convolution(c0,np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2s,mask2s = nested_loop_convolution(c1s,np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3s,mask3s = nested_loop_convolution(c2s,np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    pred_nested_loops = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"nested_loops\"].append(end_time-start_time)\n",
    "    dict_pred[\"nested_loops\"].append(np.array(pred_nested_loops))\n",
    "\n",
    "    ############### CNN im2col Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = im2col_convolution(c0,np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2f,mask2f = im2col_convolution(c1f,np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3f,mask3f = im2col_convolution(c2f,np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    pred_im2col = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"im2col\"].append(end_time-start_time)\n",
    "    dict_pred[\"im2col\"].append(np.array(pred_im2col))\n",
    "\n",
    "    ############## CNN optimized im2col Implementation ###########\n",
    "    start_time = time.time()\n",
    "    c1c,mask1c = im2col_optimized(c0,np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2c,mask2c = im2col_optimized(c1c,np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3c,mask3c = im2col_optimized(c2c,np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpc = c3c.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpc,np_w1,np_b1,np_w2,np_b2)\n",
    "    pred_im2col_optim = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"im2col_optimized\"].append(end_time-start_time)\n",
    "    dict_pred[\"im2col_optimized\"].append(np.array(pred_im2col_optim))\n",
    "\n",
    "    ############## CNN BLAS im2col Implementation ###########\n",
    "    start_time = time.time()\n",
    "    c1c,mask1c = im2col_optimized_blas(c0,np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2c,mask2c = im2col_optimized_blas(c1c,np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3c,mask3c = im2col_optimized_blas(c2c,np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpc = c3c.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpc,np_w1,np_b1,np_w2,np_b2)\n",
    "    pred_im2col_blas = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"im2col_optimized_blas\"].append(end_time-start_time)\n",
    "    dict_pred[\"im2col_optimized_blas\"].append(np.array(pred_im2col_blas))\n",
    "\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    if pred_pytorch == pred_nested_loops and pred_pytorch == pred_im2col and pred_pytorch == pred_im2col_optim and pred_pytorch == pred_im2col_blas:\n",
    "        correct += 1\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    pytorch_avg_time = round(sum(dict_times['pytorch_model'])/(i+1),10) * 1000\n",
    "    nested_loops_avg_time = round(sum(dict_times['nested_loops'])/(i+1),10) * 1000\n",
    "    im2col_avg_time = round(sum(dict_times['im2col'])/(i+1),10) * 1000\n",
    "    im2col_optim_avg_time = round(sum(dict_times['im2col_optimized'])/(i+1),10) * 1000\n",
    "    im2col_blas_avg_time = round(sum(dict_times['im2col_optimized_blas'])/(i+1),10) * 1000\n",
    "\n",
    "    accuracy_percentage = (correct / count) * 100 if count > 0 else 0.0\n",
    "\n",
    "    loop.set_postfix(\n",
    "        pytorch=f\"{pytorch_avg_time:>8.4f} s\",\n",
    "        im2col=f\"{im2col_avg_time:>8.4f} s\",\n",
    "        im2col_opt=f\"{im2col_optim_avg_time:>8.4f} s\",\n",
    "        nested_loops=f\"{nested_loops_avg_time:>8.4f} s\",\n",
    "        im2col_blas=f\"{im2col_blas_avg_time:>8.4f} s\",\n",
    "        accuracy=f\"{accuracy_percentage:>7.2f} %\"\n",
    "    )\n",
    "\n",
    "sys.stdout.write(\"\\n\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "pytorch_avg_time = np.mean(dict_times['pytorch_model'])\n",
    "nested_loops_avg_time = np.mean(dict_times['nested_loops'])\n",
    "im2col_avg_time = np.mean(dict_times['im2col'])\n",
    "im2col_optim_avg_time = np.mean(dict_times['im2col_optimized'])\n",
    "im2col_blas_avg_time = np.mean(dict_times['im2col_optimized_blas'])\n",
    "\n",
    "print(f\"Average inference time in seconds:\\nPyTorch:\\t{pytorch_avg_time} s,\\nim2col:\\t\\t{im2col_avg_time} s, \\nim2col_optim:\\t{im2col_optim_avg_time} s, \\nnested_loops:\\t{nested_loops_avg_time} s,\\nim2col_blas:\\t{im2col_blas_avg_time} s\")\n",
    "\n",
    "# Plot times altogether\n",
    "\n",
    "# Create interactive traces for each method\n",
    "for method, times in dict_times.items():\n",
    "    trace = go.Scatter(\n",
    "        x=list(range(len(times))),\n",
    "        y=times,\n",
    "        mode='lines+markers',\n",
    "        name=method,\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=4)\n",
    "    )\n",
    "    if method == 'pytorch_model':\n",
    "        trace_pytorch = trace\n",
    "    elif method == 'nested_loops':\n",
    "        trace_nested_loops = trace\n",
    "    elif method == 'im2col':\n",
    "        trace_im2col = trace\n",
    "    elif method == 'im2col_optimized':\n",
    "        trace_im2col_optimized = trace\n",
    "    elif method == 'im2col_optimized_blas':\n",
    "        trace_blas = trace\n",
    "\n",
    "data = [trace_pytorch, trace_nested_loops, trace_im2col, trace_im2col_optimized, trace_blas]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Inference Time Comparison (Interactive)',\n",
    "    xaxis=dict(title='Batch Index'),\n",
    "    yaxis=dict(title='Inference Time (s)'),\n",
    "    width=1400,\n",
    "    height=500,\n",
    "    legend=dict(x=0, y=1),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "pyoff.iplot(fig)\n",
    "\n",
    "# Also plot some boxplots for better visualization\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.boxplot([dict_times['pytorch_model'], dict_times['im2col'], dict_times['im2col_optimized'], dict_times['im2col_optimized_blas']],\n",
    "            tick_labels=['PyTorch', 'im2col', 'im2col Optimized', 'im2col Optimized BLAS'],\n",
    "            showfliers=False)\n",
    "plt.ylabel('Inference Time (s)')\n",
    "plt.title('Boxplot of Inference Times')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.boxplot([dict_times['pytorch_model'], dict_times['im2col'], dict_times['im2col_optimized']],\n",
    "            tick_labels=['PyTorch', 'im2col', 'im2col Optimized'],\n",
    "            showfliers=False)\n",
    "plt.ylabel('Inference Time (s)')\n",
    "plt.title('Boxplot of Inference Times')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "df_times = pd.DataFrame({\n",
    "    'PyTorch': dict_times['pytorch_model'],\n",
    "    'Nested Loops': dict_times['nested_loops'],\n",
    "    'im2col': dict_times['im2col'],\n",
    "    'im2col Optimized': dict_times['im2col_optimized'],\n",
    "    'im2col Optimized BLAS': dict_times['im2col_optimized_blas']\n",
    "})\n",
    "print(\"\\nDescriptive Statistics of Inference Times:\")\n",
    "print(df_times.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abe784",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f8f80",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a476a",
   "metadata": {},
   "source": [
    "### NumPy Model Training: Weights Initialization\n",
    "\n",
    "For training our NumPy CNNs from scratch, weights and biases are initialized randomly.\n",
    "The shapes are taken from `numpy_weights` (derived from the PyTorch model) to maintain architectural consistency. `np.random.rand()` provides initial values (uniform in [0,1)). While more advanced initializers exist, this suffices for observing basic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "733935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "def initialize_weights():\n",
    "    weights = {}\n",
    "    weights['k1'] = np.random.rand(*numpy_weights['k1'].shape)\n",
    "    weights['bc1'] = np.random.rand(*numpy_weights['b_conv1'].shape)\n",
    "    weights['k2'] = np.random.rand(*numpy_weights['k2'].shape)\n",
    "    weights['bc2'] = np.random.rand(*numpy_weights['b_conv2'].shape)\n",
    "    weights['k3'] = np.random.rand(*numpy_weights['k3'].shape)\n",
    "    weights['bc3'] = np.random.rand(*numpy_weights['b_conv3'].shape)\n",
    "    weights['w1'] = np.random.rand(*numpy_weights['w1'].shape)\n",
    "    weights['b1'] = np.random.rand(*numpy_weights['b1'].shape)\n",
    "    weights['w2'] = np.random.rand(*numpy_weights['w2'].shape)\n",
    "    weights['b2'] = np.random.rand(*numpy_weights['b2'].shape)\n",
    "    return weights\n",
    "\n",
    "weights = initialize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb961207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested Loops (3 epochs): -0.000000\n",
      "Predict: 5\n",
      "Probabilities: [[0.08533674 0.08533674 0.08533674 0.08533674 0.08533674 0.23196934\n",
      "  0.08533674 0.08533674 0.08533674 0.08533674]]\n",
      "\n",
      "Im2Col: -0.000000\n",
      "Predict: 5\n",
      "Probabilities: [[0.08533674 0.08533674 0.08533674 0.08533674 0.08533674 0.23196934\n",
      "  0.08533674 0.08533674 0.08533674 0.08533674]]\n",
      "\n",
      "Im2Col Optimized: -0.000000\n",
      "Predict: 5\n",
      "Probabilities: [[0.08533674 0.08533674 0.08533674 0.08533674 0.08533674 0.23196934\n",
      "  0.08533674 0.08533674 0.08533674 0.08533674]]\n",
      "\n",
      "Im2Col Optimized BLAS: -0.000000\n",
      "Predict: 5\n",
      "Probabilities: [[0.08533674 0.08533674 0.08533674 0.08533674 0.08533674 0.23196934\n",
      "  0.08533674 0.08533674 0.08533674 0.08533674]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm # Ensure tqdm is imported\n",
    "\n",
    "# --- Helper: NumPy Single Image Training ---\n",
    "def train_single_image_numpy(\n",
    "    initial_weights_dict, \n",
    "    train_image_np, # (1, 1, 28, 28), normalized\n",
    "    train_label_np, # one-hot (1, 10)\n",
    "    forward_conv_fn, \n",
    "    backward_conv_fn, \n",
    "    num_epochs, \n",
    "    lr,\n",
    "    bs=1 # Batch size, fixed to 1 for single image training\n",
    "    ):\n",
    "\n",
    "    # Initialize weights (make copies to not modify the initial_weights_dict)\n",
    "    k1 = initial_weights_dict['k1'].copy().astype(np.float32)\n",
    "    bc1 = initial_weights_dict['bc1'].copy().astype(np.float32)\n",
    "    k2 = initial_weights_dict['k2'].copy().astype(np.float32)\n",
    "    bc2 = initial_weights_dict['bc2'].copy().astype(np.float32)\n",
    "    k3 = initial_weights_dict['k3'].copy().astype(np.float32)\n",
    "    bc3 = initial_weights_dict['bc3'].copy().astype(np.float32)\n",
    "    w1 = initial_weights_dict['w1'].copy().astype(np.float32)\n",
    "    b1 = initial_weights_dict['b1'].copy().astype(np.float32)\n",
    "    w2 = initial_weights_dict['w2'].copy().astype(np.float32)\n",
    "    b2 = initial_weights_dict['b2'].copy().astype(np.float32)\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        c1_out, mask1 = forward_conv_fn(train_image_np, k1, bc1, padding=0, stride=2, applyReLU=True)\n",
    "        c2_out, mask2 = forward_conv_fn(c1_out, k2, bc2, padding=1, stride=2, applyReLU=True)\n",
    "        c3_out, mask3 = forward_conv_fn(c2_out, k3, bc3, padding=0, stride=2, applyReLU=True)\n",
    "        \n",
    "        mlp_input = c3_out.reshape(bs, -1)\n",
    "        fl, fa, sl, sa = ReLU_SoftMax_FullyConnected(mlp_input, w1, b1, w2, b2)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa, train_label_np) # Ensure train_label_np is (1, 10)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        dL_i_mlp, dL_dw1, dL_db1, dL_dw2, dL_db2 = ReLU_SoftMax_FC_Backward(bs, sa, train_label_np, w1, w2, fa, fl, mlp_input)\n",
    "        dL_i_mlp_reshaped = dL_i_mlp.reshape(c3_out.shape)\n",
    "        \n",
    "        gi3, gk3, gb3 = backward_conv_fn(c2_out, dL_i_mlp_reshaped, k3, mask3, padding=0, stride=2)\n",
    "        gi2, gk2, gb2 = backward_conv_fn(c1_out, gi3, k2, mask2, padding=1, stride=2)\n",
    "        gi1, gk1, gb1 = backward_conv_fn(train_image_np, gi2, k1, mask1, padding=0, stride=2)\n",
    "        \n",
    "        # Weight update\n",
    "        w1  -= lr * dL_dw1\n",
    "        b1  -= lr * dL_db1.reshape(b1.shape) # Ensure b1 update matches b1 shape\n",
    "        w2  -= lr * dL_dw2\n",
    "        b2  -= lr * dL_db2.reshape(b2.shape) # Ensure b2 update matches b2 shape\n",
    "        k1  -= lr * gk1\n",
    "        bc1 -= lr * gb1.reshape(bc1.shape)\n",
    "        k2  -= lr * gk2\n",
    "        bc2 -= lr * gb2.reshape(bc2.shape)\n",
    "        k3  -= lr * gk3\n",
    "        bc3 -= lr * gb3.reshape(bc3.shape)\n",
    "        \n",
    "    return losses, {\"k1\":k1, \"k2\":k2, \"k3\":k3, \"w1\":w1, \"w2\":w2, \"bc1\":bc1, \"bc2\":bc2, \"bc3\":bc3, \"b1\":b1, \"b2\":b2} # Return final weights\n",
    "\n",
    "# --- Training Setup ---\n",
    "# Prepare single training image and label\n",
    "# Assuming train_images and train_labels are already loaded and one-hot encoded\n",
    "single_train_image_np = train_images[0].reshape(1, 1, 28, 28).astype(np.float32) / 255.0\n",
    "single_train_label_np = train_labels[0].reshape(1,10).astype(np.float32) # Ensure it's (1,10)\n",
    "\n",
    "single_train_image_torch = torch.from_numpy(single_train_image_np)\n",
    "# For CrossEntropyLoss, PyTorch can take one-hot targets if they have shape (N,C)\n",
    "# or class indices (N). Let's use one-hot to be consistent with NumPy loss.\n",
    "single_train_label_torch = torch.from_numpy(single_train_label_np)\n",
    "\n",
    "\n",
    "num_epochs_single_img = 5\n",
    "lr_single_img = 0.001\n",
    "\n",
    "all_losses = {}\n",
    "\n",
    "# --- Train Nested Loops NumPy Model ---\n",
    "# This will be very slow, consider reducing num_epochs_single_img for this one if needed for quick testing\n",
    "# print(\"Training Nested Loops NumPy model on a single image...\")\n",
    "nested_loops_losses, _ = train_single_image_numpy(weights, single_train_image_np, single_train_label_np, \n",
    "                                                nested_loop_convolution, nested_loop_gradient, \n",
    "                                                3, lr_single_img) # Reduced epochs for speed\n",
    "all_losses['Nested Loops (3 epochs)'] = nested_loops_losses\n",
    "# print(\"Done.\\n\")\n",
    "\n",
    "# --- Train Im2Col (Unoptimized) NumPy Model ---\n",
    "# print(\"Training Im2Col NumPy model on a single image...\")\n",
    "im2col_losses, _ = train_single_image_numpy(weights, single_train_image_np, single_train_label_np, \n",
    "                                       im2col_convolution, im2col_gradient, \n",
    "                                       num_epochs_single_img, lr_single_img)\n",
    "all_losses['Im2Col'] = im2col_losses\n",
    "# print(\"Done.\\n\")\n",
    "\n",
    "# --- Train Im2Col Optimized NumPy Model ---\n",
    "# print(\"Training Im2Col Optimized NumPy model on a single image...\")\n",
    "im2col_opt_losses, _ = train_single_image_numpy(weights, single_train_image_np, single_train_label_np, \n",
    "                                           im2col_optimized, im2col_gradient_optimized, \n",
    "                                           num_epochs_single_img, lr_single_img)\n",
    "all_losses['Im2Col Optimized'] = im2col_opt_losses\n",
    "# print(\"Done.\\n\")\n",
    "\n",
    "# --- Train Im2Col Optimized BLAS NumPy Model ---\n",
    "# print(\"Training Im2Col Optimized BLAS NumPy model on a single image...\")\n",
    "im2col_blas_losses, _ = train_single_image_numpy(weights, single_train_image_np, single_train_label_np, \n",
    "                                            im2col_optimized_blas, im2col_gradient_optimized_blas, \n",
    "                                            num_epochs_single_img, lr_single_img)\n",
    "all_losses['Im2Col Optimized BLAS'] = im2col_blas_losses\n",
    "# print(\"Done.\\n\")\n",
    "\n",
    "# --- Plotting Losses ---\n",
    "plt.figure(figsize=(7, 4))\n",
    "for method_name, losses_list in all_losses.items():\n",
    "    plt.plot(losses_list, label=method_name)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Single Image Training Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Compare final losses (approximate check) ---\n",
    "# print(\"\\nFinal losses after training on a single image:\")\n",
    "for method_name, losses_list in all_losses.items():\n",
    "    if losses_list: # Check if list is not empty\n",
    "        print(f\"{method_name}: {losses_list[-1]:.6f}\\nPredict: {np.argmax(single_train_label_np)}\\nProbabilities: {softmax(single_train_label_np)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f437e42",
   "metadata": {},
   "source": [
    "* ### Pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "950f7d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 306.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final logits: [ 1.3480205e+05  1.2573266e+05 -1.2074309e+09  1.2417880e+05\n",
      "  1.2127320e+05  1.2076914e+09  1.2308734e+05  1.2487083e+05\n",
      "  1.2810571e+05  1.2339884e+05]\n",
      "Final prediction: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Convert and reshape weights for PyTorch (Conv2d expects (out_channels, in_channels, H, W), Linear expects (out_features, in_features))\n",
    "k1 = torch.from_numpy(weights['k1'].astype(np.float32)).clone()\n",
    "bc1 = torch.from_numpy(weights['bc1'].astype(np.float32)).clone()\n",
    "k2 = torch.from_numpy(weights['k2'].astype(np.float32)).clone()\n",
    "bc2 = torch.from_numpy(weights['bc2'].astype(np.float32)).clone()\n",
    "k3 = torch.from_numpy(weights['k3'].astype(np.float32)).clone()\n",
    "bc3 = torch.from_numpy(weights['bc3'].astype(np.float32)).clone()\n",
    "w1 = torch.from_numpy(weights['w1'].astype(np.float32)).clone()\n",
    "if w1.ndim == 2:\n",
    "    w1 = w1\n",
    "elif w1.ndim == 1:\n",
    "    w1 = w1.unsqueeze(0)\n",
    "b1 = torch.from_numpy(weights['b1'].astype(np.float32)).clone().view(-1)\n",
    "w2 = torch.from_numpy(weights['w2'].astype(np.float32)).clone()\n",
    "if w2.ndim == 2:\n",
    "    w2 = w2\n",
    "elif w2.ndim == 1:\n",
    "    w2 = w2.unsqueeze(0)\n",
    "b2 = torch.from_numpy(weights['b2'].astype(np.float32)).clone().view(-1)\n",
    "\n",
    "pytorch_model = SimpleCNN(num_classes=10)\n",
    "pytorch_model.load_state_dict({\n",
    "    'conv1.weight': k1,\n",
    "    'conv1.bias': bc1,\n",
    "    'conv2.weight': k2,\n",
    "    'conv2.bias': bc2,\n",
    "    'conv3.weight': k3,\n",
    "    'conv3.bias': bc3,\n",
    "    'fc1.weight': w1.T,\n",
    "    'fc1.bias': b1,\n",
    "    'fc2.weight': w2.T,\n",
    "    'fc2.bias': b2\n",
    "})\n",
    "\n",
    "optimizer = torch.optim.SGD(pytorch_model.parameters(), lr=0.001)\n",
    "\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = torch.from_numpy(train_images[0].reshape(1,1,28,28).astype(np.float32) / 255.0)\n",
    "    \n",
    "    prediction = pytorch_model(c0)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()(prediction[0], torch.tensor(train_labels[0]))\n",
    "    avg_loss.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Weights update\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    if i == numEpochs - 1:\n",
    "        # Get the result before the last relu\n",
    "        logits = prediction[0].detach().numpy()\n",
    "    \n",
    "\n",
    "# Plot the loss\n",
    "print(f\"Final logits: {logits}\")\n",
    "print(f\"Final prediction: {np.argmax(logits)}\")\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31db12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b5032",
   "metadata": {},
   "source": [
    "### CuPy optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00419e35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcupy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# --- CuPy dilate function (similar to NumPy's) ---\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# --- CuPy dilate function (similar to NumPy's) ---\n",
    "def dilate_cupy(tensor_cp, stride):\n",
    "    if stride == 1:\n",
    "        return tensor_cp\n",
    "    batch_size, num_channels, height, width = tensor_cp.shape\n",
    "    dilated_height = height + (height - 1) * (stride - 1)\n",
    "    dilated_width = width + (width - 1) * (stride - 1)\n",
    "    dilated_tensor_cp = cp.zeros((batch_size, num_channels, dilated_height, dilated_width), dtype=tensor_cp.dtype)\n",
    "    dilated_tensor_cp[:, :, ::stride, ::stride] = tensor_cp\n",
    "    return dilated_tensor_cp\n",
    "\n",
    "# --- CuPy im2col_optimized_forward (inspired by your im2col_optimized) ---\n",
    "def im2col_cupy_forward(batch_of_images_cp, kernels_cp, biases_cp=None, padding=0, stride=1, applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels_cp.shape\n",
    "    \n",
    "    if padding > 0:\n",
    "        batch_of_images_padded_cp = cp.pad(batch_of_images_cp, \n",
    "                                         ((0,0),(0,0),(padding,padding),(padding,padding)), \n",
    "                                         mode='constant', constant_values=0)\n",
    "    else:\n",
    "        batch_of_images_padded_cp = batch_of_images_cp\n",
    "\n",
    "    batch_size, input_channels, image_height_padded, image_width_padded = batch_of_images_padded_cp.shape\n",
    "\n",
    "    # CuPy's as_strided is equivalent to np.lib.stride_tricks.sliding_window_view's underlying mechanism\n",
    "    # Output patch shape: (input_channels, kernel_height, kernel_width)\n",
    "    # Strides of batch_of_images_padded_cp\n",
    "    B_s, C_s, H_s, W_s = batch_of_images_padded_cp.strides\n",
    "    \n",
    "    output_height = (image_height_padded - kernel_height) // stride + 1\n",
    "    output_width  = (image_width_padded - kernel_width) // stride + 1\n",
    "\n",
    "    shape_col = (batch_size, output_height, output_width, input_channels, kernel_height, kernel_width)\n",
    "    strides_col = (B_s, H_s * stride, W_s * stride, C_s, H_s, W_s)\n",
    "    \n",
    "    X_col_view = cp.lib.stride_tricks.as_strided(batch_of_images_padded_cp, shape=shape_col, strides=strides_col)\n",
    "    X_col = X_col_view.reshape(batch_size * output_height * output_width, -1) # (B*H_out*W_out, C_in*KH*KW)\n",
    "\n",
    "    W_col = kernels_cp.reshape(kernels_number, -1).T # (C_in*KH*KW, K_num)\n",
    "    \n",
    "    images_dot_kernels_cp = cp.matmul(X_col, W_col) # (B*H_out*W_out, K_num)\n",
    "    \n",
    "    output_reshaped_cp = images_dot_kernels_cp.reshape(batch_size, output_height, output_width, kernels_number)\n",
    "    output_cp = output_reshaped_cp.transpose(0, 3, 1, 2) # (B, K_num, H_out, W_out)\n",
    "\n",
    "    if biases_cp is not None and biases_cp.any() != 0: # .any() is needed for CuPy\n",
    "        output_cp = output_cp + biases_cp.reshape(1, -1, 1, 1)\n",
    "\n",
    "    mask_cp = None\n",
    "    if applyReLU:\n",
    "        output_activated_cp = cp.maximum(0, output_cp)\n",
    "        mask_cp = (output_activated_cp > 0).astype(output_cp.dtype)\n",
    "        output_cp = output_activated_cp\n",
    "    else:\n",
    "        mask_cp = cp.ones_like(output_cp, dtype=output_cp.dtype)\n",
    "        \n",
    "    return output_cp, mask_cp\n",
    "\n",
    "# --- CuPy MLP Forward (Softmax needs CuPy implementation if not already done) ---\n",
    "def softmax_cupy(x_cp):\n",
    "    e_x_cp = cp.exp(x_cp - cp.max(x_cp, axis=-1, keepdims=True))\n",
    "    return e_x_cp / cp.sum(e_x_cp, axis=-1, keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected_cupy(input_array_cp, w1_cp, b1_cp, w2_cp, b2_cp):\n",
    "    fl_cp = cp.matmul(input_array_cp, w1_cp) + b1_cp\n",
    "    fa_cp = cp.maximum(0, fl_cp)\n",
    "    sl_cp = cp.matmul(fa_cp, w2_cp) + b2_cp\n",
    "    sa_cp = softmax_cupy(sl_cp)\n",
    "    return fl_cp, fa_cp, sl_cp, sa_cp\n",
    "\n",
    "# Note: CuPy backward pass (im2col_cupy_gradient) would be needed for training.\n",
    "# For inference-only comparison, we only need the forward pass.\n",
    "# If you need CuPy training, the im2col_gradient_optimized_blas structure can be adapted for CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f69dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
