{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42490d20",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "82b0d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "#-------------- Data Extraction ---------------------------\n",
    "\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcf859",
   "metadata": {},
   "source": [
    "## CNN - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c17803",
   "metadata": {},
   "source": [
    "The PyTorch model will be used as a reference to compute the weights since it's the fastest in training and the least prone to errors. If everything is written well, both slow and fast implementations of a CNN in numpy will give the same result, since the weights are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c11b9",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Flatten(start_dim=1, end_dim=-1)\n",
      "\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Tempo: 8.29s - Training Loss: 0.2951\n",
      "Epoch 1/5 - Test Loss: 0.0003 - Test Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Tempo: 8.79s - Training Loss: 0.0780\n",
      "Epoch 2/5 - Test Loss: 0.0004 - Test Accuracy: 99.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Tempo: 8.09s - Training Loss: 0.0531\n",
      "Epoch 3/5 - Test Loss: 0.0005 - Test Accuracy: 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Tempo: 8.08s - Training Loss: 0.0398\n",
      "Epoch 4/5 - Test Loss: 0.0003 - Test Accuracy: 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Tempo: 9.16s - Training Loss: 0.0324\n",
      "Epoch 5/5 - Test Loss: 0.0003 - Test Accuracy: 99.22%\n",
      "\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 1.------------------ CNN declaration -------------------\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        print(self.flatten)\n",
    "        fc_input_size = 128 * 3 * 3\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x6x6\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # Third Convolution: from 1x64x6x6 to 1x128x2x2\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9541aa5",
   "metadata": {},
   "source": [
    "### Weights extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5906ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten(start_dim=1, end_dim=-1)\n",
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "----------------------------------------\n",
      "[[[-0.50176585 -0.3435954 ]\n",
      "  [ 0.07998651 -0.02189105]]]\n",
      "----------------------------------------\n",
      "[[[-0.50176585 -0.3435954 ]\n",
      "  [ 0.07998651 -0.02189105]]]\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 3, 3), NumPy Shape=(64, 32, 3, 3)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 3, 3), NumPy Shape=(128, 64, 3, 3)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 1152), NumPy Shape=(1152, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pyt_k1_w = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pyt_k1_w\n",
    "print(\"----------------------------------------\")\n",
    "print(pyt_k1_w[0][:1])\n",
    "print(\"----------------------------------------\")\n",
    "print(numpy_weights['k1'][0][:1])\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pyt_k1_w.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pyt_k2_w = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pyt_k2_w\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pyt_k2_w.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pyt_k3_w = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pyt_k3_w\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pyt_k3_w.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pyt_w1 = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pyt_w1.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pyt_b1 = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pyt_b1.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pyt_w1.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pyt_b1.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pyt_w2 = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pyt_w2.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pyt_b2 = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pyt_b2.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pyt_w2.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pyt_b2.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']\n",
    "\n",
    "\n",
    "\n",
    "# [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]]\n",
    "\n",
    "\n",
    "#  [[[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]]\n",
    "\n",
    "\n",
    "\n",
    "#    [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]\n",
    "\n",
    "#   [[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba32882",
   "metadata": {},
   "source": [
    "## CNN - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1185e05",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b49fb",
   "metadata": {},
   "source": [
    "`np.pad()` takes as first argument the matrix to pad and as second argument a set of specification: for every dimension (in our case 4) it takes the number of paddings to add before and after the end of the dimension. If the objective is to pad only the image itself, which is found in the last two dimension, we should write:\n",
    "\n",
    "`np.pad(img9,((0,0),(0,0),(pad,pad),(pad,pad)))` \n",
    "\n",
    "since dimensions are: BATCH, CHANNELS, HEIGHT, WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0599825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "img9 = np.arange(1,37).reshape(2,2,3,3)\n",
    "pad_img9 = np.pad(img9,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(img9)\n",
    "print(pad_img9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b7c53",
   "metadata": {},
   "source": [
    "### Delating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce9344",
   "metadata": {},
   "source": [
    "`delateOne` adds one zero between each element in the matrix given in input. this is done to be able to do the backward phase with stride 1 even in the forward it was 2, by modifying the gradient of the output. Motivations will be better analyzed in the next sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7ee6997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delateOne(matrix):\n",
    "    indix = np.arange(1,matrix.shape[3])\n",
    "    matrix = np.insert(matrix,indix,0,3)\n",
    "    indix = np.arange(-(matrix.shape[-2]-1),0)\n",
    "    matrix = np.insert(matrix,indix,0,-2)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d1319",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c26aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[ 1.  2.  3.  4.]\n",
      "   [ 5.  6.  7.  8.]\n",
      "   [ 9. 10. 11. 12.]]]]\n",
      "-------ker-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[  5.  19.  13.]\n",
      "   [ 47.  95.  45.]]\n",
      "\n",
      "  [[ 10.  40.  30.]\n",
      "   [104. 232. 126.]]]]\n",
      "-------Conv PyTorch-------\n",
      "tensor([[[[  5.,  19.,  13.],\n",
      "          [ 47.,  95.,  45.]],\n",
      "\n",
      "         [[ 10.,  40.,  30.],\n",
      "          [104., 232., 126.]]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This is a PyTorch Convolution example to be used to check if the convolution implemented in both slow and fast approaches are correct\n",
    "\n",
    "class CustomConv(nn.Module):\n",
    "    def __init__(self, kernel: torch.Tensor, bias: torch.Tensor = None, \n",
    "                 stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernel.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(bias is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernel)\n",
    "            if bias is not None:\n",
    "                self.conv.bias.copy_(bias)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if bias is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "def Slow_ReLU_Conv(img,ker,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        out_ch, in_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = out_ch\n",
    "    else: # Backward case\n",
    "        in_ch, out_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = in_ch\n",
    "\n",
    "    # bias has shape out_ch, 1, 1. It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # number of channels taken in input by the kernel 'in_ch' \n",
    "    # must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    n_images, channels, i_height, i_width  = img.shape\n",
    "    ni_height = int(((i_height - k_height) / stride) + 1) # new image height # Padding is already added\n",
    "    ni_width = int(((i_width - k_width) / stride) + 1) # new image width\n",
    "    ni = np.zeros((n_images, out_ch, ni_height, ni_width)).astype(np.float32) # new image\n",
    "\n",
    "    if in_ch != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({in_ch}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for one_img in range(n_images):\n",
    "        for one_k_channel in range(nk_channel):\n",
    "            for i_nih in range(ni_height): # which cycles row by row of the new image\n",
    "                for i_niw in range(ni_width): # which cycles column by column of the new image\n",
    "                    current_sum = 0.0 # convolution sum for the specific output cell\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(channels): # channels == in_ch\n",
    "                        for i_kh in range(k_height):\n",
    "                            input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "                            for i_kw in range(k_width):\n",
    "                                input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "                                    input_val = img[one_img, channel, input_y, input_x]\n",
    "                                    kernel_val = ker[one_k_channel, channel, i_kh, i_kw]\n",
    "                                    current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "                    ni[one_img, one_k_channel, i_nih, i_niw] = current_sum\n",
    "    if bias.all() != 0:\n",
    "        bias = bias.reshape(bias.shape[0],1,1)\n",
    "        if bias.shape[0] != out_ch:\n",
    "            raise ValueError(f\"bias dimension ({bias.shape[0]}) doesn't match kernel's number of channels ({out_ch})\")\n",
    "        ni = ni + bias\n",
    "    ni = ni.astype(np.float32)\n",
    "    if applyReLU:\n",
    "        ni = np.maximum(0, ni)\n",
    "        mask = ni.copy()\n",
    "        mask[mask > 0] = 1\n",
    "        return ni,mask\n",
    "    else:\n",
    "        return ni\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "img = np.arange(1,3*4+1).reshape(1,1,3,4).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=1,stride=2)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "\n",
    "my_kernel = torch.from_numpy(ker).float()\n",
    "\n",
    "my_bias = torch.from_numpy(np.array([1,2])).float()\n",
    "\n",
    "modelC = CustomConv(kernel=my_kernel,bias=my_bias, stride=2, padding=1)\n",
    "\n",
    "# input di prova (batch=1, canali=1, H=5, W=5)\n",
    "x = torch.from_numpy(img)\n",
    "y = modelC(x)\n",
    "print(\"-------Conv PyTorch-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec0a01",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805b797",
   "metadata": {},
   "source": [
    "**Actors:**\n",
    "1. W is the kernel\n",
    "2. $\\delta$ is the gradient\n",
    "3. x is the input to the convolution layer during forward\n",
    "4. b is the bias\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Derive delta**\n",
    "\n",
    "Deriving delta with respect to ReLU activation consists in the hadamard product (element-wise product) of the gradient ($\\delta$) and the mask obtained at the forward step, that is, all the elements in the convolved image greater than zero are put to one, the rest is zero.\n",
    "$$\n",
    "\\delta^{(i)} = \\delta_{\\text{flat reshaped}} \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "- **Gradient with respect to W**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(x^{(i)}, \\delta)\n",
    "$$\n",
    "This convolution creates a matrix for every channel of input image $x^{i}$ and for every channel of output image $\\delta$, thus resulting in the correct number of channels\n",
    "\n",
    "- **Gradient w.r.t. the input \\( x \\)** (To go to the preceding layer):\n",
    "\n",
    "$$\n",
    "\\delta^{(i-1)} = \\text{Full\\_Convolution}(\\delta^{(i)}, W^{(i)})\n",
    "$$\n",
    "\n",
    "- **Gradient w.r.t the bias**\n",
    "\n",
    "Since the bias is added equally across the spatial dimensions of each output channel, the gradient is the sum of all elements in each output channel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{h,w} \\delta^{(i)}_{c,h,w}\n",
    "$$\n",
    "\n",
    "For batched inputs, sum also across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n,h,w} \\delta^{(i)}_{n,c,h,w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "5de649af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imAge: (1, 1, 7, 7)\n",
      "kerNel: (2, 1, 2, 2)\n",
      "dimAge: (1, 2, 4, 4)\n",
      "ggi: (1, 1, 7, 7)\n",
      "ggk: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "def Slow_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "    ############################################# Gradient of Input Image ####################################\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image delated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel 180 degrees rotation (flipped vertically and then horizontally)\n",
    "    # FullConvolution(d_imgDelated, Rotated180Deg(kernel)) with stride 1\n",
    "    out_ch, in_ch, k_height, k_width = ker.shape\n",
    "    batch_s, in_ch, img_height, img_width = img.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "\n",
    "    # Delating the gradient of output\n",
    "    if stride == 2:\n",
    "        d_img = delateOne(d_img)\n",
    "    elif stride > 2:\n",
    "        raise ValueError(f\"Stride greater than 2 is not acceptable\")\n",
    "    d_imgPadded = np.pad(d_img,((0,0),(0,0),(k_height-1-pad,k_height-1-pad),(k_width-1-pad,k_width-1-pad)))\n",
    "    batch_s, out_ch, dimg_height, dimg_width = d_img.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    ker180 = np.rot90(ker,2,(-2,-1))\n",
    "\n",
    "    # Computation\n",
    "    gi = np.zeros_like(img)\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(img_height):\n",
    "            for i_giw in range(img_width):\n",
    "                for i_outch in range(out_ch):\n",
    "                    for i_inch in range(in_ch):\n",
    "                        for i_kh in range(k_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(k_width):\n",
    "                                x = i_gih + i_kw\n",
    "\n",
    "                                if 0 <= y < d_imgPadded.shape[-2] and 0 <= x < d_imgPadded.shape[-1]:\n",
    "                                    input_val = d_imgPadded[bs,i_outch,y,x]\n",
    "                                    ker_val = ker180[i_outch,i_inch,i_kh,i_kw] \n",
    "                                else:\n",
    "                                    break\n",
    "                                current_sum += input_val*ker_val\n",
    "                    gi[bs,i_inch,i_gih,i_giw] = current_sum\n",
    "                    current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the delated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    gk = np.zeros_like(ker)\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(k_height):\n",
    "            for i_giw in range(k_width):\n",
    "                for i_inch in range(in_ch):\n",
    "                    for i_outch in range(out_ch):\n",
    "                        for i_kh in range(dimg_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(dimg_width):\n",
    "                                x = i_gih + i_kw\n",
    "                                if 0 <= y < img_height and 0 <= x < img_width:\n",
    "                                    input_val = img[bs,i_inch,y,x]\n",
    "                                    ker_val = d_img[bs,i_outch,i_kh,i_kw] \n",
    "                                    current_sum += input_val*ker_val\n",
    "                                else:\n",
    "                                    break\n",
    "                        gk[i_outch,i_inch,i_gih,i_giw] = current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gb = d_img.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "in_ch = 1\n",
    "out_ch = 2\n",
    "idim = 7\n",
    "kdim = 2\n",
    "imAge = np.arange(1,1*in_ch*idim*idim+1).reshape(1,in_ch,idim,idim)\n",
    "kerNel = np.arange(1,out_ch*in_ch*(kdim**2)+1).reshape(out_ch,in_ch,kdim,kdim)\n",
    "dimAge,mask = Slow_ReLU_Conv(imAge,kerNel,stride=2,pad=1) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "ggi,ggk,ggb = Slow_ReLU_Gradient(imAge,dimAge,kerNel,mask,stride=2,pad=1)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggk: {ggk.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "02f87035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# OLD APPROACH ######################\n",
    "# def Slow_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "#     \"\"\"\n",
    "#     Performs the backward pass of the convolution layer. It takes the original image, \n",
    "#     the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "#     It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "#     \"\"\" \n",
    "\n",
    "#     out_ch, in_ch, k_height, k_width  = ker.shape \n",
    "#                                             # Example #\n",
    "#     # Convolving an RGB image with 32 2x2 kernels will give a shape of (32, 3, 2, 2) to the kernel. #\n",
    "    \n",
    "#     n_images, channels, i_height, i_width  = img.shape\n",
    "#     n_images, dch, di_height, di_width  = d_img.shape\n",
    "    \n",
    "#     ni_height = (i_height-1)*stride-(2*pad)+k_height # new image height\n",
    "#     ni_width =  (i_width-1)*stride-(2*pad)+k_width # new image width\n",
    "#     height_to_pad = ni_height-i_height\n",
    "#     width_to_pad = ni_width-i_width\n",
    "#     d_img = np.multiply(d_img,mask)\n",
    "#     d_imgP = np.pad(d_img,((0,0),(0,0),(height_to_pad,height_to_pad),(width_to_pad,width_to_pad)))\n",
    "#     gi = np.zeros_like(img).astype(np.float32) # gradient of original image\n",
    "#     gk = np.zeros_like(ker).astype(np.float32) # gradient of kernel\n",
    "\n",
    "# ############################## Computing the gradient of the original image ######################################\n",
    "#     current_sum = 0.0 # convolution sum for the specific output cell\n",
    "#     for one_img in range(n_images):\n",
    "#         for channel in range(channels):\n",
    "#             for i_nih in range(i_height): # which cycles row by row of the new image\n",
    "#                 for i_niw in range(i_width): # which cycles column by column of the new image\n",
    "#                     # Convolution cycles\n",
    "#                     for one_k_channel in range(out_ch): # channels == out_ch\n",
    "#                         for i_kh in range(k_height):\n",
    "#                             input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "#                             for i_kw in range(k_width):\n",
    "#                                 input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "#                                 # check that everything stays in the measures\n",
    "#                                 if 0 <= input_y < d_imgP.shape[2] and 0 <= input_x < d_imgP.shape[3]:\n",
    "#                                     input_val = d_imgP[one_img, one_k_channel, input_y, input_x]\n",
    "#                                     kernel_val = ker[one_k_channel,channel, i_kh, i_kw]\n",
    "#                                     current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "#                     gi[one_img, channel, i_nih, i_niw] = current_sum\n",
    "#                     current_sum = 0.0\n",
    "    \n",
    "# ############################## Computing the gradient of the kernel ##############################################\n",
    "# # Need to convolve the gradient of the image with the original image, using the gradient of the image as the kernel and \n",
    "# # keeping the same stride and padding (Otherwise the kernel won't work)\n",
    "#     current_sum = 0.0\n",
    "#     for one_img in range(n_images):\n",
    "#         for in_k_ch in range(in_ch): # which in the example is 3\n",
    "#             for out_k_ch in range(out_ch): # which in the example is 32\n",
    "#                 for k_gh in range(k_height):\n",
    "#                     for k_gw in range(k_width):\n",
    "#                     # gk[out_k_ch,in_k_ch,k_gh,k_gw] = something\n",
    "#                         for i_dh in range(di_height):\n",
    "#                             input_y = (k_gh * stride) + i_dh # get the y location, the height\n",
    "#                             for i_dw in range(di_width):\n",
    "#                                 input_x = (k_gw * stride) + i_dw\n",
    "#                                 # check that everything stays in the measures\n",
    "#                                 if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "#                                     input_val = img[one_img, in_k_ch, input_y, input_x]\n",
    "#                                     kernel_val = d_img[one_img, out_k_ch, i_dh, i_dw]\n",
    "#                                     current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "#                         gk[out_k_ch, in_k_ch, k_gh, k_gw] += current_sum\n",
    "\n",
    "# ############################## Computing the gradient of the bias ##############################################\n",
    "#     gb = d_img.sum((0,-1,-2)) # sum over batch, height and width\n",
    "# ################################################### Return Results ###############################################\n",
    "#     return gi,gk,gb\n",
    "\n",
    "# img = np.arange(1,17).reshape(1,1,4,4)\n",
    "# ker = np.arange(1,9).reshape(2,1,2,2)\n",
    "# bias = np.array([1,1])\n",
    "# d_img,mask=Slow_ReLU_Conv(img,ker,bias)\n",
    "# print(\"-------------d_img--------------\")\n",
    "# d_img = d_img - 2\n",
    "# print(d_img)\n",
    "# print(d_img.shape)\n",
    "# print(\"--------------------------------\")\n",
    "# a,b,c = Slow_ReLU_Gradient(img,d_img,ker,mask)\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876559ef",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3469f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Conv Slow-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[  85.  170.  192.   94.]\n",
      "   [ 183.  357.  393.  187.]\n",
      "   [ 243.  465.  501.  235.]\n",
      "   [ 111.  206.  220.  100.]]\n",
      "\n",
      "  [[ 174.  363.  417.  215.]\n",
      "   [ 408.  838.  938.  476.]\n",
      "   [ 564. 1138. 1238.  620.]\n",
      "   [ 296.  591.  637.  317.]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[15011. 20885. 16057.]\n",
      "   [23607. 32231. 24383.]\n",
      "   [18779. 25317. 18937.]]\n",
      "\n",
      "  [[35636. 50230. 39354.]\n",
      "   [57176. 79176. 61088.]\n",
      "   [47692. 65286. 49882.]]]]\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Conv(batch_of_images,kernel,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    kc, ac, kw, kh = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    # im2col: Window creation\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    window_m = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))[:,:,::stride,::stride].reshape((-1,(kw*kh*nc))) # window matrix\n",
    "    # Convolution\n",
    "    kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)\n",
    "    c_m = (window_m @ kernel).astype(np.float32) # convolved image matrix\n",
    "    # ReLU activation\n",
    "    nih = int(((ih-kh) / stride) + 1) # new image height # Padding is already added\n",
    "    niw = int(((iw-kw) / stride) + 1) # new image width\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output_temp = c_m.reshape(bs, nih, niw, kc)\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    reshaped_correct_order = output_temp.transpose(0,3,1,2).astype(np.float32)\n",
    "    if bias.any() != 0:\n",
    "        reshaped_correct_order = (reshaped_correct_order + bias.reshape(1,-1,1,1))\n",
    "    if applyReLU:\n",
    "        reshaped_correct_order = np.maximum(0,reshaped_correct_order)\n",
    "    mask = np.copy(reshaped_correct_order)\n",
    "    mask[mask>0]=1\n",
    "    return reshaped_correct_order,mask\n",
    "\n",
    "\n",
    "\n",
    "img = np.arange(1,2*3*3+1).reshape(1,2,3,3).astype(np.float32)\n",
    "# print(\"-------img-------\")\n",
    "# print(img)\n",
    "ker = np.arange(1,16+1).reshape(2,2,2,2)\n",
    "# print(\"-------ker-------\")\n",
    "# print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(img,ker,bias,pad = 1,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)\n",
    "res,mask = Slow_ReLU_Conv(res,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(X_c,ker,bias,pad = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91cb44",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b7be149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------img--------------\n",
      "(1, 3, 28, 28)\n",
      "-------------ker--------------\n",
      "(32, 3, 3, 3)\n",
      "################################\n",
      "-------------d_img--------------\n",
      "(1, 32, 14, 14)\n",
      "************************************\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 7776 into shape (32,3,3,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(d_img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m************************************\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m a,b,c \u001b[38;5;241m=\u001b[39m \u001b[43mFast_ReLU_Gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43md_img\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mker\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------gi-----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[70], line 26\u001b[0m, in \u001b[0;36mFast_ReLU_Gradient\u001b[1;34m(batch_of_images, d_image, kernel, mask, pad, stride)\u001b[0m\n\u001b[0;32m     24\u001b[0m window_boi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mstride_tricks\u001b[38;5;241m.\u001b[39msliding_window_view(batch_of_images,(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,dh,dw))[:,:,::stride,::stride]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,(dw\u001b[38;5;241m*\u001b[39mdh\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m))) \u001b[38;5;66;03m# window matrix\u001b[39;00m\n\u001b[0;32m     25\u001b[0m d_image \u001b[38;5;241m=\u001b[39m d_image\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,(dw\u001b[38;5;241m*\u001b[39mdh\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m)))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m gk \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mwindow_boi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md_image\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# convolved image matrix\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m########################################## Gradient of Image ###################################################\u001b[39;00m\n\u001b[0;32m     29\u001b[0m gi,_ \u001b[38;5;241m=\u001b[39m Fast_ReLU_Conv(d_imgP,kernel\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m),stride \u001b[38;5;241m=\u001b[39m stride,pad\u001b[38;5;241m=\u001b[39mpad,applyReLU\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 7776 into shape (32,3,3,3)"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Gradient(batch_of_images,d_image,kernel,mask,pad=0,stride=1):\n",
    "    out_ch, in_ch, kh, kw = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    bs, nc, i_height,i_width = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "\n",
    "    batchSize, out_ch, dh, dw = d_image.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    ni_height = int(((i_height-1)*stride)+kh) # new image height\n",
    "    ni_width =  int(((i_width-1)*stride)+kw) # new image width\n",
    "    height_to_pad = (ni_height-dh)\n",
    "    width_to_pad = (ni_width-dw)\n",
    "\n",
    "    half_htp = height_to_pad//2\n",
    "    half_wtp = width_to_pad//2\n",
    "\n",
    "    d_image = np.multiply(d_image,mask)\n",
    "    d_imgP = np.pad(d_image,((0,0),(0,0),(half_htp,half_htp),(half_wtp,half_wtp)))\n",
    "\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    \n",
    "    ############################## Computing the gradient of the bias ##############################################\n",
    "    gb = d_image.sum((0,-1,-2)) # sum over batch, height and width\n",
    "\n",
    "    ########################################## Gradient of Kernel ###################################################\n",
    "    window_boi = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,1,dh,dw))[:,:,::stride,::stride].reshape((-1,(dw*dh*1))) # window matrix\n",
    "    d_image = d_image.reshape((-1,(dw*dh*1))).transpose(1,0)\n",
    "    gk = (window_boi @ d_image).transpose(1,0).reshape(out_ch, in_ch, kh, kw,).astype(np.float32) # convolved image matrix\n",
    "\n",
    "    ########################################## Gradient of Image ###################################################\n",
    "    gi,_ = Fast_ReLU_Conv(d_imgP,kernel.transpose(1,0,2,3),stride = stride,pad=pad,applyReLU=False)\n",
    "    # window_dboi = np.lib.stride_tricks.sliding_window_view(d_imgP,(1,out_ch,kh,kw))[:,:,::stride,::stride].reshape((-1,(kw*kh*out_ch))) # window matrix\n",
    "    # kernel = kernel.reshape((-1,(kw*kh*out_ch))).transpose(1,0)\n",
    "    # gi = (window_dboi @ kernel).reshape(bs, i_height, i_width, nc).transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "s = 2\n",
    "p = 1\n",
    "in_ch = 3\n",
    "out_ch = 32\n",
    "i_dim = 28\n",
    "k_dim = 3\n",
    "img = np.arange(1,i_dim*in_ch*i_dim+1).reshape(1,in_ch,i_dim,i_dim)\n",
    "ker = np.arange(1,out_ch*k_dim*in_ch*k_dim+1).reshape(out_ch,in_ch,k_dim,k_dim)\n",
    "bias = np.ones(out_ch)\n",
    "d_img,mask = Fast_ReLU_Conv(img,ker,bias,stride=s,pad=p)\n",
    "\n",
    "print(\"-------------img--------------\")\n",
    "#print(img)\n",
    "print(img.shape)\n",
    "print(\"-------------ker--------------\")\n",
    "#print(ker)\n",
    "print(ker.shape)\n",
    "print(\"################################\")\n",
    "print(\"-------------d_img--------------\")\n",
    "#print(d_img-2)\n",
    "print(d_img.shape)\n",
    "\n",
    "print(\"************************************\")\n",
    "a,b,c = Fast_ReLU_Gradient(img,d_img-2,ker,mask,stride=s,pad=p)\n",
    "print(\"-------------gi-----------------\")\n",
    "print(a.shape)\n",
    "print(\"--------------gk-----------------\")\n",
    "print(b.shape)\n",
    "print(\"--------------gb-----------------\")\n",
    "print(c.shape)\n",
    "\n",
    "####################### Expected result ##########################\n",
    "# [[[[ 1  2  3  4]\n",
    "#    [ 5  6  7  8]\n",
    "#    [ 9 10 11 12]\n",
    "#    [13 14 15 16]]]]\n",
    "# -------------d_img--------------\n",
    "# [[[[ 43.  53.  63.]\n",
    "#    [ 83.  93. 103.]\n",
    "#    [123. 133. 143.]]\n",
    "\n",
    "#   [[ 99. 125. 151.]\n",
    "#    [203. 229. 255.]\n",
    "#    [307. 333. 359.]]]]\n",
    "# (1, 2, 3, 3)\n",
    "# --------------------------------\n",
    "# [[[[ 964. 2034. 2494. 1246.]\n",
    "#    [2636. 5268. 6044. 2912.]\n",
    "#    [4332. 8372. 9148. 4320.]\n",
    "#    [2088. 3922. 4238. 1938.]]]]\n",
    "# [[[[ 6042.  6879.]\n",
    "#    [ 9390. 10227.]]]\n",
    "\n",
    "\n",
    "#  [[[15018. 17079.]\n",
    "#    [23262. 25323.]]]]\n",
    "# [ 837. 2061.]\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08df4505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7776/32/3/9/9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc8707",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70cbf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa\n",
    "\n",
    "#print(softmax([1,2,3,100000]))\n",
    "#print(softmax_no_NS([1,2,3,1000]))\n",
    "#r = np.array(np.array([1,2,777,2]))\n",
    "#print(softmax(r))\n",
    "#r = np.array((np.array([1,2,777,2]),np.array([1,2,777,2]),np.array([1,2,777,2])))\n",
    "#print(softmax(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108f244",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7942596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels[0:bs]\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d3b94",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb425b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(p,t):\n",
    "    # p stands for prediction and t stands for true label\n",
    "    # p = [0,0,1] and t = [1,0,0]\n",
    "    p = p+(1/100000) # for numerical stability\n",
    "    return -np.dot(t,np.log(p).T)\n",
    "\n",
    "#c = [1,1000000000000000,1,1]\n",
    "#c = softmax(c)\n",
    "#print(c)\n",
    "#c = crossEntropy(c,[0,1,0,0])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fd4f5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad189b98",
   "metadata": {},
   "source": [
    "In this section the three implementations will be compared in terms of time. Recall that all the predictions should be the same since the weights are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f3cf59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 100/100 [06:59<00:00,  4.20s/it, average_times=t: 0.0023 s, s: 4.1928 s, f: 0.0017 s, correct_predictions=100.0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward execution time in seconds: \n",
      "PyTorch: 0.0023 s, \n",
      "Slow: 4.1928 s, \n",
      "Fast: 0.0017 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "\n",
    "#length = test_labels.shape[0]\n",
    "length = 100\n",
    "correct = 0\n",
    "skip = True\n",
    "loop = tqdm(range(length),desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "    ############### CNN Slow Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=1,stride=2)\n",
    "    c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),np_k2,np_b_conv2,pad=0,stride=2)\n",
    "    c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted2 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "    ############### CNN Fast Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = Fast_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=1,stride=2)\n",
    "    c2f,mask2f = Fast_ReLU_Conv(c1f.astype(np.float32),np_k2,np_b_conv2,pad=0,stride=2)\n",
    "    c3f,mask3f = Fast_ReLU_Conv(c2f.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted3 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    t = int(predicted1[0])\n",
    "    s = int(predicted2[0])\n",
    "    f = int(predicted3[0])\n",
    "    if t == s and t == f:\n",
    "        correct+=1\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    tat = round(sum(dict_times['ctorch'])/(i+1),4)\n",
    "    sat = round(sum(dict_times['cslow'])/(i+1),4)\n",
    "    fat = round(sum(dict_times['cfast'])/(i+1),4)\n",
    "    loop.set_postfix(average_times =f\"t: {tat} s, s: {sat} s, f: {fat} s\" , correct_predictions=f\"{100*correct/(i+1)}%\")\n",
    "tat = round(sum(dict_times['ctorch'])/length,4)\n",
    "sat = round(sum(dict_times['cslow'])/length,4)\n",
    "fat = round(sum(dict_times['cfast'])/length,4)\n",
    "print(f\"Average forward execution time in seconds: \\nPyTorch: {tat} s, \\nSlow: {sat} s, \\nFast: {fat} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae1ea1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97018854",
   "metadata": {},
   "source": [
    "### Test for Slow approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b559791",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf57c6",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f3b00564",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c8760193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88167606",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe14aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "(1, 32, 14, 14)\n",
      "(1, 64, 6, 6)\n",
      "(1, 128, 2, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1152 is different from 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[288], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(c3s\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     23\u001b[0m imlps \u001b[38;5;241m=\u001b[39m c3s\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m fl,fa,sl,sa \u001b[38;5;241m=\u001b[39m \u001b[43mReLU_SoftMax_FullyConnected\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimlps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m sfte \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# slow forward time end\u001b[39;00m\n\u001b[0;32m     26\u001b[0m sft \u001b[38;5;241m=\u001b[39m sfte \u001b[38;5;241m-\u001b[39m sfts\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mReLU_SoftMax_FullyConnected\u001b[1;34m(input_array, w1, b1, w2, b2)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReLU_SoftMax_FullyConnected\u001b[39m(input_array,w1,b1,w2,b2):\n\u001b[1;32m----> 6\u001b[0m     fl \u001b[38;5;241m=\u001b[39m (\u001b[43minput_array\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m)\u001b[38;5;241m+\u001b[39mb1 \u001b[38;5;66;03m# first layer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     fa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m,fl) \u001b[38;5;66;03m# first activation: ReLU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     sl \u001b[38;5;241m=\u001b[39m (fa \u001b[38;5;241m@\u001b[39m w2)\u001b[38;5;241m+\u001b[39mb2 \u001b[38;5;66;03m# second layer\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1152 is different from 512)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToBeTrained = True\n",
    "if ToBeTrained:\n",
    "    avg_loss = []\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    numEpochs = 20\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    loop = tqdm(range(numEpochs))\n",
    "    for i in loop:\n",
    "        c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "        \n",
    "        # Forward\n",
    "        sfts = time.time() # slow forward time start\n",
    "        c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "        c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "        c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "        print(c0.shape)\n",
    "        print(c1s.shape)\n",
    "        print(c2s.shape)\n",
    "        print(c3s.shape)\n",
    "        imlps = c3s.reshape(1,-1)\n",
    "        fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "        sfte = time.time() # slow forward time end\n",
    "        sft = sfte - sfts\n",
    "        forward_time.append(sft)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa,train_labels[0])\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        sbts = time.time() # slow backward time start\n",
    "        dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "        dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "        gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "\n",
    "        gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "        gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "        sbte = time.time() # slow backward time end\n",
    "        sbt = sbte - sbts\n",
    "        backward_time.append(sbt)\n",
    "\n",
    "        # Weights update\n",
    "        w1 -= lr*dL_dw1\n",
    "        b1 -= lr*dL_db1\n",
    "        w2 -= lr*dL_dw2\n",
    "        b2 -= lr*dL_db2\n",
    "        k3 -= lr*gk3\n",
    "        k2 -= lr*gk2\n",
    "        k1 -= lr*gk1\n",
    "        bc3 -= lr*gb3.reshape(-1)\n",
    "        bc2 -= lr*gb2.reshape(-1)\n",
    "        bc1 -= lr*gb1.reshape(-1)\n",
    "        \n",
    "        if len(avg_loss) >= 2:\n",
    "            loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "    plt.plot(avg_loss)\n",
    "    plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdde0d",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 4.9017 s\n",
    "- average backward time : 22.5251 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Slow Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e8aa7",
   "metadata": {},
   "source": [
    "### Test for Fast approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84440e1b",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f98a2",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d54b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ea750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac1aa3",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0b604f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1152 is different from 2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[284], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m c3s,mask3s \u001b[38;5;241m=\u001b[39m Fast_ReLU_Conv(c2s\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),k3,bc3,pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m imlps \u001b[38;5;241m=\u001b[39m c3s\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m fl,fa,sl,sa \u001b[38;5;241m=\u001b[39m \u001b[43mReLU_SoftMax_FullyConnected\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimlps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m sfte \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# slow forward time end\u001b[39;00m\n\u001b[0;32m     20\u001b[0m sft \u001b[38;5;241m=\u001b[39m sfte \u001b[38;5;241m-\u001b[39m sfts\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mReLU_SoftMax_FullyConnected\u001b[1;34m(input_array, w1, b1, w2, b2)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReLU_SoftMax_FullyConnected\u001b[39m(input_array,w1,b1,w2,b2):\n\u001b[1;32m----> 6\u001b[0m     fl \u001b[38;5;241m=\u001b[39m (\u001b[43minput_array\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m)\u001b[38;5;241m+\u001b[39mb1 \u001b[38;5;66;03m# first layer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     fa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m,fl) \u001b[38;5;66;03m# first activation: ReLU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     sl \u001b[38;5;241m=\u001b[39m (fa \u001b[38;5;241m@\u001b[39m w2)\u001b[38;5;241m+\u001b[39mb2 \u001b[38;5;66;03m# second layer\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1152 is different from 2048)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = Fast_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=1,stride=2)\n",
    "    c2s,mask2s = Fast_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = Fast_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=1,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = Fast_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=1,stride=2)\n",
    "    print(c3s.shape)\n",
    "    print(gi3.shape)\n",
    "    print(c2s.shape)\n",
    "    print(gk3.shape)\n",
    "    print(gb3.shape)\n",
    "    print(bc3.shape)\n",
    "    gi2,gk2,gb2 = Fast_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "    gi1,gk1,gb1 = Fast_ReLU_Gradient(c0,gi2,k1,mask1s,pad=1,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdb31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51200"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "51200/32/64/5/5\n",
    "800*64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
