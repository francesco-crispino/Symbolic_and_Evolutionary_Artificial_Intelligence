{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42490d20",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b0d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "#-------------- Data Extraction ---------------------------\n",
    "\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images-idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcf859",
   "metadata": {},
   "source": [
    "## CNN - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c17803",
   "metadata": {},
   "source": [
    "The PyTorch model will be used as a reference to compute the weights since it's the fastest in training and the least prone to errors. If everything is written well, both slow and fast implementations of a CNN in numpy will give the same result, since the weights are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c11b9",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a1d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 1.------------------ CNN declaration -------------------\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # Second Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # Third Convolution\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        print(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9541aa5",
   "metadata": {},
   "source": [
    "### Weights extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5906ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "----------------------------------------\n",
      "[[[-0.06239345  0.16331542  0.28573602]\n",
      "  [ 0.299534    0.48019555  0.25194943]\n",
      "  [-0.24432278  0.3191273  -0.06802213]]]\n",
      "----------------------------------------\n",
      "[[[-0.06239345  0.16331542  0.28573602]\n",
      "  [ 0.299534    0.48019555  0.25194943]\n",
      "  [-0.24432278  0.3191273  -0.06802213]]]\n",
      "k1: PyTorch Shape=(32, 1, 3, 3), NumPy Shape=(32, 1, 3, 3)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 3, 3), NumPy Shape=(64, 32, 3, 3)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 3, 3), NumPy Shape=(128, 64, 3, 3)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pyt_k1_w = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pyt_k1_w\n",
    "print(\"----------------------------------------\")\n",
    "print(pyt_k1_w[0][:1])\n",
    "print(\"----------------------------------------\")\n",
    "print(numpy_weights['k1'][0][:1])\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pyt_k1_w.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pyt_k2_w = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pyt_k2_w\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pyt_k2_w.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pyt_k3_w = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pyt_k3_w\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pyt_k3_w.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pyt_w1 = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pyt_w1.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pyt_b1 = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pyt_b1.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pyt_w1.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pyt_b1.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pyt_w2 = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pyt_w2.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pyt_b2 = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pyt_b2.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pyt_w2.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pyt_b2.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']\n",
    "\n",
    "\n",
    "\n",
    "# [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]]\n",
    "\n",
    "\n",
    "#  [[[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]]\n",
    "\n",
    "\n",
    "\n",
    "#    [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]\n",
    "\n",
    "#   [[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba32882",
   "metadata": {},
   "source": [
    "## CNN - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d1319",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0599825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "img9 = np.arange(1,37).reshape(2,2,3,3)\n",
    "pad_img9 = np.pad(img9,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(img9)\n",
    "print(pad_img9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe4d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomConv(nn.Module):\n",
    "    def __init__(self, kernel: torch.Tensor, bias: torch.Tensor = None, \n",
    "                 stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        # kernel deve avere shape (out_ch, in_ch, k_h, k_w)\n",
    "        out_ch, in_ch, k_h, k_w = kernel.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # definiamo un Conv2d “vuoto” senza parametri\n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(bias is not None))\n",
    "        \n",
    "        # sostituiamo i parametri con quelli che vogliamo\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernel)\n",
    "            if bias is not None:\n",
    "                self.conv.bias.copy_(bias)\n",
    "        \n",
    "        # blocchiamo l’aggiornamento se non vogliamo fare training\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if bias is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "# Esempio di utilizzo\n",
    "# supponiamo di avere un kernel 1×1×3×3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9d68306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[ 1  2  3  4]\n",
      "   [ 5  6  7  8]\n",
      "   [ 9 10 11 12]\n",
      "   [13 14 15 16]]]]\n",
      "-------ker-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------res-------\n",
      "[[[[ 45.  55.  65.]\n",
      "   [ 85.  95. 105.]\n",
      "   [125. 135. 145.]]\n",
      "\n",
      "  [[102. 128. 154.]\n",
      "   [206. 232. 258.]\n",
      "   [310. 336. 362.]]]]\n",
      "------mask-------\n",
      "[[[[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]]]\n",
      "tensor([[[[ 45.,  55.,  65.],\n",
      "          [ 85.,  95., 105.],\n",
      "          [125., 135., 145.]],\n",
      "\n",
      "         [[102., 128., 154.],\n",
      "          [206., 232., 258.],\n",
      "          [310., 336., 362.]]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1,educational=False):\n",
    "    out_ch, in_ch, k_width, k_height = ker.shape\n",
    "    bias = bias.reshape(bias.shape[0],1,1)\n",
    "    # bias has shape out_ch, 1, 1. It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # number of channels taken in input by the kernel 'in_ch' \n",
    "    # must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    n_images, channels, i_width, i_height = img.shape\n",
    "    ni_height = int(((i_height - k_height) / stride) + 1) # new image height # Padding is already added\n",
    "    ni_width = int(((i_width - k_width) / stride) + 1) # new image width\n",
    "    ni = np.zeros((n_images, out_ch, ni_height, ni_width)) # new image\n",
    "\n",
    "    if in_ch != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({in_ch}) must be the same as the number of channels of the image ({channels})\")\n",
    "    if bias.shape[0] != out_ch:\n",
    "        raise ValueError(f\"bias dimension ({bias.shape[0]}) doesn't match kernel's number of channels ({out_ch})\")\n",
    "\n",
    "    for one_img in range(n_images):\n",
    "        for one_k_channel in range(out_ch):\n",
    "            for i_nih in range(ni_height): # which cycles row by row of the new image\n",
    "                for i_niw in range(ni_width): # which cycles column by column of the new image\n",
    "                    current_sum = 0.0 # convolution sum for the specific output cell\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(channels): # channels == in_ch\n",
    "                        for i_kh in range(k_height):\n",
    "                            input_y = (i_nih * stride) - pad + i_kh # get the y location, the height\n",
    "                            for i_kw in range(k_width):\n",
    "                                input_x = (i_niw * stride) - pad + i_kw # get the x location, the width\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "                                    input_val = img[one_img, channel, input_y, input_x]\n",
    "                                    kernel_val = ker[one_k_channel, channel, i_kh, i_kw]\n",
    "                                    current_sum += input_val * kernel_val\n",
    "                                    if educational:\n",
    "                                        with open(\"Convolution behaviour.txt\",\"a\") as f:\n",
    "                                            f.write(\"-----------In--------\\n\")\n",
    "                                            f.write(f\"actual input channel:{channel}\\n\")\n",
    "                                            f.write(f\"actual kernel channel:{one_k_channel}\\n\")\n",
    "                                            f.write(f\"img value:{input_val}\\n\")\n",
    "                                            f.write(f\"ker value:{kernel_val}\\n\")\n",
    "                                            f.write(f\"current_sum value:{current_sum}\\n\")\n",
    "                                            f.write(\"-----------Out--------\\n\")\n",
    "                    ni[one_img, one_k_channel, i_nih, i_niw] = current_sum\n",
    "    ni = ni + bias\n",
    "    ni = np.maximum(0, ni)\n",
    "    mask = ni.copy()\n",
    "    mask[mask > 0] = 1\n",
    "    return ni,mask\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "img = np.arange(1,4*4+1).reshape(1,1,4,4)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias)\n",
    "print(\"-------res-------\")\n",
    "print(res)\n",
    "print(\"------mask-------\")\n",
    "print(mask)\n",
    "\n",
    "\n",
    "my_kernel = torch.from_numpy(ker).float()\n",
    "\n",
    "my_bias = torch.from_numpy(np.array([1,2])).float()\n",
    "\n",
    "modelC = CustomConv(kernel=my_kernel,bias=my_bias, stride=1, padding=0)\n",
    "\n",
    "# input di prova (batch=1, canali=1, H=5, W=5)\n",
    "x = torch.arange(1,4*4+1, dtype=torch.float32).reshape(1,1,4,4)\n",
    "y = modelC(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876559ef",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3469f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image\n",
      "[[[[ 1  2  3  4]\n",
      "   [ 5  6  7  8]\n",
      "   [ 9 10 11 12]\n",
      "   [13 14 15 16]]\n",
      "\n",
      "  [[17 18 19 20]\n",
      "   [21 22 23 24]\n",
      "   [25 26 27 28]\n",
      "   [29 30 31 32]]]]\n",
      "Kernel\n",
      "[[[[1. 1.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [1. 1.]]]]\n",
      "Convolved Image\n",
      "[[[[ 93. 109.]\n",
      "   [157. 173.]]\n",
      "\n",
      "  [[ 94. 110.]\n",
      "   [158. 174.]]]]\n",
      "Mask\n",
      "[[[[1. 1.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [1. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Conv(batch_of_images,kernel,bias,pad=0,stride=1):\n",
    "    ac, kc, kw, kh = kernel.shape # kernel width, height and number of channels\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    #p = 0 # padding\n",
    "    #s = 1 # stride\n",
    "    # im2col: Window creation\n",
    "    window_m = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))[:,:,::stride,::stride].reshape(-1,(kw*kh*nc)) # window matrix\n",
    "    # Convolution\n",
    "    kernel = kernel.reshape((kw*kh*nc),-1)\n",
    "    c_m = window_m @ kernel # convolved image matrix\n",
    "    # ReLU activation\n",
    "    r_c_m = np.maximum(0,c_m) # convolved image matrix after ReLU activation\n",
    "    \n",
    "    niw = round(((iw-kw+(2*pad))/stride)+1) # new image width\n",
    "    nih = round(((ih-kh+(2*pad))/stride)+1) # new image height\n",
    "    \n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output_temp = r_c_m.reshape(bs, nih, niw, kc)\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    reshaped_correct_order = output_temp.transpose(0, 3, 1, 2)\n",
    "    reshaped_correct_order = reshaped_correct_order + bias\n",
    "    mask = np.copy(reshaped_correct_order)\n",
    "    mask[mask>0]=1\n",
    "    return reshaped_correct_order,mask\n",
    "\n",
    "\n",
    "\n",
    "X=np.arange(1,2*4*4+1).reshape(1,2,4,4)\n",
    "k = np.ones(2*2*2*2).reshape(2,2,2,2)\n",
    "bias = np.array([1,2]).reshape(1,2,1,1)\n",
    "X_c,mask = Fast_ReLU_Conv(X,k,bias,stride=2)\n",
    "print(\"Input Image\")\n",
    "print(X)\n",
    "print(\"Kernel\")\n",
    "print(k)\n",
    "print(\"Convolved Image\")\n",
    "print(X_c)\n",
    "print(\"Mask\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e261bcd",
   "metadata": {},
   "source": [
    "### Slow Convolution layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805b797",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "1. W is the kernel\n",
    "2. $\\delta$ is the gradient\n",
    "3. x is the input to the convolution layer during forward\n",
    "4. bias b\n",
    "- **Derive delta**\n",
    "\n",
    "Deriving delta with respect to ReLU activation consists in the hadamard product (element-wise product) of the gradient ($\\delta$) and the mask obtained at the forward step, that is, all the elements in the convolved image greater than zero are put to one, the rest is zero.\n",
    "$$\n",
    "\\delta^{(i)} = \\delta_{\\text{flat reshaped}} \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "- **Gradient with respect to W**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(x^{(i)}, \\delta)\n",
    "$$\n",
    "\n",
    "- **Gradient w.r.t. the input \\( x \\)** (To go to the preceding layer):\n",
    "\n",
    "$$\n",
    "\\delta^{(i-1)} = \\text{Transposed\\_Convolution}(\\delta^{(i)}, W^{(i)})\n",
    "$$\n",
    "\n",
    "<img src=\"MNIST\\tc2.png\" style=\"width:600px; left:1000px\">\n",
    "\n",
    "- **Gradient w.r.t the bias**\n",
    "\n",
    "Since the bias is added equally across the spatial dimensions of each output channel, the gradient is the sum of all elements in each output channel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{h,w} \\delta^{(i)}_{c,h,w}\n",
    "$$\n",
    "\n",
    "For batched inputs, sum also across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n,h,w} \\delta^{(i)}_{n,c,h,w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89be3c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 1]\n",
      "  [2 3]]]\n",
      "[[[1 2]\n",
      "  [3 4]]]\n",
      "k 0 and l 0\n",
      "m 0 and n 0\n",
      "k 0 and l 2\n",
      "m 0 and n 1\n",
      "k 2 and l 0\n",
      "m 1 and n 0\n",
      "k 2 and l 2\n",
      "m 1 and n 1\n",
      "[[[ 0.  0.  1.  2.]\n",
      "  [ 0.  0.  3.  4.]\n",
      "  [ 2.  4.  3.  6.]\n",
      "  [ 6.  8.  9. 12.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "stride = 2\n",
    "pad = 0\n",
    "i_height=4\n",
    "i_width=4\n",
    "k_height=2\n",
    "k_width=2\n",
    "channels=1\n",
    "d_oldImage = np.zeros(channels*(i_width+(2*pad))*(i_height+(2*pad))).reshape(channels,i_width+(2*pad),i_height+(2*pad))\n",
    "ker = np.arange(1,channels*k_width*k_height+1).reshape(channels,k_width,k_height)\n",
    "ni_height = int(((i_height - k_height + (2 * pad)) / stride) + 1) # new image height\n",
    "ni_width = int(((i_width - k_width + (2 * pad)) / stride) + 1) # new image width\n",
    "d_newImage = np.arange(0,ni_height*ni_width).reshape(channels,ni_height,ni_width)\n",
    "print(d_newImage)\n",
    "print(ker)\n",
    "for c in range(channels):\n",
    "    for k in range(0,d_oldImage.shape[1],stride): #width\n",
    "        for l in range(0,d_oldImage.shape[2],stride): #height\n",
    "            print(f\"k {k} and l {l}\")\n",
    "            m = k//stride\n",
    "            n = l//stride\n",
    "            print(f\"m {m} and n {n}\")\n",
    "            for i in range(ker.shape[1]): #width\n",
    "                for j in range(ker.shape[2]): #height\n",
    "                    d_oldImage[c][k+i][l+j]+=ker[c][i][j]*d_newImage[c][m][n]\n",
    "\n",
    "print(d_oldImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc8707",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70cbf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa\n",
    "\n",
    "#print(softmax([1,2,3,100000]))\n",
    "#print(softmax_no_NS([1,2,3,1000]))\n",
    "#r = np.array(np.array([1,2,777,2]))\n",
    "#print(softmax(r))\n",
    "#r = np.array((np.array([1,2,777,2]),np.array([1,2,777,2]),np.array([1,2,777,2])))\n",
    "#print(softmax(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d3b94",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb425b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(p,t):\n",
    "    # p stands for prediction and t stands for true label\n",
    "    # p = [0,0,1] and t = [1,0,0]\n",
    "    p = p+(1/100000) # for numerical stability\n",
    "    return -np.dot(t,np.log(p).T)\n",
    "\n",
    "#c = [1,1000000000000000,1,1]\n",
    "#c = softmax(c)\n",
    "#print(c)\n",
    "#c = crossEntropy(c,[0,1,0,0])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fd4f5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad189b98",
   "metadata": {},
   "source": [
    "In this section the three implementations will be compared in terms of time. Recall that all the predictions should be the same since the weights are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3cf59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  5.4781,   0.0000,   0.0000,  ..., 485.8404, 937.6006,   1.3311]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...:   0%|          | 0/10000 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imlp\n",
      "shape(1, 2048)\n",
      "[[6.06264427e-03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  3.00637240e+02 0.00000000e+00]]\n",
      "PREDICTIONS\n",
      "tensor([7])\n",
      "[2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np_k1 = numpy_weights['k1']\n",
    "print(np_k1.shape)\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "length = test_labels.shape[0]\n",
    "\n",
    "for i in tqdm(range(length),desc=\" Inferring...\"):\n",
    "    c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "    ############### CNN Slow Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1,mask1 = Slow_ReLU_Conv(c0,np_k1,np_b_conv1,pad=1,stride=2)\n",
    "    c2,mask2 = Slow_ReLU_Conv(c1,np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3,mask3 = Slow_ReLU_Conv(c2,np_k3,np_b_conv3,pad=1,stride=2)\n",
    "    imlp = c3.reshape(1,-1)\n",
    "    print(\"imlp\")\n",
    "    print(f\"shape{imlp.shape}\")\n",
    "    print(imlp)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlp,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted2 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "    ############### CNN Fast Implementation #####################\n",
    "    # start_time = time.time()\n",
    "    # c1,mask1 = Fast_ReLU_Conv(c0,np_k1,np_b_conv1,pad=1,stride=2)\n",
    "    # c2,mask2 = Fast_ReLU_Conv(c1,np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    # c3,mask3 = Fast_ReLU_Conv(c2,np_k3,np_b_conv3,pad=1,stride=2)\n",
    "    # imlp = c3.reshape(1,-1)\n",
    "    # _,_,_,res = ReLU_SoftMax_FullyConnected(imlp,np_w1,np_b1,np_w2,np_b2)\n",
    "    # predicted3 = np.argmax(res,1)\n",
    "    # end_time = time.time()\n",
    "    # dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    # dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "    # print(predicted3)\n",
    "    print(\"PREDICTIONS\")\n",
    "    print(predicted1)\n",
    "    print(predicted2)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
