{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42490d20",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b0d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "#-------------- Data Extraction ---------------------------\n",
    "\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcf859",
   "metadata": {},
   "source": [
    "## CNN - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c17803",
   "metadata": {},
   "source": [
    "The PyTorch model will be used as a reference to compute the weights since it's the fastest in training and the least prone to errors. If everything is written well, both slow and fast implementations of a CNN in numpy will give the same result, since the weights are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c11b9",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a1d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 1.------------------ CNN declaration -------------------\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        out_fc1_relu = x.clone() # Salva\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, out_fc1_relu\n",
    "\n",
    "# # 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9541aa5",
   "metadata": {},
   "source": [
    "### Weights extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5906ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 2, 2), NumPy Shape=(64, 32, 2, 2)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 2, 2), NumPy Shape=(128, 64, 2, 2)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pyt_k1_w = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pyt_k1_w\n",
    "\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pyt_k1_w.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pyt_k2_w = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pyt_k2_w\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pyt_k2_w.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pyt_k3_w = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pyt_k3_w\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pyt_k3_w.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pyt_w1 = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pyt_w1.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pyt_b1 = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pyt_b1.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pyt_w1.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pyt_b1.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pyt_w2 = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pyt_w2.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pyt_b2 = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pyt_b2.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pyt_w2.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pyt_b2.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']\n",
    "\n",
    "\n",
    "\n",
    "# [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]]\n",
    "\n",
    "\n",
    "#  [[[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]]\n",
    "\n",
    "\n",
    "\n",
    "#    [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]\n",
    "\n",
    "#   [[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba32882",
   "metadata": {},
   "source": [
    "## CNN - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1185e05",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b49fb",
   "metadata": {},
   "source": [
    "`np.pad()` takes as first argument the matrix to pad and as second argument a set of specification: for every dimension (in our case 4) it takes the number of paddings to add before and after the end of the dimension. If the objective is to pad only the image itself, which is found in the last two dimension, we should write:\n",
    "\n",
    "`np.pad(img9,((0,0),(0,0),(pad,pad),(pad,pad)))` \n",
    "\n",
    "since dimensions are: BATCH, CHANNELS, HEIGHT, WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0599825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "img9 = np.arange(1,37).reshape(2,2,3,3)\n",
    "pad_img9 = np.pad(img9,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(img9)\n",
    "print(pad_img9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b7c53",
   "metadata": {},
   "source": [
    "### Dilating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce9344",
   "metadata": {},
   "source": [
    "`dilateOne` adds one zero between each element in the matrix given in input. this is done to be able to do the backward phase with stride 1 even in the forward it was 2, by modifying the gradient of the output. Motivations will be better analyzed in the next sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee6997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilateOne(matrix):\n",
    "    indix = np.arange(1,matrix.shape[3])\n",
    "    matrix = np.insert(matrix,indix,0,3)\n",
    "    indix = np.arange(-(matrix.shape[-2]-1),0)\n",
    "    matrix = np.insert(matrix,indix,0,-2)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d1319",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c26aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[ 1.  2.  3.]\n",
      "   [ 4.  5.  6.]\n",
      "   [ 7.  8.  9.]]]\n",
      "\n",
      "\n",
      " [[[10. 11. 12.]\n",
      "   [13. 14. 15.]\n",
      "   [16. 17. 18.]]]]\n",
      "-------ker-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[ 37.  47.]\n",
      "   [ 67.  77.]]]\n",
      "\n",
      "\n",
      " [[[127. 137.]\n",
      "   [157. 167.]]]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1] doesn't match the broadcast shape [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 104\u001b[0m\n\u001b[0;32m    100\u001b[0m my_kernel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(ker)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    102\u001b[0m my_bias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m--> 104\u001b[0m modelC \u001b[38;5;241m=\u001b[39m \u001b[43mCustomConv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# input di prova (batch=1, canali=1, H=5, W=5)\u001b[39;00m\n\u001b[0;32m    107\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img)\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mCustomConv.__init__\u001b[1;34m(self, kernel, bias, stride, padding)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcopy_(kernel)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1] doesn't match the broadcast shape [2]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This is a PyTorch Convolution example to be used to check if the convolution implemented in both slow and fast approaches are correct\n",
    "\n",
    "class CustomConv(nn.Module):\n",
    "    def __init__(self, kernel: torch.Tensor, bias: torch.Tensor = None, \n",
    "                 stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernel.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(bias is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernel)\n",
    "            if bias is not None:\n",
    "                self.conv.bias.copy_(bias)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if bias is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "def Slow_ReLU_Conv(img,ker,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        out_ch, in_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = out_ch\n",
    "    else: # Backward case\n",
    "        in_ch, out_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = in_ch\n",
    "\n",
    "    # bias has shape out_ch, 1, 1. It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # number of channels taken in input by the kernel 'in_ch' \n",
    "    # must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    n_images, channels, i_height, i_width  = img.shape\n",
    "    ni_height = int(((i_height - k_height) / stride) + 1) # new image height # Padding is already added\n",
    "    ni_width = int(((i_width - k_width) / stride) + 1) # new image width\n",
    "    ni = np.zeros((n_images, out_ch, ni_height, ni_width)).astype(np.float32) # new image\n",
    "\n",
    "    if in_ch != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({in_ch}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for one_img in range(n_images):\n",
    "        for one_k_channel in range(nk_channel):\n",
    "            for i_nih in range(ni_height): # which cycles row by row of the new image\n",
    "                for i_niw in range(ni_width): # which cycles column by column of the new image\n",
    "                    current_sum = 0.0 # convolution sum for the specific output cell\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(channels): # channels == in_ch\n",
    "                        for i_kh in range(k_height):\n",
    "                            input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "                            for i_kw in range(k_width):\n",
    "                                input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "                                    input_val = img[one_img, channel, input_y, input_x]\n",
    "                                    kernel_val = ker[one_k_channel, channel, i_kh, i_kw]\n",
    "                                    current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "                    ni[one_img, one_k_channel, i_nih, i_niw] = current_sum\n",
    "    if bias.all() != 0:\n",
    "        bias = bias.reshape(bias.shape[0],1,1)\n",
    "        if bias.shape[0] != out_ch:\n",
    "            raise ValueError(f\"bias dimension ({bias.shape[0]}) doesn't match kernel's number of channels ({out_ch})\")\n",
    "        ni = ni + bias\n",
    "    ni = ni.astype(np.float32)\n",
    "    if applyReLU:\n",
    "        ni = np.maximum(0, ni)\n",
    "        mask = ni.copy()\n",
    "        mask[mask > 0] = 1\n",
    "        return ni,mask\n",
    "    else:\n",
    "        return ni\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "img = np.arange(1,18+1).reshape(2,1,3,3).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.array([1,2,3,4]).reshape(1,1,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([0]).reshape(1,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "\n",
    "my_kernel = torch.from_numpy(ker).float()\n",
    "\n",
    "my_bias = torch.from_numpy(np.array([1,2])).float()\n",
    "\n",
    "modelC = CustomConv(kernel=my_kernel,bias=my_bias, stride=1, padding=0)\n",
    "\n",
    "# input di prova (batch=1, canali=1, H=5, W=5)\n",
    "x = torch.from_numpy(img)\n",
    "y = modelC(x)\n",
    "print(\"-------Conv PyTorch-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec0a01",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805b797",
   "metadata": {},
   "source": [
    "**Actors:**\n",
    "1. W is the kernel\n",
    "2. $\\delta$ is the gradient\n",
    "3. x is the input to the convolution layer during forward\n",
    "4. b is the bias\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Derive delta**\n",
    "\n",
    "Deriving delta with respect to ReLU activation consists in the hadamard product (element-wise product) of the gradient ($\\delta$) and the mask obtained at the forward step, that is, all the elements in the convolved image greater than zero are put to one, the rest is zero.\n",
    "$$\n",
    "\\delta^{(i)} = \\delta_{\\text{flat reshaped}} \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "- **Gradient with respect to W**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(x^{(i)}, \\delta)\n",
    "$$\n",
    "This convolution creates a matrix for every channel of input image $x^{i}$ and for every channel of output image $\\delta$, thus resulting in the correct number of channels\n",
    "\n",
    "- **Gradient w.r.t. the input \\( x \\)** (To go to the preceding layer):\n",
    "\n",
    "$$\n",
    "\\delta^{(i-1)} = \\text{Full\\_Convolution}(\\delta^{(i)}, W^{(i)})\n",
    "$$\n",
    "\n",
    "- **Gradient w.r.t the bias**\n",
    "\n",
    "Since the bias is added equally across the spatial dimensions of each output channel, the gradient is the sum of all elements in each output channel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{h,w} \\delta^{(i)}_{c,h,w}\n",
    "$$\n",
    "\n",
    "For batched inputs, sum also across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n,h,w} \\delta^{(i)}_{n,c,h,w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5de649af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imAge: (1, 1, 7, 7)\n",
      "kerNel: (2, 1, 2, 2)\n",
      "dimAge: (1, 2, 4, 4)\n",
      "ggi: (1, 1, 7, 7)\n",
      "ggk: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "def Slow_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "    ############################################# Gradient of Input Image ####################################\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image delated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel 180 degrees rotation (flipped vertically and then horizontally)\n",
    "    # FullConvolution(d_imgDelated, Rotated180Deg(kernel)) with stride 1\n",
    "    out_ch, in_ch, k_height, k_width = ker.shape\n",
    "    batch_s, in_ch, img_height, img_width = img.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "\n",
    "    # Delating the gradient of output\n",
    "    if stride == 2:\n",
    "        d_img = dilateOne(d_img)\n",
    "    elif stride > 2:\n",
    "        raise ValueError(f\"Stride greater than 2 is not acceptable\")\n",
    "    d_imgPadded = np.pad(d_img,((0,0),(0,0),(k_height-1-pad,k_height-1-pad),(k_width-1-pad,k_width-1-pad)))\n",
    "    batch_s, out_ch, dimg_height, dimg_width = d_img.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    ker180 = np.rot90(ker,2,(-2,-1))\n",
    "\n",
    "    # Computation\n",
    "    gi = np.zeros_like(img)\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(img_height):\n",
    "            for i_giw in range(img_width):\n",
    "                for i_outch in range(out_ch):\n",
    "                    for i_inch in range(in_ch):\n",
    "                        for i_kh in range(k_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(k_width):\n",
    "                                x = i_giw + i_kw\n",
    "\n",
    "                                if 0 <= y < d_imgPadded.shape[-2] and 0 <= x < d_imgPadded.shape[-1]:\n",
    "                                    input_val = d_imgPadded[bs,i_outch,y,x]\n",
    "                                    ker_val = ker180[i_outch,i_inch,i_kh,i_kw] \n",
    "                                else:\n",
    "                                    break\n",
    "                                current_sum += input_val*ker_val\n",
    "                    gi[bs,i_inch,i_gih,i_giw] = current_sum\n",
    "                    current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the delated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    gk = np.zeros_like(ker)\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(k_height):\n",
    "            for i_giw in range(k_width):\n",
    "                for i_inch in range(in_ch):\n",
    "                    for i_outch in range(out_ch):\n",
    "                        for i_kh in range(dimg_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(dimg_width):\n",
    "                                x = i_gih + i_kw\n",
    "                                if 0 <= y < img_height and 0 <= x < img_width:\n",
    "                                    input_val = img[bs,i_inch,y,x]\n",
    "                                    ker_val = d_img[bs,i_outch,i_kh,i_kw] \n",
    "                                    current_sum += input_val*ker_val\n",
    "                                else:\n",
    "                                    break\n",
    "                        gk[i_outch,i_inch,i_gih,i_giw] = current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gb = d_img.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "in_ch = 1\n",
    "out_ch = 2\n",
    "idim = 7\n",
    "kdim = 2\n",
    "s = 2\n",
    "p = 1\n",
    "imAge = np.arange(1,1*in_ch*idim*idim+1).reshape(1,in_ch,idim,idim)\n",
    "kerNel = np.arange(1,out_ch*in_ch*(kdim**2)+1).reshape(out_ch,in_ch,kdim,kdim)\n",
    "dimAge,mask = Slow_ReLU_Conv(imAge,kerNel,stride=s,pad=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "ggi,ggk,ggb = Slow_ReLU_Gradient(imAge,dimAge,kerNel,mask,stride=s,pad=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggk: {ggk.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef690f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imAge: (1, 1, 7, 7)\n",
      "kerNel: (2, 1, 2, 2)\n",
      "dimAge: (1, 2, 4, 4)\n",
      "ggi: (1, 1, 7, 7)\n",
      "ggk: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Slow_ReLU_Gradient(img_fwd, d_out_values, ker, mask_relu_fwd, pad_fwd=0, stride_fwd=1):\n",
    "    \"\"\"\n",
    "    Calcola i gradienti per un layer Conv+ReLU.\n",
    "    img_fwd: input del forward pass.\n",
    "    d_out_values: gradiente dell'output del layer (dopo ReLU).\n",
    "    ker: kernel del forward pass.\n",
    "    mask_relu_fwd: maschera della ReLU del forward (attivazioni > 0).\n",
    "    pad_fwd: padding usato nel forward.\n",
    "    stride_fwd: stride usato nel forward.\n",
    "    Restituisce: grad_input, grad_kernel, grad_bias.\n",
    "    \"\"\"\n",
    "    batch_s, in_ch, img_height, img_width = img_fwd.shape\n",
    "    out_ch, _, k_height, k_width = ker.shape\n",
    "    _, _, out_h_fwd, out_w_fwd = d_out_values.shape # Dimensioni dell'output del forward\n",
    "\n",
    "    # 0. Normalizza pad e stride se sono interi\n",
    "    if isinstance(pad_fwd, int):\n",
    "        pad_fwd_h, pad_fwd_w = pad_fwd, pad_fwd\n",
    "    else:\n",
    "        pad_fwd_h, pad_fwd_w = pad_fwd\n",
    "\n",
    "    if isinstance(stride_fwd, int):\n",
    "        stride_fwd_h, stride_fwd_w = stride_fwd, stride_fwd\n",
    "    else:\n",
    "        stride_fwd_h, stride_fwd_w = stride_fwd\n",
    "\n",
    "    # 1. Backward pass attraverso la ReLU\n",
    "    # Moltiplica il gradiente in arrivo per la maschera della derivata della ReLU.\n",
    "    # dL/d(output_conv) = dL/d(output_relu) * ReLU'(input_relu)\n",
    "    # d_conv_out è il gradiente rispetto all'output della convoluzione (prima della ReLU)\n",
    "    d_conv_out = np.multiply(d_out_values, mask_relu_fwd)\n",
    "\n",
    "    # 2. Calcolo del gradiente rispetto ai BIAS (dL/db)\n",
    "    # Somma d_conv_out (gradiente dopo backward ReLU) su batch, altezza e larghezza per ogni canale di output.\n",
    "    # grad_bias ha shape (out_ch,)\n",
    "    grad_bias = np.sum(d_conv_out, axis=(0, 2, 3))\n",
    "\n",
    "    # 3. Calcolo del gradiente rispetto al KERNEL (dL/dk)\n",
    "    # Questo è una convoluzione tra l'input del forward (img_fwd, paddato come nel forward)\n",
    "    # e il gradiente d_conv_out (dilatato se stride_fwd > 1).\n",
    "    # PyTorch lo fa come: correlate(input_paddato, grad_output_dilatato)\n",
    "    \n",
    "    # Dilatazione di d_conv_out se stride > 1 (per il calcolo di dL/dk e dL/din)\n",
    "    # d_conv_out_dilated = d_conv_out # Inizializza\n",
    "    # if stride_fwd_h > 1 or stride_fwd_w > 1:\n",
    "        # Qui è necessaria una funzione di dilatazione che inserisca stride-1 zeri\n",
    "        # tra gli elementi di d_conv_out. Per semplicità, se la tua 'delateOne'\n",
    "        # funziona per stride=2, la usiamo. Altrimenti, va implementata.\n",
    "        # Questa dilatazione è usata specificamente per il calcolo del gradiente dell'input.\n",
    "        # Per il gradiente del kernel, PyTorch usa l'input originale e il d_conv_out non dilatato,\n",
    "        # ma con uno \"stride\" nella patch di input (o \"dilation\" nel kernel della convoluzione per dL/dk).\n",
    "\n",
    "    # Usiamo un approccio standard: convolvere l'input (paddato) con d_conv_out (non dilatato)\n",
    "    # L'input al layer convoluzionale, img_fwd, deve essere paddato come nel forward.\n",
    "    img_fwd_padded = np.pad(img_fwd,\n",
    "                            ((0, 0), (0, 0), (pad_fwd_h, pad_fwd_h), (pad_fwd_w, pad_fwd_w)),\n",
    "                            mode='constant', constant_values=0)\n",
    "\n",
    "    grad_kernel = np.zeros_like(ker)\n",
    "    for n in range(batch_s): # Somma i contributi di ogni campione del batch\n",
    "        for c_out in range(out_ch):\n",
    "            for c_in in range(in_ch):\n",
    "                for r_k in range(k_height):\n",
    "                    for c_k in range(k_width):\n",
    "                        val = 0.0\n",
    "                        for r_o in range(out_h_fwd): # Altezza dell'output del forward\n",
    "                            for c_o in range(out_w_fwd): # Larghezza dell'output del forward\n",
    "                                # Coordinate nell'input paddato che hanno contribuito a output[n, c_out, r_o, c_o]\n",
    "                                # usando kernel[c_out, c_in, r_k, c_k]\n",
    "                                img_r = r_o * stride_fwd_h + r_k\n",
    "                                img_c = c_o * stride_fwd_w + c_k\n",
    "                                val += img_fwd_padded[n, c_in, img_r, img_c] * d_conv_out[n, c_out, r_o, c_o]\n",
    "                        grad_kernel[c_out, c_in, r_k, c_k] += val\n",
    "\n",
    "\n",
    "    # 4. Calcolo del gradiente rispetto all'INPUT (dL/din)\n",
    "    # Questo è concettualmente una \"full convolution\" o \"transposed convolution\".\n",
    "    # Si può implementare come una convoluzione standard:\n",
    "    #   Input della convoluzione: d_conv_out (dilatato e paddato in modo speciale)\n",
    "    #   Kernel della convoluzione: ker (ruotato di 180 gradi)\n",
    "    \n",
    "    # Dilatazione di d_conv_out: inserire (stride - 1) zeri\n",
    "    d_conv_out_dilated = d_conv_out # Default per stride=1\n",
    "    if stride_fwd_h > 1 or stride_fwd_w > 1:\n",
    "        # Creazione di un array più grande per la dilatazione\n",
    "        dil_h = out_h_fwd + (out_h_fwd - 1) * (stride_fwd_h - 1)\n",
    "        dil_w = out_w_fwd + (out_w_fwd - 1) * (stride_fwd_w - 1)\n",
    "        d_conv_out_dilated = np.zeros((batch_s, out_ch, dil_h, dil_w), dtype=d_conv_out.dtype)\n",
    "        d_conv_out_dilated[:, :, ::stride_fwd_h, ::stride_fwd_w] = d_conv_out\n",
    "\n",
    "    # Padding per la \"full convolution\" (convoluzione trasposta)\n",
    "    # p' = k - 1 - p_fwd\n",
    "    pad_tp_h = k_height - 1 - pad_fwd_h\n",
    "    pad_tp_w = k_width - 1 - pad_fwd_w\n",
    "    \n",
    "    # Assicurati che il padding non sia negativo (può succedere se p_fwd è grande)\n",
    "    # In tal caso, si potrebbe dover \"croppare\" invece di paddare, o aggiustare.\n",
    "    # PyTorch gestisce questo con output_padding nella ConvTranspose.\n",
    "    # Per ora, assumiamo pad_tp_h/w >= 0. Se sono negativi, la logica di np.pad non va bene.\n",
    "    # Considera il padding necessario per ottenere le dimensioni di img_fwd.\n",
    "    # Le dimensioni dell'output di questa convoluzione devono essere img_height, img_width.\n",
    "    # Se out_dil = (H_in - K + 2P_tp)/S_tp + 1, con S_tp=1.  H_out_dil = H_img_fwd\n",
    "    # H_img_fwd = H_d_conv_out_dilated - K + 2P_tp + 1\n",
    "    # 2P_tp = H_img_fwd - H_d_conv_out_dilated + K - 1\n",
    "    # P_tp = (H_img_fwd - H_d_conv_out_dilated + K - 1) / 2\n",
    "    # Questo è più complesso. Il padding p' = K-1-p_fwd è un modo comune di pensarlo.\n",
    "    # Se p' è negativo, significa che dopo la convoluzione, il risultato è più grande dell'input originale\n",
    "    # e andrebbe croppato. np.pad non gestisce padding negativo.\n",
    "\n",
    "    # Per semplicità, se pad_tp_h o pad_tp_w sono negativi, impostali a 0 e si dovrà croppare dopo.\n",
    "    # Questo è un punto delicato.\n",
    "    effective_pad_tp_h_pre = max(0, pad_tp_h)\n",
    "    effective_pad_tp_w_pre = max(0, pad_tp_w)\n",
    "    effective_pad_tp_h_post = max(0, pad_tp_h) # Simmetrico per ora\n",
    "    effective_pad_tp_w_post = max(0, pad_tp_w) # Simmetrico per ora\n",
    "    \n",
    "    # Se usi la formula del padding per conv transpose p' = k - 1 - p_fwd, e il risultato\n",
    "    # della convoluzione è più grande, allora bisogna fare un crop.\n",
    "    # Calcoliamo le dimensioni attese dell'output della convoluzione (senza crop/pad finale)\n",
    "    # H_conv_out = d_conv_out_dilated.shape[2] - k_height + 2*effective_pad_tp_h + 1\n",
    "    # W_conv_out = d_conv_out_dilated.shape[3] - k_width + 2*effective_pad_tp_w + 1\n",
    "    # Se queste non corrispondono a img_height/img_width, è necessario un aggiustamento (crop o padding aggiuntivo).\n",
    "\n",
    "    d_conv_out_dilated_padded = np.pad(d_conv_out_dilated,\n",
    "                                       ((0,0), (0,0), (effective_pad_tp_h_pre, effective_pad_tp_h_post), (effective_pad_tp_w_pre, effective_pad_tp_w_post)),\n",
    "                                       mode='constant', constant_values=0)\n",
    "\n",
    "    # Kernel ruotato di 180 gradi\n",
    "    # ker [out_ch, in_ch, k_h, k_w] -> ker_flipped [in_ch, out_ch, k_h, k_w] per conv_transpose\n",
    "    # O ker_flipped [out_ch, in_ch, k_h, k_w] e si itera diversamente.\n",
    "    # PyTorch usa il kernel originale ma riarrangia i canali.\n",
    "    # Per una convoluzione standard, il kernel è [out_channels_conv, in_channels_conv, kh, kw]\n",
    "    # L'input è d_conv_out_dilated_padded (in_channels_conv = out_ch originali)\n",
    "    # L'output è grad_input (out_channels_conv = in_ch originali)\n",
    "    # Quindi il kernel per questa convoluzione deve avere shape [in_ch, out_ch, k_h, k_w]\n",
    "    # e i suoi valori sono quelli di ker originale, ma flippati e con i canali scambiati.\n",
    "    \n",
    "    kernel_for_grad_input = np.zeros((in_ch, out_ch, k_height, k_width), dtype=ker.dtype)\n",
    "    for c_o in range(out_ch):\n",
    "        for c_i in range(in_ch):\n",
    "            kernel_for_grad_input[c_i, c_o, :, :] = ker[c_o, c_i, ::-1, ::-1] # Flip spaziale\n",
    "\n",
    "    grad_input = np.zeros_like(img_fwd)\n",
    "    # Convoluzione standard: grad_input = conv(d_conv_out_dilated_padded, kernel_for_grad_input)\n",
    "    # (stride di questa convoluzione è 1)\n",
    "    for n in range(batch_s):\n",
    "        for c_i_gi in range(in_ch): # Canale di output di questa convoluzione\n",
    "            for r_gi in range(img_height): # Riga nell'output grad_input\n",
    "                for c_gi in range(img_width): # Colonna nell'output grad_input\n",
    "                    val = 0.0\n",
    "                    for c_o_dcnv in range(out_ch): # Canale di input di questa convoluzione\n",
    "                        for r_k in range(k_height):\n",
    "                            for c_k in range(k_width):\n",
    "                                # Coordinate nell'input d_conv_out_dilated_padded\n",
    "                                # Stride qui è 1\n",
    "                                dcnv_r = r_gi + r_k\n",
    "                                dcnv_c = c_gi + c_k\n",
    "                                # Verifica confini per d_conv_out_dilated_padded\n",
    "                                if 0 <= dcnv_r < d_conv_out_dilated_padded.shape[2] and \\\n",
    "                                   0 <= dcnv_c < d_conv_out_dilated_padded.shape[3]:\n",
    "                                    val += d_conv_out_dilated_padded[n, c_o_dcnv, dcnv_r, dcnv_c] * \\\n",
    "                                           kernel_for_grad_input[c_i_gi, c_o_dcnv, r_k, c_k]\n",
    "                    grad_input[n, c_i_gi, r_gi, c_gi] = val\n",
    "    \n",
    "    # Eventuale cropping di grad_input se il padding ha prodotto dimensioni maggiori\n",
    "    # Questo è un punto complesso da generalizzare senza conoscere le esatte dimensioni\n",
    "    # risultanti dal padding. Se il padding `p' = k-1-p_fwd` è stato usato correttamente,\n",
    "    # le dimensioni dovrebbero corrispondere. Se sono stati usati `max(0, p')`,\n",
    "    # allora `grad_input` potrebbe essere più piccolo e non necessitare crop, oppure\n",
    "    # il calcolo del padding deve essere più preciso per garantire la dimensione corretta.\n",
    "    # Se `img_height = (d_conv_out_dilated_padded.shape[2] - k_height)/1 + 1`, allora ok.\n",
    "\n",
    "    return grad_input, grad_kernel, grad_bias\n",
    "\n",
    "in_ch = 1\n",
    "out_ch = 2\n",
    "idim = 7\n",
    "kdim = 2\n",
    "s = 2\n",
    "p = 1\n",
    "imAge = np.arange(1,1*in_ch*idim*idim+1).reshape(1,in_ch,idim,idim)\n",
    "kerNel = np.arange(1,out_ch*in_ch*(kdim**2)+1).reshape(out_ch,in_ch,kdim,kdim)\n",
    "dimAge,mask = Slow_ReLU_Conv(imAge,kerNel,stride=s,pad=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "ggi,ggk,ggb = Slow_ReLU_Gradient(imAge,dimAge,kerNel,mask,stride_fwd=s,pad_fwd=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggk: {ggk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876559ef",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3469f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Conv Slow-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Conv(batch_of_images,kernel,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    kc, ac, kw, kh = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    # im2col: Window creation\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    window_m = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))[:,:,::stride,::stride].reshape((-1,(kw*kh*nc))) # window matrix\n",
    "    # Convolution\n",
    "    kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)\n",
    "    c_m = (window_m @ kernel).astype(np.float32) # convolved image matrix\n",
    "    # ReLU activation\n",
    "    nih = int(((ih-kh) / stride) + 1) # new image height # Padding is already added\n",
    "    niw = int(((iw-kw) / stride) + 1) # new image width\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output_temp = c_m.reshape(bs, nih, niw, kc)\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    reshaped_correct_order = output_temp.transpose(0,3,1,2).astype(np.float32)\n",
    "    if bias.any() != 0:\n",
    "        reshaped_correct_order = (reshaped_correct_order + bias.reshape(1,-1,1,1))\n",
    "    if applyReLU:\n",
    "        reshaped_correct_order = np.maximum(0,reshaped_correct_order)\n",
    "    mask = np.copy(reshaped_correct_order)\n",
    "    mask[mask>0]=1\n",
    "    return reshaped_correct_order,mask\n",
    "\n",
    "\n",
    "\n",
    "img = np.arange(1,2*3*3+1).reshape(1,2,3,3).astype(np.float32)\n",
    "# print(\"-------img-------\")\n",
    "# print(img)\n",
    "ker = np.arange(1,16+1).reshape(2,2,2,2)\n",
    "# print(\"-------ker-------\")\n",
    "# print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(img,ker,bias,pad = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)\n",
    "res,mask = Slow_ReLU_Conv(res,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(X_c,ker,bias,pad = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1cbb4a",
   "metadata": {},
   "source": [
    "Dimensional reshape test to see if everything goes into the right place (it does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d8576ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1245. 1323.]\n",
      "   [1479. 1557.]]\n",
      "\n",
      "  [[2973. 3195.]\n",
      "   [3639. 3861.]]\n",
      "\n",
      "  [[4701. 5067.]\n",
      "   [5799. 6165.]]\n",
      "\n",
      "  [[6429. 6939.]\n",
      "   [7959. 8469.]]]]\n",
      "[[ 1  2  4  5]\n",
      " [ 2  3  5  6]\n",
      " [ 4  5  7  8]\n",
      " [ 5  6  8  9]\n",
      " [10 11 13 14]\n",
      " [11 12 14 15]\n",
      " [13 14 16 17]\n",
      " [14 15 17 18]\n",
      " [19 20 22 23]\n",
      " [20 21 23 24]\n",
      " [22 23 25 26]\n",
      " [23 24 26 27]]\n",
      "[[1245. 1323. 1479. 1557.]\n",
      " [2973. 3195. 3639. 3861.]\n",
      " [4701. 5067. 5799. 6165.]\n",
      " [6429. 6939. 7959. 8469.]]\n",
      "[[1245. 2973. 4701. 6429.]\n",
      " [1323. 3195. 5067. 6939.]\n",
      " [1479. 3639. 5799. 7959.]\n",
      " [1557. 3861. 6165. 8469.]]\n",
      "[[ 17592.  43224.  68856.  94488.]\n",
      " [ 23196.  56892.  90588. 124284.]\n",
      " [ 34404.  84228. 134052. 183876.]\n",
      " [ 40008.  97896. 155784. 213672.]\n",
      " [ 68028. 166236. 264444. 362652.]\n",
      " [ 73632. 179904. 286176. 392448.]\n",
      " [ 84840. 207240. 329640. 452040.]\n",
      " [ 90444. 220908. 351372. 481836.]\n",
      " [118464. 289248. 460032. 630816.]\n",
      " [124068. 302916. 481764. 660612.]\n",
      " [135276. 330252. 525228. 720204.]\n",
      " [140880. 343920. 546960. 750000.]]\n",
      "[[[[ 17592.  23196.]\n",
      "   [ 34404.  40008.]]\n",
      "\n",
      "  [[ 68028.  73632.]\n",
      "   [ 84840.  90444.]]\n",
      "\n",
      "  [[118464. 124068.]\n",
      "   [135276. 140880.]]]\n",
      "\n",
      "\n",
      " [[[ 43224.  56892.]\n",
      "   [ 84228.  97896.]]\n",
      "\n",
      "  [[166236. 179904.]\n",
      "   [207240. 220908.]]\n",
      "\n",
      "  [[289248. 302916.]\n",
      "   [330252. 343920.]]]\n",
      "\n",
      "\n",
      " [[[ 68856.  90588.]\n",
      "   [134052. 155784.]]\n",
      "\n",
      "  [[264444. 286176.]\n",
      "   [329640. 351372.]]\n",
      "\n",
      "  [[460032. 481764.]\n",
      "   [525228. 546960.]]]\n",
      "\n",
      "\n",
      " [[[ 94488. 124284.]\n",
      "   [183876. 213672.]]\n",
      "\n",
      "  [[362652. 392448.]\n",
      "   [452040. 481836.]]\n",
      "\n",
      "  [[630816. 660612.]\n",
      "   [720204. 750000.]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "s = 1\n",
    "p = 0\n",
    "in_ch = 3\n",
    "out_ch = 4\n",
    "i_dim = 3\n",
    "k_dim = 2\n",
    "img = np.arange(1,i_dim*in_ch*i_dim+1).reshape(1,in_ch,i_dim,i_dim)\n",
    "ker = np.arange(1,out_ch*k_dim*in_ch*k_dim+1).reshape(out_ch,in_ch,k_dim,k_dim)\n",
    "d_img,mask = Fast_ReLU_Conv(img,ker,stride=s,pad=p)\n",
    "print(d_img)\n",
    "_,_,dimg_height,dimg_width = d_img.shape\n",
    "windom_pimg = np.lib.stride_tricks.sliding_window_view(img,(1,1,dimg_height,dimg_width)).reshape(-1,dimg_height*dimg_width)\n",
    "print(windom_pimg)\n",
    "print(d_img.reshape(-1,dimg_height*dimg_width))\n",
    "d_img = d_img.reshape(-1,dimg_height*dimg_width).transpose(1,0)\n",
    "print(d_img)\n",
    "iop = windom_pimg @ d_img\n",
    "print(iop)\n",
    "print(iop.transpose(1,0).reshape(out_ch,in_ch,k_dim,kdim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91cb44",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimAge: (1, 4, 2, 2)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'delateOne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m dimAge \u001b[38;5;241m=\u001b[39m dimAge\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(dimAge)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimAge: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdimAge\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m ggi,ggk,ggb \u001b[38;5;241m=\u001b[39m \u001b[43mFast_ReLU_Gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimAge\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdimAge\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkerNel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimAge: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimAge\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkerNel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkerNel\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m, in \u001b[0;36mFast_ReLU_Gradient\u001b[1;34m(img, d_img, ker, mask, pad, stride)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Delating the gradient of output\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m     d_img \u001b[38;5;241m=\u001b[39m \u001b[43mdelateOne\u001b[49m(d_img)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m stride \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStride greater than 2 is not acceptable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'delateOne' is not defined"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "    ############################################# Gradient of Input Image ####################################\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image delated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel 180 degrees rotation (flipped vertically and then horizontally)\n",
    "    # FullConvolution(d_imgDelated, Rotated180Deg(kernel)) with stride 1\n",
    "    out_ch, in_ch, k_height, k_width = ker.shape\n",
    "    batch_s, in_ch, img_height, img_width = img.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "\n",
    "    # Delating the gradient of output\n",
    "    if stride == 2:\n",
    "        d_img = dilateOne(d_img)\n",
    "    elif stride > 2:\n",
    "        raise ValueError(f\"Stride greater than 2 is not acceptable\")\n",
    "    d_imgPadded = np.pad(d_img,((0,0),(0,0),(k_height-1-pad,k_height-1-pad),(k_width-1-pad,k_width-1-pad)))\n",
    "\n",
    "    batch_s, out_ch, dimg_height, dimg_width = d_img.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    ker180 = np.rot90(ker,2,(-2,-1))\n",
    "\n",
    "    window_dPad = np.lib.stride_tricks.sliding_window_view(d_imgPadded,(1,out_ch,k_width,k_height)).reshape(-1,(k_width*k_height*out_ch)) # window matrix\n",
    "    \n",
    "    # Convolution\n",
    "    ker = ker180.reshape((-1,(k_width*k_height*out_ch))).transpose(1,0)\n",
    "\n",
    "    gi = (window_dPad @ ker).astype(np.float32).transpose(1,0).reshape(img.shape)\n",
    "    \n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the delated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    imgPad = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    windom_pimg = np.lib.stride_tricks.sliding_window_view(imgPad,(1,1,dimg_height,dimg_width)).reshape(-1,dimg_height*dimg_width)\n",
    "    d_img = d_img.reshape(-1,dimg_height*dimg_width).transpose(1,0)\n",
    "    gk = (windom_pimg @ d_img).astype(np.float32).transpose(1,0).reshape(out_ch,in_ch,k_height,k_width)\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gb = d_img.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "in_ch = 3\n",
    "out_ch = 4\n",
    "idim = 5\n",
    "kdim = 3\n",
    "s = 2\n",
    "p = 0\n",
    "imAge = np.arange(1,1*in_ch*idim*idim+1).reshape(1,in_ch,idim,idim)\n",
    "kerNel = np.arange(1,out_ch*in_ch*(kdim**2)+1).reshape(out_ch,in_ch,kdim,kdim)\n",
    "dimAge,mask = Fast_ReLU_Conv(imAge,kerNel,stride=s,pad=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "\n",
    "ggi,ggk,ggb = Fast_ReLU_Gradient(imAge,dimAge,kerNel,mask,stride=s,pad=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggk: {ggk.shape}\")\n",
    "\n",
    "# in_ch = 1\n",
    "# out_ch = 2\n",
    "# idim = 7\n",
    "# kdim = 2\n",
    "# s = 1\n",
    "# p = 0\n",
    "\n",
    "# imAge: (1, 1, 7, 7)\n",
    "# kerNel: (2, 1, 2, 2)\n",
    "# dimAge: (1, 2, 6, 6)\n",
    "# ggi: (1, 1, 7, 7)\n",
    "# ggk: (2, 1, 2, 2)\n",
    "\n",
    "# in_ch = 1\n",
    "# out_ch = 2\n",
    "# idim = 7\n",
    "# kdim = 2\n",
    "# s = 2\n",
    "# p = 1\n",
    "\n",
    "# imAge: (1, 1, 7, 7)\n",
    "# kerNel: (2, 1, 2, 2)\n",
    "# dimAge: (1, 2, 4, 4)\n",
    "# ggi: (1, 1, 7, 7)\n",
    "# ggk: (2, 1, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc8707",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbf3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00235563 0.04731416 0.95033021]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108f244",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7942596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d3b94",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb425b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "def crossEntropy(probabilities, true_labels_one_hot):\n",
    "    # probabilities: (N, C), output della softmax\n",
    "    # true_labels_one_hot: (N, C), etichette one-hot\n",
    "    \n",
    "    N = probabilities.shape[0] # Dimensione del batch\n",
    "    epsilon = 1e-9 # Piccolo valore per stabilità\n",
    "\n",
    "    # Calcola la cross-entropy per ogni campione\n",
    "    # log_likelihood = -np.sum(true_labels_one_hot * np.log(probabilities + epsilon), axis=1)\n",
    "    # Oppure, se true_labels_one_hot è veramente one-hot, puoi selezionare direttamente la probabilità della classe corretta\n",
    "    # Questo è più efficiente se hai gli indici delle classi vere:\n",
    "    # log_probs = np.log(probabilities + epsilon)\n",
    "    # true_class_indices = np.argmax(true_labels_one_hot, axis=1)\n",
    "    # log_likelihood = -log_probs[np.arange(N), true_class_indices]\n",
    "\n",
    "    # Metodo generale con etichette one-hot:\n",
    "    # Moltiplica elemento per elemento e somma lungo l'asse delle classi (axis=1)\n",
    "    # Questo seleziona efficacemente log(p_k) per la classe corretta k, dato che t_j è 0 per j!=k e t_k=1\n",
    "    log_likelihood_per_sample = -np.sum(true_labels_one_hot * np.log(probabilities + epsilon), axis=1)\n",
    "    \n",
    "    # La loss totale è solitamente la media delle loss per campione\n",
    "    mean_loss = np.sum(log_likelihood_per_sample) / N\n",
    "    \n",
    "    # Se vuoi solo la somma (come fa PyTorch di default se reduction='sum')\n",
    "    # sum_loss = np.sum(log_likelihood_per_sample)\n",
    "    \n",
    "    return mean_loss # o sum_loss, a seconda di cosa vuoi confrontare con PyTorch\n",
    "\n",
    "#c = [1,1000000000000000,1,1]\n",
    "#c = softmax(c)\n",
    "#print(c)\n",
    "#c = crossEntropy(c,[0,1,0,0])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fd4f5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad189b98",
   "metadata": {},
   "source": [
    "In this section the three implementations will be compared in terms of time. Recall that all the predictions should be the same since the weights are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it, average_times=t: 0.0027 s, s: 5.6619 s, f: 0.001 s, correct_predictions=100.0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward execution time in seconds: \n",
      "PyTorch: 0.0027 s, \n",
      "Slow: 5.6619 s, \n",
      "Fast: 0.001 s\n",
      "Verifica pesi conv1:\n",
      "  np_k1 vs model.conv1.weight: True\n",
      "  np_bc1 vs model.conv1.bias: True\n",
      "  np_k1 vs model.conv1.weight: True\n",
      "  np_bc1 vs model.conv1.bias: True\n",
      "  np_k1 vs model.conv1.weight: True\n",
      "  np_bc1 vs model.conv1.bias: True\n",
      "  np_w1.T vs model.fc1.weight: True\n",
      "  np_b1 vs model.fc1.bias: True\n",
      "  np_w1.T vs model.fc1.weight: True\n",
      "  np_b1 vs model.fc1.bias: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "\n",
    "#length = test_labels.shape[0]\n",
    "# length = 100\n",
    "length = 1\n",
    "correct = 0\n",
    "skip = True\n",
    "step = 2\n",
    "count = 0\n",
    "loop = tqdm(range(0,length,step),desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i:i+step].reshape(step,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "    ############### CNN Slow Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(step,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted2 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "    ############### CNN Fast Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = Fast_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2f,mask2f = Fast_ReLU_Conv(c1f.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3f,mask3f = Fast_ReLU_Conv(c2f.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlpf = c3f.reshape(step,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted3 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    t = np.array(predicted1)\n",
    "    s = np.array(predicted2)\n",
    "    f = np.array(predicted3)\n",
    "    if t.all() == s.all() and t.all() == f.all():\n",
    "        correct+=1\n",
    "    count+=1\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    tat = round(sum(dict_times['ctorch'])/(i+1),4)\n",
    "    sat = round(sum(dict_times['cslow'])/(i+1),4)\n",
    "    fat = round(sum(dict_times['cfast'])/(i+1),4)\n",
    "    loop.set_postfix(average_times =f\"t: {tat} s, s: {sat} s, f: {fat} s\" , correct_predictions=f\"{100*correct/count}%\")\n",
    "tat = round(sum(dict_times['ctorch'])/length,4)\n",
    "sat = round(sum(dict_times['cslow'])/length,4)\n",
    "fat = round(sum(dict_times['cfast'])/length,4)\n",
    "print(f\"Average forward execution time in seconds: \\nPyTorch: {tat} s, \\nSlow: {sat} s, \\nFast: {fat} s\")\n",
    "print(\"Verifica pesi conv1:\")\n",
    "print(\"  np_k1 vs model.conv1.weight:\", np.allclose(np_k1, model.conv1.weight.data.cpu().numpy()))\n",
    "print(\"  np_bc1 vs model.conv1.bias:\",  np.allclose(np_b_conv1, model.conv1.bias.data.cpu().numpy()))\n",
    "print(\"  np_k1 vs model.conv1.weight:\", np.allclose(np_k2, model.conv2.weight.data.cpu().numpy()))\n",
    "print(\"  np_bc1 vs model.conv1.bias:\",  np.allclose(np_b_conv2, model.conv2.bias.data.cpu().numpy()))\n",
    "print(\"  np_k1 vs model.conv1.weight:\", np.allclose(np_k3, model.conv3.weight.data.cpu().numpy()))\n",
    "print(\"  np_bc1 vs model.conv1.bias:\",  np.allclose(np_b_conv3, model.conv3.bias.data.cpu().numpy()))\n",
    "print(\"  np_w1.T vs model.fc1.weight:\", np.allclose(np_w1.T, model.fc1.weight.data.cpu().numpy()))\n",
    "print(\"  np_b1 vs model.fc1.bias:\",     np.allclose(np_b1.reshape(-1), model.fc1.bias.data.cpu().numpy()))\n",
    "print(\"  np_w1.T vs model.fc1.weight:\", np.allclose(np_w2.T, model.fc2.weight.data.cpu().numpy()))\n",
    "print(\"  np_b1 vs model.fc1.bias:\",     np.allclose(np_b2.reshape(-1), model.fc2.bias.data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae1ea1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97018854",
   "metadata": {},
   "source": [
    "### Test for Slow approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b559791",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf57c6",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b00564",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8760193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88167606",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe14aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToBeTrained = True\n",
    "if ToBeTrained:\n",
    "    avg_loss = []\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    numEpochs = 20\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    loop = tqdm(range(numEpochs))\n",
    "    for i in loop:\n",
    "        c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "        \n",
    "        # Forward\n",
    "        sfts = time.time() # slow forward time start\n",
    "        c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "        c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "        c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "\n",
    "        imlps = c3s.reshape(1,-1)\n",
    "        fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "        sfte = time.time() # slow forward time end\n",
    "        sft = sfte - sfts\n",
    "        forward_time.append(sft)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa,train_labels[0])\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        sbts = time.time() # slow backward time start\n",
    "        dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "        dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "        gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "\n",
    "        gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "        gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "        sbte = time.time() # slow backward time end\n",
    "        sbt = sbte - sbts\n",
    "        backward_time.append(sbt)\n",
    "\n",
    "        # Weights update\n",
    "        w1 -= lr*dL_dw1\n",
    "        b1 -= lr*dL_db1\n",
    "        w2 -= lr*dL_dw2\n",
    "        b2 -= lr*dL_db2\n",
    "        k3 -= lr*gk3\n",
    "        k2 -= lr*gk2\n",
    "        k1 -= lr*gk1\n",
    "        bc3 -= lr*gb3.reshape(-1)\n",
    "        bc2 -= lr*gb2.reshape(-1)\n",
    "        bc1 -= lr*gb1.reshape(-1)\n",
    "        \n",
    "        if len(avg_loss) >= 2:\n",
    "            loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "    plt.plot(avg_loss)\n",
    "    plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdde0d",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 3.6265 s\n",
    "- average backward time : 9.8262 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Slow Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf9bdc",
   "metadata": {},
   "source": [
    "### PyTorch Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9dc683",
   "metadata": {},
   "source": [
    "#### Weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cc5c0010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifica pesi conv1:\n",
      "  p_k1 vs modelComp.conv1.weight: True\n",
      "  p_bc1 vs modelComp.conv1.bias: True\n",
      "Verifica pesi conv2:\n",
      "  p_k1 vs modelComp.conv1.weight: True\n",
      "  p_bc1 vs modelComp.conv1.bias: True\n",
      "Verifica pesi conv3:\n",
      "  p_k1 vs modelComp.conv1.weight: True\n",
      "  p_bc1 vs modelComp.conv1.bias: True\n",
      "Verifica pesi fc1:\n",
      "  p_w1.T vs modelComp.fc1.weight: True\n",
      "  p_b1 vs modelComp.fc1.bias: True\n",
      "Verifica pesi fc2:\n",
      "  p_w1.T vs modelComp.fc1.weight: True\n",
      "  p_b1 vs modelComp.fc1.bias: True\n"
     ]
    }
   ],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)\n",
    "p_k1 = np.copy(k1)\n",
    "p_bc1 = np.copy(bc1)\n",
    "p_k2 = np.copy(k2)\n",
    "p_bc2 = np.copy(bc2)\n",
    "p_k3 = np.copy(k3)\n",
    "p_bc3 = np.copy(bc3)\n",
    "p_w1 = np.copy(w1)\n",
    "p_b1 = np.copy(b1)\n",
    "p_w2 = np.copy(w2)\n",
    "p_b2 = np.copy(b2)\n",
    "modelComp = SimpleCNN()\n",
    "with torch.no_grad():\n",
    "    modelComp.conv1.weight.copy_(torch.from_numpy(p_k1))\n",
    "    modelComp.conv1.bias.copy_(torch.from_numpy(p_bc1))\n",
    "    modelComp.conv2.weight.copy_(torch.from_numpy(p_k2))\n",
    "    modelComp.conv2.bias.copy_(torch.from_numpy(p_bc2))\n",
    "    modelComp.conv3.weight.copy_(torch.from_numpy(p_k3))\n",
    "    modelComp.conv3.bias.copy_(torch.from_numpy(p_bc3))\n",
    "    modelComp.fc1.weight.copy_(torch.from_numpy(p_w1.T))\n",
    "    modelComp.fc1.bias.copy_(torch.from_numpy(p_b1).reshape(-1))\n",
    "    modelComp.fc2.weight.copy_(torch.from_numpy(p_w2.T))\n",
    "    modelComp.fc2.bias.copy_(torch.from_numpy(p_b2).reshape(-1))\n",
    "\n",
    "print(\"Verifica pesi conv1:\")\n",
    "print(\"  p_k1 vs modelComp.conv1.weight:\", np.allclose(k1, modelComp.conv1.weight.data.cpu().numpy()))\n",
    "print(\"  p_bc1 vs modelComp.conv1.bias:\", np.allclose(bc1, modelComp.conv1.bias.data.cpu().numpy()))\n",
    "print(\"Verifica pesi conv2:\")\n",
    "print(\"  p_k1 vs modelComp.conv1.weight:\", np.allclose(k2, modelComp.conv2.weight.data.cpu().numpy()))\n",
    "print(\"  p_bc1 vs modelComp.conv1.bias:\", np.allclose(bc2, modelComp.conv2.bias.data.cpu().numpy()))\n",
    "print(\"Verifica pesi conv3:\")\n",
    "print(\"  p_k1 vs modelComp.conv1.weight:\", np.allclose(k3, modelComp.conv3.weight.data.cpu().numpy()))\n",
    "print(\"  p_bc1 vs modelComp.conv1.bias:\", np.allclose(bc3, modelComp.conv3.bias.data.cpu().numpy()))\n",
    "print(\"Verifica pesi fc1:\")\n",
    "print(\"  p_w1.T vs modelComp.fc1.weight:\", np.allclose(w1.T, modelComp.fc1.weight.data.cpu().numpy()))\n",
    "print(\"  p_b1 vs modelComp.fc1.bias:\", np.allclose(b1.reshape(-1), modelComp.fc1.bias.data.cpu().numpy()))\n",
    "print(\"Verifica pesi fc2:\")\n",
    "print(\"  p_w1.T vs modelComp.fc1.weight:\", np.allclose(w2.T, modelComp.fc2.weight.data.cpu().numpy()))\n",
    "print(\"  p_b1 vs modelComp.fc1.bias:\", np.allclose(b2.reshape(-1), modelComp.fc2.bias.data.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e7a97bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -32.344532   -24.180666   242.01834      3.14395    -97.3935\n",
      "    -1.7314793 -150.55801    174.62186     47.483128   -12.162725 ]]\n",
      "[[ -32.344532   -24.180666   242.01834      3.14395    -97.3935\n",
      "    -1.7314793 -150.55801    174.62186     47.483128   -12.162725 ]]\n",
      "tensor([[ -32.3445,  -24.1806,  242.0184,    3.1440,  -97.3935,   -1.7315,\n",
      "         -150.5579,  174.6218,   47.4831,  -12.1627]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Verifica pesi conv1:\n",
      "  p_k1 vs modelComp.conv1.weight: True\n",
      "  p_bc1 vs modelComp.conv1.bias: True\n",
      "Verifica pesi conv2:\n",
      "  p_k1 vs modelComp.conv1.weight: True\n",
      "  p_bc1 vs modelComp.conv1.bias: True\n",
      "Verifica pesi conv3:\n",
      "  p_k1 vs modelComp.conv1.weight: True\n",
      "  p_bc1 vs modelComp.conv1.bias: True\n",
      "Verifica pesi fc1:\n",
      "  p_w1.T vs modelComp.fc1.weight: True\n",
      "  p_b1 vs modelComp.fc1.bias: True\n",
      "Verifica pesi fc2:\n",
      "  p_w1.T vs modelComp.fc1.weight: True\n",
      "  p_b1 vs modelComp.fc1.bias: True\n",
      "Losses: p:243.74984741210938 cf:243.74981689453125 cs:243.7498321533203\n",
      "\n",
      "--- CONFRONTO GRADIENTI PRIMA EPOCA (dopo backward, prima di update) ---\n",
      "FC2 w2 close: False\n",
      "FC2 b2 close: False\n",
      "PyTorch fc2.weight.grad (sample):\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.5359878e+01 0.0000000e+00 ... 8.4180431e+00\n",
      "  0.0000000e+00 3.5463215e+01]\n",
      " ...\n",
      " [0.0000000e+00 8.2499863e-29 0.0000000e+00 ... 4.5214386e-29\n",
      "  0.0000000e+00 1.9047746e-28]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "NumPy dL_dw2.T (sample):\n",
      " [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.53598871e+01 0.00000000e+00 ... 8.41804981e+00\n",
      "  0.00000000e+00 3.54632187e+01]\n",
      " ...\n",
      " [0.00000000e+00 8.25011827e-29 0.00000000e+00 ... 4.52151152e-29\n",
      "  0.00000000e+00 1.90480403e-28]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "FC1 w1 close: False\n",
      "FC1 b1 close: False\n",
      "PyTorch fc1.weight.grad (sample):\n",
      " [[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.9786965  1.3450291  1.3075296 ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " ...\n",
      " [ 2.3696735  3.256658   3.165862  ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [-1.1568004 -1.5897985 -1.5454746 ... -0.        -0.        -0.       ]]\n",
      "NumPy dL_dw1.T (sample):\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 1.15258644  1.58400745  1.53984501 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.18871171  1.63365466  1.58810804 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.20168858 -0.277182   -0.26945411 ...  0.          0.\n",
      "   0.        ]]\n",
      "Conv3 k3 close: False\n",
      "Conv3 bc3 close: False\n",
      "PyTorch conv3.weight.grad (sample):\n",
      " [[[[ 2.9489942  -2.131194  ]\n",
      "   [ 0.57200027 -2.1555972 ]]\n",
      "\n",
      "  [[ 0.26501888  1.0699525 ]\n",
      "   [-0.07931605  0.1345891 ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.6122172  -1.1920484 ]\n",
      "   [-0.16375257 -1.497859  ]]\n",
      "\n",
      "  [[ 0.8873754   0.77383465]\n",
      "   [-0.5250157  -1.4368879 ]]\n",
      "\n",
      "  [[ 0.47782046  0.19328003]\n",
      "   [-0.37290812 -0.8487786 ]]]\n",
      "\n",
      "\n",
      " [[[ 3.4845583   3.8716733 ]\n",
      "   [-2.196341    3.5336373 ]]\n",
      "\n",
      "  [[-0.7160425   1.4991072 ]\n",
      "   [-0.21335343 -0.3030587 ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.6611644   2.3178465 ]\n",
      "   [ 0.5866203  -0.3712421 ]]\n",
      "\n",
      "  [[ 0.30072683  3.057106  ]\n",
      "   [ 1.5366452   1.6068921 ]]\n",
      "\n",
      "  [[-0.803205    0.168443  ]\n",
      "   [ 0.08482813  1.1778862 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.16852483  0.22639248]\n",
      "   [ 0.12455504  0.263349  ]]\n",
      "\n",
      "  [[ 0.01127349  0.        ]\n",
      "   [ 0.02688966  0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.5142579   0.8132998 ]\n",
      "   [ 0.7918525   0.8295046 ]]\n",
      "\n",
      "  [[ 0.02444723 -0.0788755 ]\n",
      "   [ 0.11155389  0.20169431]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]]\n",
      "NumPy gk3 (sample):\n",
      " [[[[ 1.0667772  -1.0185524 ]\n",
      "   [ 0.15267685 -1.6878499 ]]\n",
      "\n",
      "  [[ 0.04104673  0.4012922 ]\n",
      "   [-0.06886112 -0.00842624]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.46791762 -1.0367074 ]\n",
      "   [-0.01348086 -0.39989215]]\n",
      "\n",
      "  [[ 0.6466518   0.21963109]\n",
      "   [ 0.07280927 -0.90855145]]\n",
      "\n",
      "  [[ 0.0532333  -0.17135768]\n",
      "   [-0.22738326 -0.6721615 ]]]\n",
      "\n",
      "\n",
      " [[[ 2.0182152   2.270619  ]\n",
      "   [-0.42401674  2.5708416 ]]\n",
      "\n",
      "  [[-0.29339817  0.63476294]\n",
      "   [ 0.0354454  -0.15445827]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.6456803   2.8353992 ]\n",
      "   [ 1.6510807   1.6755322 ]]\n",
      "\n",
      "  [[ 0.5308052   2.1761837 ]\n",
      "   [ 0.4368839   0.4639862 ]]\n",
      "\n",
      "  [[-0.43145818  0.12255585]\n",
      "   [-0.09995402  0.6249136 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]\n",
      "\n",
      "\n",
      " [[[-0.12706883 -0.09300779]\n",
      "   [-0.10891854 -0.12505393]]\n",
      "\n",
      "  [[-0.0340587   0.        ]\n",
      "   [-0.08123729  0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.14192294 -0.33305463]\n",
      "   [ 0.02532202 -0.29763067]]\n",
      "\n",
      "  [[-0.07385837  0.03603904]\n",
      "   [ 0.01069621 -0.05039967]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]]\n",
      "Conv2 k2 close: False\n",
      "Conv2 bc2 close: False\n",
      "Conv1 k1 close: False\n",
      "Conv1 bc1 close: False\n",
      "[[ -15.51823     14.791333   -42.63124     -2.9552598  -75.00726\n",
      "   -11.610147  -134.63612    128.86304     66.50432    -23.00302  ]]\n",
      "[[ -15.51823     14.791333   -42.63124     -2.9552598  -75.00726\n",
      "   -11.610147  -134.63612    128.86304     66.50432    -23.00302  ]]\n",
      "tensor([[   2.6421,   14.2765,   -2.5122,  -24.9493,  -70.1429,  174.0170,\n",
      "         -141.1576,  116.7533,   43.1943,  -40.8174]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Verifica pesi conv1:\n",
      "  p_k1 vs modelComp.conv1.weight: False\n",
      "  p_bc1 vs modelComp.conv1.bias: False\n",
      "Verifica pesi conv2:\n",
      "  p_k1 vs modelComp.conv1.weight: False\n",
      "  p_bc1 vs modelComp.conv1.bias: False\n",
      "Verifica pesi conv3:\n",
      "  p_k1 vs modelComp.conv1.weight: False\n",
      "  p_bc1 vs modelComp.conv1.bias: False\n",
      "Verifica pesi fc1:\n",
      "  p_w1.T vs modelComp.fc1.weight: False\n",
      "  p_b1 vs modelComp.fc1.bias: False\n",
      "Verifica pesi fc2:\n",
      "  p_w1.T vs modelComp.fc1.weight: False\n",
      "  p_b1 vs modelComp.fc1.bias: False\n",
      "Losses: p:0.0 cf:140.4731903076172 cs:140.47320556640625\n",
      "\n",
      "--- CONFRONTO GRADIENTI PRIMA EPOCA (dopo backward, prima di update) ---\n",
      "FC2 w2 close: False\n",
      "FC2 b2 close: False\n",
      "PyTorch fc2.weight.grad (sample):\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 2.5207445e-24 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 4.6120165e-24]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "NumPy dL_dw2.T (sample):\n",
      " [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 9.29539299e+00 3.79855800e+00 ... 2.02876598e-01\n",
      "  0.00000000e+00 2.67688122e+01]\n",
      " [0.00000000e+00 7.69467737e-27 3.14442631e-27 ... 1.67940179e-28\n",
      "  0.00000000e+00 2.21590818e-26]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "FC1 w1 close: False\n",
      "FC1 b1 close: False\n",
      "PyTorch fc1.weight.grad (sample):\n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [-5.8243221e-26 -5.9054627e-26 -5.3291868e-26 ... -0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [-5.4674692e-26 -5.5436380e-26 -5.0026708e-26 ... -0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00]]\n",
      "NumPy dL_dw1.T (sample):\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.3645546  -0.5223734  -0.4462778  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13754339  0.1970871   0.16837687 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.19529689 -0.27984259 -0.23907713 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.34221853 -0.49036786 -0.41893459 ...  0.          0.\n",
      "   0.        ]]\n",
      "Conv3 k3 close: False\n",
      "Conv3 bc3 close: False\n",
      "PyTorch conv3.weight.grad (sample):\n",
      " [[[[ 1.64437908e-25  5.85509512e-26]\n",
      "   [-1.65432920e-25 -2.13624963e-25]]\n",
      "\n",
      "  [[-2.18411133e-26  3.83315557e-26]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  1.07337468e-28]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.55516470e-25 -1.33576486e-25]\n",
      "   [-1.92012220e-25 -2.85605119e-25]]\n",
      "\n",
      "  [[-6.30042497e-26 -3.16889986e-26]\n",
      "   [-1.19448936e-25 -1.73830123e-25]]\n",
      "\n",
      "  [[-1.11525913e-25  6.03129768e-26]\n",
      "   [ 2.89083964e-26 -1.12961381e-25]]]\n",
      "\n",
      "\n",
      " [[[-2.41805318e-25 -3.66917104e-25]\n",
      "   [-8.70209105e-27 -2.51406125e-25]]\n",
      "\n",
      "  [[-3.79052793e-28  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00 -4.24847675e-28]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-4.94288558e-25 -4.71183907e-25]\n",
      "   [-3.89597274e-25 -4.58449424e-25]]\n",
      "\n",
      "  [[-6.96990164e-26 -1.58813428e-25]\n",
      "   [-3.37243929e-25 -2.63234576e-25]]\n",
      "\n",
      "  [[-8.17924498e-27 -8.08820750e-26]\n",
      "   [-4.10868126e-26 -1.21738395e-25]]]\n",
      "\n",
      "\n",
      " [[[ 3.91174084e-25  7.04522677e-26]\n",
      "   [ 4.13938786e-25  1.26670834e-25]]\n",
      "\n",
      "  [[ 5.11348359e-26  1.16754264e-26]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 3.27393743e-25  1.92824155e-25]\n",
      "   [ 1.76222910e-25  2.55860305e-25]]\n",
      "\n",
      "  [[ 1.40756907e-25  2.52179159e-25]\n",
      "   [ 1.53284166e-25  2.01587845e-25]]\n",
      "\n",
      "  [[ 2.20686402e-25  7.94379912e-26]\n",
      "   [ 2.27755822e-26  1.13365056e-25]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[ 2.04734750e-26  2.44855199e-26]\n",
      "   [ 6.27119768e-27  1.68937475e-26]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  5.09225122e-28]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 8.23883926e-26  4.10377153e-26]\n",
      "   [ 7.04539008e-26  7.15514590e-26]]\n",
      "\n",
      "  [[ 0.00000000e+00  2.21473670e-26]\n",
      "   [ 6.34888260e-26  5.40507463e-26]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 2.51409339e-26  8.45248128e-27]]]\n",
      "\n",
      "\n",
      " [[[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]]]\n",
      "NumPy gk3 (sample):\n",
      " [[[[ 2.5707517   1.61277   ]\n",
      "   [-0.3741152  -0.17183296]]\n",
      "\n",
      "  [[-0.26700965  1.2480663 ]\n",
      "   [ 0.01573831 -0.08610807]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.60204333  0.981157  ]\n",
      "   [-0.29727075 -0.41988486]]\n",
      "\n",
      "  [[-0.1158222   0.68564737]\n",
      "   [ 0.26837108 -0.6032418 ]]\n",
      "\n",
      "  [[-0.31828037  0.90146303]\n",
      "   [ 0.43965098  0.05361534]]]\n",
      "\n",
      "\n",
      " [[[-3.211971   -1.8831403 ]\n",
      "   [-0.59103984 -1.3719147 ]]\n",
      "\n",
      "  [[-0.2687092  -0.72198224]\n",
      "   [-0.15012394 -0.13690017]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-3.4815116  -3.5174696 ]\n",
      "   [-1.6413037  -2.792989  ]]\n",
      "\n",
      "  [[-0.3084832  -1.8977071 ]\n",
      "   [-1.8727267  -1.1044642 ]]\n",
      "\n",
      "  [[-0.37073344 -0.5830864 ]\n",
      "   [-0.44131455 -0.8614652 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.84557563  0.        ]\n",
      "   [ 1.3275814   0.23110591]]\n",
      "\n",
      "  [[ 0.62722635  0.        ]\n",
      "   [ 0.          0.30835748]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.84625053  0.36103094]\n",
      "   [ 0.32319185  0.718925  ]]\n",
      "\n",
      "  [[ 0.27026448  0.39942446]\n",
      "   [ 0.1773399   0.29979903]]\n",
      "\n",
      "  [[ 0.70740247  0.15601328]\n",
      "   [ 0.          0.47244808]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.12465016  0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.09811543  0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.5342137   1.3754947 ]\n",
      "   [ 0.71532387  1.1992228 ]]\n",
      "\n",
      "  [[ 0.          0.04480022]\n",
      "   [ 0.48529807  0.6321324 ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.09517679  0.0229664 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]]\n",
      "Conv2 k2 close: False\n",
      "Conv2 bc2 close: False\n",
      "Conv1 k1 close: False\n",
      "Conv1 bc1 close: False\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa\n",
    "\n",
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels[0:bs]\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2\n",
    "\n",
    "def log_softmax_numpy(x):\n",
    "    # x sono i logit, shape (N, C)\n",
    "    max_x = np.max(x, axis=-1, keepdims=True)\n",
    "    stable_x = x - max_x\n",
    "    # Aggiungi epsilon a np.sum per evitare log(0) se tutti gli exp sono 0 (improbabile con max sottratto)\n",
    "    log_sum_exp = np.log(np.sum(np.exp(stable_x), axis=-1, keepdims=True) + 1e-9) \n",
    "    return stable_x - log_sum_exp\n",
    "\n",
    "def nll_loss_numpy(log_probs, true_class_indices_flat):\n",
    "    # log_probs: output di log_softmax_numpy, shape (N, C)\n",
    "    # true_class_indices_flat: indici delle classi vere, shape (N,) es. np.array([idx1, idx2...])\n",
    "    N = log_probs.shape[0]\n",
    "    # Assicurati che true_class_indices_flat sia un array di interi per l'indicizzazione\n",
    "    return -log_probs[np.arange(N), true_class_indices_flat.astype(int)].mean()\n",
    "\n",
    "k1  = (np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)-0.5).astype(np.float32)\n",
    "bc1 = (np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)-0.5).astype(np.float32)\n",
    "k2  = (np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)-0.5).astype(np.float32)\n",
    "bc2 = (np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)-0.5).astype(np.float32)\n",
    "k3  = (np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)-0.5).astype(np.float32)\n",
    "bc3 = (np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)-0.5).astype(np.float32)\n",
    "w1  = (np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)-0.5).astype(np.float32)\n",
    "b1  = (np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)-0.5).astype(np.float32)\n",
    "w2  = (np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)-0.5).astype(np.float32)\n",
    "b2  = (np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)-0.5).astype(np.float32)\n",
    "p_k1 = np.copy(k1)\n",
    "p_bc1 = np.copy(bc1)\n",
    "p_k2 = np.copy(k2)\n",
    "p_bc2 = np.copy(bc2)\n",
    "p_k3 = np.copy(k3)\n",
    "p_bc3 = np.copy(bc3)\n",
    "p_w1 = np.copy(w1)\n",
    "p_b1 = np.copy(b1)\n",
    "p_w2 = np.copy(w2)\n",
    "p_b2 = np.copy(b2)\n",
    "modelComp = SimpleCNN()\n",
    "modelComp = modelComp.float()\n",
    "with torch.no_grad():\n",
    "    modelComp.conv1.weight.copy_(torch.from_numpy(p_k1))\n",
    "    modelComp.conv1.bias.copy_(torch.from_numpy(p_bc1))\n",
    "    modelComp.conv2.weight.copy_(torch.from_numpy(p_k2))\n",
    "    modelComp.conv2.bias.copy_(torch.from_numpy(p_bc2))\n",
    "    modelComp.conv3.weight.copy_(torch.from_numpy(p_k3))\n",
    "    modelComp.conv3.bias.copy_(torch.from_numpy(p_bc3))\n",
    "    modelComp.fc1.weight.copy_(torch.from_numpy(p_w1.T))\n",
    "    modelComp.fc1.bias.copy_(torch.from_numpy(p_b1).reshape(-1))\n",
    "    modelComp.fc2.weight.copy_(torch.from_numpy(p_w2.T))\n",
    "    modelComp.fc2.bias.copy_(torch.from_numpy(p_b2).reshape(-1))\n",
    "\n",
    "# Loss definition\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Optimisation definition\n",
    "lr = learning_rate = 0.001\n",
    "# SOSTITUISCI ADAM CON SGD\n",
    "optimizer = torch.optim.SGD(\n",
    "    modelComp.parameters(),\n",
    "    lr=learning_rate,\n",
    "    momentum=0,\n",
    "    dampening=0,\n",
    "    weight_decay=0,\n",
    "    nesterov=False\n",
    ")\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    c0 = (train_images[0].reshape(1,1,28,28)/255).astype(np.float32)\n",
    "    l0 = train_labels[0]\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "    torch_l0 = torch.from_numpy(np.array([np.argmax(l0)])).long()\n",
    "    #########################################################################\n",
    "    ########################### ColFast #####################################\n",
    "    #########################################################################\n",
    "        \n",
    "    c1f,mask1f = Fast_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "    c2f,mask2f = Fast_ReLU_Conv(c1f.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3f,mask3f = Fast_ReLU_Conv(c2f.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    fl_f,fa_f,sl_f,sa_f = ReLU_SoftMax_FullyConnected(imlpf,w1,b1,w2,b2)\n",
    "    print(sl_f)\n",
    "    # Loss\n",
    "    lossCF = crossEntropy(sa_f,l0)\n",
    "\n",
    "    #########################################################################\n",
    "    ########################### ColSlow #####################################\n",
    "    #########################################################################\n",
    "\n",
    "    c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "    c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl_s,fa_s,sl_s,sa_s = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    print(sl_f)\n",
    "    # Loss\n",
    "    lossCS = crossEntropy(sa_s,l0)\n",
    "\n",
    "    #########################################################################\n",
    "    ########################### PyTorch #####################################\n",
    "    #########################################################################\n",
    "\n",
    "    # make all gradients zero to avoid learning on gradients of previous steps\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = modelComp(torch_c0)\n",
    "    print(outputs)\n",
    "\n",
    "    print(\"Verifica pesi conv1:\")\n",
    "    print(\"  p_k1 vs modelComp.conv1.weight:\", np.allclose(k1, modelComp.conv1.weight.data.cpu().numpy()))\n",
    "    print(\"  p_bc1 vs modelComp.conv1.bias:\", np.allclose(bc1, modelComp.conv1.bias.data.cpu().numpy()))\n",
    "    print(\"Verifica pesi conv2:\")\n",
    "    print(\"  p_k1 vs modelComp.conv1.weight:\", np.allclose(k2, modelComp.conv2.weight.data.cpu().numpy()))\n",
    "    print(\"  p_bc1 vs modelComp.conv1.bias:\", np.allclose(bc2, modelComp.conv2.bias.data.cpu().numpy()))\n",
    "    print(\"Verifica pesi conv3:\")\n",
    "    print(\"  p_k1 vs modelComp.conv1.weight:\", np.allclose(k3, modelComp.conv3.weight.data.cpu().numpy()))\n",
    "    print(\"  p_bc1 vs modelComp.conv1.bias:\", np.allclose(bc3, modelComp.conv3.bias.data.cpu().numpy()))\n",
    "    print(\"Verifica pesi fc1:\")\n",
    "    print(\"  p_w1.T vs modelComp.fc1.weight:\", np.allclose(w1.T, modelComp.fc1.weight.data.cpu().numpy()))\n",
    "    print(\"  p_b1 vs modelComp.fc1.bias:\", np.allclose(b1.reshape(-1), modelComp.fc1.bias.data.cpu().numpy()))\n",
    "    print(\"Verifica pesi fc2:\")\n",
    "    print(\"  p_w1.T vs modelComp.fc1.weight:\", np.allclose(w2.T, modelComp.fc2.weight.data.cpu().numpy()))\n",
    "    print(\"  p_b1 vs modelComp.fc1.bias:\", np.allclose(b2.reshape(-1), modelComp.fc2.bias.data.cpu().numpy()))\n",
    "\n",
    "    l0_idx_flat = np.array([np.argmax(l0)]) # Shape (1,)\n",
    "\n",
    "    # Per ColFast (dopo aver calcolato sl_f)\n",
    "    log_probs_f = log_softmax_numpy(sl_f)\n",
    "    lossCF = nll_loss_numpy(log_probs_f, l0_idx_flat)\n",
    "    # print(f\"Loss ColFast (NLL): {lossCF}\") # Debug\n",
    "\n",
    "    # Per ColSlow (dopo aver calcolato sl_s)\n",
    "    log_probs_s = log_softmax_numpy(sl_s)\n",
    "    lossCS = nll_loss_numpy(log_probs_s, l0_idx_flat)\n",
    "\n",
    "    loss = criterion(outputs, torch_l0)\n",
    "\n",
    "    print(f\"Losses: p:{loss} cf:{lossCF} cs:{lossCS}\")\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(1,sa_s,train_labels[0],w1,w2,fa_s,fl_s,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad_fwd=0,stride_fwd=2)\n",
    "\n",
    "    gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad_fwd=1,stride_fwd=2)\n",
    "    gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad_fwd=0,stride_fwd=2)\n",
    "\n",
    "\n",
    "    print(\"\\n--- CONFRONTO GRADIENTI PRIMA EPOCA (dopo backward, prima di update) ---\")\n",
    "\n",
    "    # Layer FC2\n",
    "    grad_w2_torch = modelComp.fc2.weight.grad.data.cpu().numpy()\n",
    "    grad_b2_torch = modelComp.fc2.bias.grad.data.cpu().numpy()\n",
    "    print(f\"FC2 w2 close: {np.allclose(grad_w2_torch, dL_dw2.T, atol=1e-5)}\") # dL_dw2 è (F_mid, C_out), PyTorch è (C_out, F_mid)\n",
    "    print(f\"FC2 b2 close: {np.allclose(grad_b2_torch, dL_db2.reshape(-1), atol=1e-5)}\")\n",
    "    if not np.allclose(grad_w2_torch, dL_dw2.T, atol=1e-5):\n",
    "        print(\"PyTorch fc2.weight.grad (sample):\\n\", grad_w2_torch)\n",
    "        print(\"NumPy dL_dw2.T (sample):\\n\", dL_dw2.T)\n",
    "\n",
    "    # Layer FC1\n",
    "    grad_w1_torch = modelComp.fc1.weight.grad.data.cpu().numpy()\n",
    "    grad_b1_torch = modelComp.fc1.bias.grad.data.cpu().numpy()\n",
    "    print(f\"FC1 w1 close: {np.allclose(grad_w1_torch, dL_dw1.T, atol=1e-5)}\")\n",
    "    print(f\"FC1 b1 close: {np.allclose(grad_b1_torch, dL_db1.reshape(-1), atol=1e-5)}\")\n",
    "    if not np.allclose(grad_w1_torch, dL_dw1.T, atol=1e-5):\n",
    "        print(\"PyTorch fc1.weight.grad (sample):\\n\", grad_w1_torch)\n",
    "        print(\"NumPy dL_dw1.T (sample):\\n\", dL_dw1.T)\n",
    "\n",
    "    # Layer Conv3\n",
    "    grad_k3_torch = modelComp.conv3.weight.grad.data.cpu().numpy()\n",
    "    grad_bc3_torch = modelComp.conv3.bias.grad.data.cpu().numpy()\n",
    "    print(f\"Conv3 k3 close: {np.allclose(grad_k3_torch, gk3, atol=1e-5)}\")\n",
    "    print(f\"Conv3 bc3 close: {np.allclose(grad_bc3_torch, gb3.reshape(-1), atol=1e-5)}\")\n",
    "    if not np.allclose(grad_k3_torch, gk3, atol=1e-5):\n",
    "        print(\"PyTorch conv3.weight.grad (sample):\\n\", grad_k3_torch)\n",
    "        print(\"NumPy gk3 (sample):\\n\", gk3)\n",
    "\n",
    "    # Layer Conv2\n",
    "    grad_k2_torch = modelComp.conv2.weight.grad.data.cpu().numpy()\n",
    "    grad_bc2_torch = modelComp.conv2.bias.grad.data.cpu().numpy()\n",
    "    print(f\"Conv2 k2 close: {np.allclose(grad_k2_torch, gk2, atol=1e-5)}\")\n",
    "    print(f\"Conv2 bc2 close: {np.allclose(grad_bc2_torch, gb2.reshape(-1), atol=1e-5)}\")\n",
    "\n",
    "    # Layer Conv1\n",
    "    grad_k1_torch = modelComp.conv1.weight.grad.data.cpu().numpy()\n",
    "    grad_bc1_torch = modelComp.conv1.bias.grad.data.cpu().numpy()\n",
    "    print(f\"Conv1 k1 close: {np.allclose(grad_k1_torch, gk1, atol=1e-5)}\")\n",
    "    print(f\"Conv1 bc1 close: {np.allclose(grad_bc1_torch, gb1.reshape(-1), atol=1e-5)}\")\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3.reshape(-1)\n",
    "    bc2 -= lr*gb2.reshape(-1)\n",
    "    bc1 -= lr*gb1.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17423b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c55e8aa7",
   "metadata": {},
   "source": [
    "### Test for Fast approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84440e1b",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f98a2",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac1aa3",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b604f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = Fast_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "    c2s,mask2s = Fast_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = Fast_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = Fast_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "    gi2,gk2,gb2 = Fast_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "    gi1,gk1,gb1 = Fast_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7d327",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 0.0022 s\n",
    "- average backward time : 0.0097 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Fast Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3af563",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce21cfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Risultati NumPy ---\n",
      "grad_input_np:\n",
      " [[[[ 0.18224959  0.20094183  0.04806364]\n",
      "   [ 1.2589653   0.9540252   0.22438703]\n",
      "   [ 2.1727996   0.83869433 -0.15613317]]]]\n",
      "grad_kernel_np:\n",
      " [[[[3.8814468  3.4949708 ]\n",
      "   [3.0683658  0.29788435]]]]\n",
      "grad_bias_np:\n",
      " [2.0664585]\n",
      "\n",
      "--- Risultati PyTorch ---\n",
      "img_torch.grad:\n",
      " [[[[ 0.18224959  0.20094183  0.04806364]\n",
      "   [ 1.2589653   0.95402527  0.22438703]\n",
      "   [ 2.1727996   0.83869433 -0.15613317]]]]\n",
      "kernel_torch.grad:\n",
      " [[[[3.8814468  3.4949708 ]\n",
      "   [3.0683656  0.29788435]]]]\n",
      "bias_torch.grad:\n",
      " [2.0664585]\n",
      "\n",
      "--- Confronto dei Risultati ---\n",
      "Confronto grad_input: True\n",
      "Confronto grad_kernel: True\n",
      "Confronto grad_bias: True\n",
      "\n",
      "--- Verifica Dimensioni Output del Forward ---\n",
      "NumPy out_relu_np shape: (1, 1, 2, 2)\n",
      "PyTorch out_relu_torch shape: torch.Size([1, 1, 2, 2])\n",
      "\n",
      "--- Verifica Valori Output del Forward ---\n",
      "Forward outputs close: True\n",
      "\n",
      "\n",
      "--- TEST CON STRIDE=2, PADDING=1 ---\n",
      "--- Confronto dei Risultati (Stride=2, Padding=1) ---\n",
      "Confronto grad_input (s2p1): True\n",
      "Confronto grad_kernel (s2p1): True\n",
      "Confronto grad_bias (s2p1): True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Parametri del Test ---\n",
    "N = 1  # Batch size\n",
    "C_in = 1 # Canali di input\n",
    "H_in = 3 # Altezza input\n",
    "W_in = 3 # Larghezza input\n",
    "\n",
    "C_out = 1 # Canali di output\n",
    "K_h = 2  # Altezza Kernel\n",
    "K_w = 2  # Larghezza Kernel\n",
    "\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "# --- Dati di Input (NumPy) ---\n",
    "np.random.seed(0) # Per riproducibilità\n",
    "img_np = np.random.randn(N, C_in, H_in, W_in).astype(np.float32)\n",
    "# img_np = np.arange(1, N*C_in*H_in*W_in+1).reshape(N, C_in, H_in, W_in).astype(np.float32) / 10.0\n",
    "kernel_np = np.random.randn(C_out, C_in, K_h, K_w).astype(np.float32)\n",
    "# kernel_np = np.arange(1, C_out*C_in*K_h*K_w+1).reshape(C_out, C_in, K_h, K_w).astype(np.float32) / 5.0\n",
    "bias_np = np.random.randn(C_out).astype(np.float32)\n",
    "# bias_np = np.array([0.5], dtype=np.float32)\n",
    "\n",
    "\n",
    "# --- Calcolo Forward Pass (NumPy) ---\n",
    "# 1. Convoluzione\n",
    "out_conv_h_np = (H_in - K_h + 2 * padding) // stride + 1\n",
    "out_conv_w_np = (W_in - K_w + 2 * padding) // stride + 1\n",
    "out_conv_np = np.zeros((N, C_out, out_conv_h_np, out_conv_w_np), dtype=np.float32)\n",
    "\n",
    "img_padded_np = np.pad(img_np, ((0,0), (0,0), (padding,padding), (padding,padding)), mode='constant')\n",
    "\n",
    "for n in range(N):\n",
    "    for c_o in range(C_out):\n",
    "        for r_o in range(out_conv_h_np):\n",
    "            for c_o_conv in range(out_conv_w_np):\n",
    "                val = bias_np[c_o]\n",
    "                for c_i in range(C_in):\n",
    "                    for r_k in range(K_h):\n",
    "                        for c_k in range(K_w):\n",
    "                            img_r = r_o * stride + r_k\n",
    "                            img_c = c_o_conv * stride + c_k\n",
    "                            val += img_padded_np[n, c_i, img_r, img_c] * kernel_np[c_o, c_i, r_k, c_k]\n",
    "                out_conv_np[n, c_o, r_o, c_o_conv] = val\n",
    "\n",
    "# 2. ReLU\n",
    "out_relu_np = np.maximum(0, out_conv_np)\n",
    "mask_relu_np = (out_conv_np > 0).astype(np.float32)\n",
    "\n",
    "\n",
    "# --- Gradiente dell'Output (NumPy) - Simula il gradiente dal layer successivo ---\n",
    "# d_out_relu_np è il \"d_out_values\" per la tua funzione\n",
    "d_out_relu_np = np.random.randn(N, C_out, out_conv_h_np, out_conv_w_np).astype(np.float32)\n",
    "# d_out_relu_np = np.ones_like(out_relu_np).astype(np.float32) * 0.1\n",
    "\n",
    "\n",
    "# --- Calcolo Backward Pass (Tua Implementazione NumPy) ---\n",
    "grad_input_np, grad_kernel_np, grad_bias_np = Slow_ReLU_Gradient(\n",
    "    img_np, d_out_relu_np, kernel_np, mask_relu_np, pad_fwd=padding, stride_fwd=stride\n",
    ")\n",
    "\n",
    "print(\"--- Risultati NumPy ---\")\n",
    "print(\"grad_input_np:\\n\", grad_input_np)\n",
    "print(\"grad_kernel_np:\\n\", grad_kernel_np)\n",
    "print(\"grad_bias_np:\\n\", grad_bias_np)\n",
    "\n",
    "\n",
    "# --- Calcolo con PyTorch per Confronto ---\n",
    "img_torch = torch.tensor(img_np, requires_grad=True)\n",
    "kernel_torch = torch.tensor(kernel_np, requires_grad=True)\n",
    "bias_torch = torch.tensor(bias_np, requires_grad=True)\n",
    "\n",
    "# Forward pass PyTorch\n",
    "out_conv_torch = F.conv2d(img_torch, kernel_torch, bias_torch, stride=stride, padding=padding)\n",
    "out_relu_torch = F.relu(out_conv_torch)\n",
    "\n",
    "# Backward pass PyTorch\n",
    "# Passiamo lo stesso gradiente dell'output che abbiamo usato per NumPy\n",
    "d_out_relu_torch = torch.tensor(d_out_relu_np)\n",
    "out_relu_torch.backward(gradient=d_out_relu_torch)\n",
    "\n",
    "print(\"\\n--- Risultati PyTorch ---\")\n",
    "print(\"img_torch.grad:\\n\", img_torch.grad.data.numpy())\n",
    "print(\"kernel_torch.grad:\\n\", kernel_torch.grad.data.numpy())\n",
    "print(\"bias_torch.grad:\\n\", bias_torch.grad.data.numpy())\n",
    "\n",
    "\n",
    "# --- Confronto ---\n",
    "print(\"\\n--- Confronto dei Risultati ---\")\n",
    "print(\"Confronto grad_input:\", np.allclose(grad_input_np, img_torch.grad.data.numpy(), atol=1e-5))\n",
    "print(\"Confronto grad_kernel:\", np.allclose(grad_kernel_np, kernel_torch.grad.data.numpy(), atol=1e-5))\n",
    "print(\"Confronto grad_bias:\", np.allclose(grad_bias_np, bias_torch.grad.data.numpy(), atol=1e-5))\n",
    "\n",
    "print(\"\\n--- Verifica Dimensioni Output del Forward ---\")\n",
    "print(\"NumPy out_relu_np shape:\", out_relu_np.shape)\n",
    "print(\"PyTorch out_relu_torch shape:\", out_relu_torch.shape)\n",
    "assert out_relu_np.shape == out_relu_torch.shape, \"Le dimensioni dell'output del forward non corrispondono!\"\n",
    "\n",
    "print(\"\\n--- Verifica Valori Output del Forward ---\")\n",
    "print(\"Forward outputs close:\", np.allclose(out_relu_np, out_relu_torch.detach().numpy(), atol=1e-5))\n",
    "if not np.allclose(out_relu_np, out_relu_torch.detach().numpy(), atol=1e-5):\n",
    "    print(\"NumPy out_relu_np:\\n\", out_relu_np)\n",
    "    print(\"PyTorch out_relu_torch:\\n\", out_relu_torch.detach().numpy())\n",
    "\n",
    "\n",
    "# Test con STRIDE = 2, PADDING = 1\n",
    "print(\"\\n\\n--- TEST CON STRIDE=2, PADDING=1 ---\")\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# --- Dati di Input (NumPy) ---\n",
    "img_np_s2p1 = np.random.randn(N, C_in, H_in+1, W_in+1).astype(np.float32) # Immagine leggermente più grande\n",
    "kernel_np_s2p1 = np.random.randn(C_out, C_in, K_h, K_w).astype(np.float32)\n",
    "bias_np_s2p1 = np.random.randn(C_out).astype(np.float32)\n",
    "\n",
    "# --- Calcolo Forward Pass (NumPy) ---\n",
    "out_conv_h_np_s2p1 = (img_np_s2p1.shape[2] - K_h + 2 * padding) // stride + 1\n",
    "out_conv_w_np_s2p1 = (img_np_s2p1.shape[3] - K_w + 2 * padding) // stride + 1\n",
    "out_conv_np_s2p1 = np.zeros((N, C_out, out_conv_h_np_s2p1, out_conv_w_np_s2p1), dtype=np.float32)\n",
    "img_padded_np_s2p1 = np.pad(img_np_s2p1, ((0,0), (0,0), (padding,padding), (padding,padding)), mode='constant')\n",
    "for n in range(N):\n",
    "    for c_o in range(C_out):\n",
    "        for r_o in range(out_conv_h_np_s2p1):\n",
    "            for c_o_conv in range(out_conv_w_np_s2p1):\n",
    "                val = bias_np_s2p1[c_o]\n",
    "                for c_i in range(C_in):\n",
    "                    for r_k in range(K_h):\n",
    "                        for c_k in range(K_w):\n",
    "                            img_r = r_o * stride + r_k\n",
    "                            img_c = c_o_conv * stride + c_k\n",
    "                            val += img_padded_np_s2p1[n, c_i, img_r, img_c] * kernel_np_s2p1[c_o, c_i, r_k, c_k]\n",
    "                out_conv_np_s2p1[n, c_o, r_o, c_o_conv] = val\n",
    "out_relu_np_s2p1 = np.maximum(0, out_conv_np_s2p1)\n",
    "mask_relu_np_s2p1 = (out_conv_np_s2p1 > 0).astype(np.float32)\n",
    "d_out_relu_np_s2p1 = np.random.randn(N, C_out, out_conv_h_np_s2p1, out_conv_w_np_s2p1).astype(np.float32)\n",
    "\n",
    "# --- Calcolo Backward Pass (Tua Implementazione NumPy) ---\n",
    "grad_input_np_s2p1, grad_kernel_np_s2p1, grad_bias_np_s2p1 = Slow_ReLU_Gradient(\n",
    "    img_np_s2p1, d_out_relu_np_s2p1, kernel_np_s2p1, mask_relu_np_s2p1, pad_fwd=padding, stride_fwd=stride\n",
    ")\n",
    "\n",
    "# --- Calcolo con PyTorch per Confronto ---\n",
    "img_torch_s2p1 = torch.tensor(img_np_s2p1, requires_grad=True)\n",
    "kernel_torch_s2p1 = torch.tensor(kernel_np_s2p1, requires_grad=True)\n",
    "bias_torch_s2p1 = torch.tensor(bias_np_s2p1, requires_grad=True)\n",
    "out_conv_torch_s2p1 = F.conv2d(img_torch_s2p1, kernel_torch_s2p1, bias_torch_s2p1, stride=stride, padding=padding)\n",
    "out_relu_torch_s2p1 = F.relu(out_conv_torch_s2p1)\n",
    "d_out_relu_torch_s2p1 = torch.tensor(d_out_relu_np_s2p1)\n",
    "out_relu_torch_s2p1.backward(gradient=d_out_relu_torch_s2p1)\n",
    "\n",
    "print(\"--- Confronto dei Risultati (Stride=2, Padding=1) ---\")\n",
    "print(\"Confronto grad_input (s2p1):\", np.allclose(grad_input_np_s2p1, img_torch_s2p1.grad.data.numpy(), atol=1e-5))\n",
    "print(\"Confronto grad_kernel (s2p1):\", np.allclose(grad_kernel_np_s2p1, kernel_torch_s2p1.grad.data.numpy(), atol=1e-5))\n",
    "print(\"Confronto grad_bias (s2p1):\", np.allclose(grad_bias_np_s2p1, bias_torch_s2p1.grad.data.numpy(), atol=1e-5))\n",
    "\n",
    "if not np.allclose(grad_input_np_s2p1, img_torch_s2p1.grad.data.numpy(), atol=1e-5):\n",
    "    print(\"NumPy grad_input_s2p1:\\n\", grad_input_np_s2p1)\n",
    "    print(\"PyTorch grad_input_s2p1:\\n\", img_torch_s2p1.grad.data.numpy())\n",
    "if not np.allclose(grad_kernel_np_s2p1, kernel_torch_s2p1.grad.data.numpy(), atol=1e-5):\n",
    "    print(\"NumPy grad_kernel_s2p1:\\n\", grad_kernel_np_s2p1)\n",
    "    print(\"PyTorch grad_kernel_s2p1:\\n\", kernel_torch_s2p1.grad.data.numpy())\n",
    "if not np.allclose(grad_bias_np_s2p1, bias_torch_s2p1.grad.data.numpy(), atol=1e-5):\n",
    "    print(\"NumPy grad_bias_s2p1:\\n\", grad_bias_np_s2p1)\n",
    "    print(\"PyTorch grad_bias_s2p1:\\n\", bias_torch_s2p1.grad.data.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
