{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42490d20",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b0d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "#-------------- Data Extraction ---------------------------\n",
    "\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcf859",
   "metadata": {},
   "source": [
    "## CNN - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c17803",
   "metadata": {},
   "source": [
    "The PyTorch model will be used as a reference to compute the weights since it's the fastest in training and the least prone to errors. If everything is written well, both slow and fast implementations of a CNN in numpy will give the same result, since the weights are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c11b9",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a1d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 1.------------------ CNN declaration -------------------\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # Second Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # Third Convolution\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9541aa5",
   "metadata": {},
   "source": [
    "### Weights extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5906ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "----------------------------------------\n",
      "[[[-0.06239345  0.16331542  0.28573602]\n",
      "  [ 0.299534    0.48019555  0.25194943]\n",
      "  [-0.24432278  0.3191273  -0.06802213]]]\n",
      "----------------------------------------\n",
      "[[[-0.06239345  0.16331542  0.28573602]\n",
      "  [ 0.299534    0.48019555  0.25194943]\n",
      "  [-0.24432278  0.3191273  -0.06802213]]]\n",
      "k1: PyTorch Shape=(32, 1, 3, 3), NumPy Shape=(32, 1, 3, 3)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 3, 3), NumPy Shape=(64, 32, 3, 3)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 3, 3), NumPy Shape=(128, 64, 3, 3)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pyt_k1_w = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pyt_k1_w\n",
    "print(\"----------------------------------------\")\n",
    "print(pyt_k1_w[0][:1])\n",
    "print(\"----------------------------------------\")\n",
    "print(numpy_weights['k1'][0][:1])\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pyt_k1_w.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pyt_k2_w = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pyt_k2_w\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pyt_k2_w.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pyt_k3_w = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pyt_k3_w\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pyt_k3_w.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pyt_w1 = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pyt_w1.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pyt_b1 = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pyt_b1.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pyt_w1.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pyt_b1.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pyt_w2 = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pyt_w2.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pyt_b2 = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pyt_b2.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pyt_w2.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pyt_b2.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']\n",
    "\n",
    "\n",
    "\n",
    "# [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]]\n",
    "\n",
    "\n",
    "#  [[[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]]\n",
    "\n",
    "\n",
    "\n",
    "#    [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]\n",
    "\n",
    "#   [[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba32882",
   "metadata": {},
   "source": [
    "## CNN - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1185e05",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b49fb",
   "metadata": {},
   "source": [
    "`np.pad()` takes as first argument the matrix to pad and as second argument a set of specification: for every dimension (in our case 4) it takes the number of paddings to add before and after the end of the dimension. If the objective is to pad only the image itself, which is found in the last two dimension, we should write:\n",
    "\n",
    "`np.pad(img9,((0,0),(0,0),(pad,pad),(pad,pad)))` \n",
    "\n",
    "since dimensions are: BATCH, CHANNELS, HEIGHT, WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0599825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "img9 = np.arange(1,37).reshape(2,2,3,3)\n",
    "pad_img9 = np.pad(img9,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(img9)\n",
    "print(pad_img9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d1319",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c26aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[ 1.  2.  3.  4.]\n",
      "   [ 5.  6.  7.  8.]\n",
      "   [ 9. 10. 11. 12.]]]]\n",
      "-------ker-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[  5.  19.  13.]\n",
      "   [ 47.  95.  45.]]\n",
      "\n",
      "  [[ 10.  40.  30.]\n",
      "   [104. 232. 126.]]]]\n",
      "-------Conv PyTorch-------\n",
      "tensor([[[[  5.,  19.,  13.],\n",
      "          [ 47.,  95.,  45.]],\n",
      "\n",
      "         [[ 10.,  40.,  30.],\n",
      "          [104., 232., 126.]]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This is a PyTorch Convolution example to be used to check if the convolution implemented in both slow and fast approaches are correct\n",
    "\n",
    "class CustomConv(nn.Module):\n",
    "    def __init__(self, kernel: torch.Tensor, bias: torch.Tensor = None, \n",
    "                 stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernel.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(bias is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernel)\n",
    "            if bias is not None:\n",
    "                self.conv.bias.copy_(bias)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if bias is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "def Slow_ReLU_Conv(img,ker,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        out_ch, in_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = out_ch\n",
    "    else: # Backward case\n",
    "        in_ch, out_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = in_ch\n",
    "\n",
    "    # bias has shape out_ch, 1, 1. It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # number of channels taken in input by the kernel 'in_ch' \n",
    "    # must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    n_images, channels, i_height, i_width  = img.shape\n",
    "    ni_height = int(((i_height - k_height) / stride) + 1) # new image height # Padding is already added\n",
    "    ni_width = int(((i_width - k_width) / stride) + 1) # new image width\n",
    "    ni = np.zeros((n_images, out_ch, ni_height, ni_width)).astype(np.float32) # new image\n",
    "\n",
    "    if in_ch != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({in_ch}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for one_img in range(n_images):\n",
    "        for one_k_channel in range(nk_channel):\n",
    "            for i_nih in range(ni_height): # which cycles row by row of the new image\n",
    "                for i_niw in range(ni_width): # which cycles column by column of the new image\n",
    "                    current_sum = 0.0 # convolution sum for the specific output cell\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(channels): # channels == in_ch\n",
    "                        for i_kh in range(k_height):\n",
    "                            input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "                            for i_kw in range(k_width):\n",
    "                                input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "                                    input_val = img[one_img, channel, input_y, input_x]\n",
    "                                    kernel_val = ker[one_k_channel, channel, i_kh, i_kw]\n",
    "                                    current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "                    ni[one_img, one_k_channel, i_nih, i_niw] = current_sum\n",
    "    if bias.all() != 0:\n",
    "        bias = bias.reshape(bias.shape[0],1,1)\n",
    "        if bias.shape[0] != out_ch:\n",
    "            raise ValueError(f\"bias dimension ({bias.shape[0]}) doesn't match kernel's number of channels ({out_ch})\")\n",
    "        ni = ni + bias\n",
    "    ni = ni.astype(np.float32)\n",
    "    if applyReLU:\n",
    "        ni = np.maximum(0, ni)\n",
    "        mask = ni.copy()\n",
    "        mask[mask > 0] = 1\n",
    "        return ni,mask\n",
    "    else:\n",
    "        return ni\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "img = np.arange(1,3*4+1).reshape(1,1,3,4).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=1,stride=2)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "\n",
    "my_kernel = torch.from_numpy(ker).float()\n",
    "\n",
    "my_bias = torch.from_numpy(np.array([1,2])).float()\n",
    "\n",
    "modelC = CustomConv(kernel=my_kernel,bias=my_bias, stride=2, padding=1)\n",
    "\n",
    "# input di prova (batch=1, canali=1, H=5, W=5)\n",
    "x = torch.from_numpy(img)\n",
    "y = modelC(x)\n",
    "print(\"-------Conv PyTorch-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec0a01",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805b797",
   "metadata": {},
   "source": [
    "**Actors:**\n",
    "1. W is the kernel\n",
    "2. $\\delta$ is the gradient\n",
    "3. x is the input to the convolution layer during forward\n",
    "4. b is the bias\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Derive delta**\n",
    "\n",
    "Deriving delta with respect to ReLU activation consists in the hadamard product (element-wise product) of the gradient ($\\delta$) and the mask obtained at the forward step, that is, all the elements in the convolved image greater than zero are put to one, the rest is zero.\n",
    "$$\n",
    "\\delta^{(i)} = \\delta_{\\text{flat reshaped}} \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "- **Gradient with respect to W**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(x^{(i)}, \\delta)\n",
    "$$\n",
    "This convolution creates a matrix for every channel of input image $x^{i}$ and for every channel of output image $\\delta$, thus resulting in the correct number of channels\n",
    "\n",
    "- **Gradient w.r.t. the input \\( x \\)** (To go to the preceding layer):\n",
    "\n",
    "$$\n",
    "\\delta^{(i-1)} = \\text{Full\\_Convolution}(\\delta^{(i)}, W^{(i)})\n",
    "$$\n",
    "\n",
    "- **Gradient w.r.t the bias**\n",
    "\n",
    "Since the bias is added equally across the spatial dimensions of each output channel, the gradient is the sum of all elements in each output channel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{h,w} \\delta^{(i)}_{c,h,w}\n",
    "$$\n",
    "\n",
    "For batched inputs, sum also across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n,h,w} \\delta^{(i)}_{n,c,h,w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "89be3c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------d_img--------------\n",
      "[[[[ -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      "   [ -1.   3.  10.  17.  24.  11.  -1.]\n",
      "   [ -1.  21.  43.  53.  63.  27.  -1.]\n",
      "   [ -1.  45.  83.  93. 103.  43.  -1.]\n",
      "   [ -1.  69. 123. 133. 143.  59.  -1.]\n",
      "   [ -1.  25.  40.  43.  46.  15.  -1.]\n",
      "   [ -1.  -1.  -1.  -1.  -1.  -1.  -1.]]\n",
      "\n",
      "  [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      "   [ -1.   7.  22.  37.  52.  27.  -1.]\n",
      "   [ -1.  45.  99. 125. 151.  75.  -1.]\n",
      "   [ -1. 101. 203. 229. 255. 123.  -1.]\n",
      "   [ -1. 157. 307. 333. 359. 171.  -1.]\n",
      "   [ -1.  77. 148. 159. 170.  79.  -1.]\n",
      "   [ -1.  -1.  -1.  -1.  -1.  -1.  -1.]]]]\n",
      "(1, 2, 7, 7)\n",
      "--------------------------------\n",
      "[[[[ -12.  -22.  -22.  -10.]\n",
      "   [ -20.   44.  260.  178.]\n",
      "   [ -20.  476. 1532.  942.]\n",
      "   [  -8.  306.  926.  538.]]]]\n",
      "[[[[ 10.  -8.]\n",
      "   [-11.  -6.]]]\n",
      "\n",
      "\n",
      " [[[ 34.  -8.]\n",
      "   [-11.  -6.]]]]\n",
      "[1311. 3487.]\n"
     ]
    }
   ],
   "source": [
    "def Slow_ReLU_FullConv(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "\n",
    "    out_ch, in_ch, k_height, k_width  = ker.shape \n",
    "                                            # Example #\n",
    "    # Convolving an RGB image with 32 2x2 kernels will give a shape of (32, 3, 2, 2) to the kernel. #\n",
    "    \n",
    "    n_images, channels, i_height, i_width  = img.shape\n",
    "    n_images, dch, di_height, di_width  = d_img.shape\n",
    "    \n",
    "    ni_height = (i_height-1)*stride-(2*pad)+k_height # new image height\n",
    "    ni_width =  (i_width-1)*stride-(2*pad)+k_width # new image width\n",
    "    height_to_pad = ni_height-i_height\n",
    "    width_to_pad = ni_width-i_width\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "    d_imgP = np.pad(d_img,((0,0),(0,0),(height_to_pad,height_to_pad),(width_to_pad,width_to_pad)))\n",
    "    gi = np.zeros_like(img).astype(np.float32) # gradient of original image\n",
    "    gk = np.zeros_like(ker).astype(np.float32) # gradient of kernel\n",
    "\n",
    "############################## Computing the gradient of the original image ######################################\n",
    "    current_sum = 0.0 # convolution sum for the specific output cell\n",
    "    for one_img in range(n_images):\n",
    "        for channel in range(channels):\n",
    "            for i_nih in range(i_height): # which cycles row by row of the new image\n",
    "                for i_niw in range(i_width): # which cycles column by column of the new image\n",
    "                    # Convolution cycles\n",
    "                    for one_k_channel in range(out_ch): # channels == out_ch\n",
    "                        for i_kh in range(k_height):\n",
    "                            input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "                            for i_kw in range(k_width):\n",
    "                                input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "                                    input_val = d_imgP[one_img, one_k_channel, input_y, input_x]\n",
    "                                    kernel_val = ker[one_k_channel,channel, i_kh, i_kw]\n",
    "                                    current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "                    gi[one_img, channel, i_nih, i_niw] = current_sum\n",
    "                    current_sum = 0.0\n",
    "    \n",
    "############################## Computing the gradient of the kernel ##############################################\n",
    "# Need to convolve the gradient of the image with the original image, using the gradient of the image as the kernel and \n",
    "# keeping the same stride and padding (Otherwise the kernel won't work)\n",
    "    current_sum = 0.0\n",
    "    for one_img in range(n_images):\n",
    "        for in_k_ch in range(in_ch): # which in the example is 3\n",
    "            for out_k_ch in range(out_ch): # which in the example is 32\n",
    "                for k_gh in range(k_height):\n",
    "                    for k_gw in range(k_width):\n",
    "                    # gk[out_k_ch,in_k_ch,k_gh,k_gw] = something\n",
    "                        for i_dh in range(di_height):\n",
    "                            input_y = (k_gh * stride) + i_dh # get the y location, the height\n",
    "                            for i_dw in range(di_width):\n",
    "                                input_x = (k_gw * stride) + i_dw\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < k_height and 0 <= input_x < k_width:\n",
    "                                    input_val = img[one_img, in_k_ch, input_y, input_x]\n",
    "                                    kernel_val = d_img[one_img, out_k_ch, i_dh, i_dw]\n",
    "                                    current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "                        gk[out_k_ch, in_k_ch, k_gh, k_gw] += current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "############################## Computing the gradient of the bias ##############################################\n",
    "    gb = d_img.sum((0,-1,-2)) # sum over batch, height and width\n",
    "################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "img = np.arange(1,17).reshape(1,1,4,4)\n",
    "ker = np.arange(1,9).reshape(2,1,2,2)\n",
    "bias = np.array([1,1])\n",
    "d_img,mask=Slow_ReLU_Conv(img,ker,bias,pad=2)\n",
    "print(\"-------------d_img--------------\")\n",
    "d_img = d_img - 2\n",
    "print(d_img)\n",
    "print(d_img.shape)\n",
    "print(\"--------------------------------\")\n",
    "a,b,c = Slow_ReLU_FullConv(img,d_img,ker,mask)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876559ef",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3469f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Conv Slow-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Conv(batch_of_images,kernel,bias,pad=0,stride=1):\n",
    "    kc, ac, kw, kh = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    # im2col: Window creation\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    window_m = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))[:,:,::stride,::stride].reshape((-1,(kw*kh*nc))) # window matrix\n",
    "    # Convolution\n",
    "    kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)\n",
    "    c_m = (window_m @ kernel).astype(np.float32) # convolved image matrix\n",
    "    # ReLU activation\n",
    "    nih = int(((ih-kh) / stride) + 1) # new image height # Padding is already added\n",
    "    niw = int(((iw-kw) / stride) + 1) # new image width\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output_temp = c_m.reshape(bs, nih, niw, kc)\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    reshaped_correct_order = output_temp.transpose(0,3,1,2)\n",
    "    reshaped_correct_order = (reshaped_correct_order + bias.reshape(1,-1,1,1)).astype(np.float32)\n",
    "    reshaped_correct_order = np.maximum(0,reshaped_correct_order)\n",
    "    mask = np.copy(reshaped_correct_order)\n",
    "    mask[mask>0]=1\n",
    "    return reshaped_correct_order,mask\n",
    "\n",
    "\n",
    "\n",
    "img = np.arange(1,2*3*3+1).reshape(1,2,3,3).astype(np.float32)\n",
    "# print(\"-------img-------\")\n",
    "# print(img)\n",
    "ker = np.arange(1,16+1).reshape(2,2,2,2)\n",
    "# print(\"-------ker-------\")\n",
    "# print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(img,ker,bias,pad = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)\n",
    "res,mask = Slow_ReLU_Conv(res,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(X_c,ker,bias,pad = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91cb44",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f076b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fast_ReLU_FullConv(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    kc, ac, kh, kw = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "    # im2col: Window creation\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    window_m = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))[:,:,::stride,::stride].reshape((-1,(kw*kh*nc))) # window matrix\n",
    "    # Convolution\n",
    "    kernel = kernel.reshape((-1,(kw*kh*nc))).transpose(1,0)\n",
    "    c_m = (window_m @ kernel).astype(np.float32) # convolved image matrix\n",
    "    # ReLU activation\n",
    "    nih = int(((ih-kh) / stride) + 1) # new image height # Padding is already added\n",
    "    niw = int(((iw-kw) / stride) + 1) # new image width\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output_temp = c_m.reshape(bs, nih, niw, kc)\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    reshaped_correct_order = output_temp.transpose(0,3,1,2)\n",
    "    reshaped_correct_order = (reshaped_correct_order + bias.reshape(1,-1,1,1)).astype(np.float32)\n",
    "    reshaped_correct_order = np.maximum(0,reshaped_correct_order)\n",
    "    mask = np.copy(reshaped_correct_order)\n",
    "    mask[mask>0]=1\n",
    "##################################################################################################################\n",
    "    out_ch, in_ch, k_height, k_width  = ker.shape \n",
    "                                            # Example #\n",
    "    # Convolving a an RGB image with 32 2x2 kernels will give a shape of (32, 3, 2, 2) to the kernel. #\n",
    "    \n",
    "    n_images, channels, i_height, i_width  = img.shape\n",
    "    n_images, dch, di_height, di_width  = d_img.shape\n",
    "    \n",
    "    ni_height = (i_height-1)*stride-(2*pad)+k_height # new image height\n",
    "    ni_width =  (i_width-1)*stride-(2*pad)+k_width# new image width\n",
    "    height_to_pad = ni_height-i_height\n",
    "    width_to_pad = ni_width-i_width\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "    d_imgP = np.pad(d_img,((0,0),(0,0),(width_to_pad,width_to_pad),(height_to_pad,height_to_pad)))\n",
    "    window_m = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,nc,kw,kh))[:,:,::stride,::stride].reshape((-1,(kw*kh*nc))) # window matrix\n",
    "    # Convolution\n",
    "    kernel = ker.reshape((-1,(kw*kh*nc))).transpose(1,0)\n",
    "    c_m = (window_m @ kernel).astype(np.float32) # convolved image matrix\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output_temp = c_m.reshape(bs, nih, niw, kc)\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    gi = output_temp.transpose(0,3,1,2).astype(np.float32)\n",
    "############################## Computing the gradient of the original image ######################################\n",
    "    gk = 0\n",
    "\n",
    "\n",
    "############################## Computing the gradient of the kernel ##############################################\n",
    "\n",
    "\n",
    "\n",
    "############################## Computing the gradient of the bias ##############################################\n",
    "    gb = d_img.sum((0,-1,-2)) # sum over batch, height and width\n",
    "################################################### Return Results ###############################################    \n",
    "\n",
    "\n",
    "\n",
    "    return gi,gk,gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc8707",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70cbf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa\n",
    "\n",
    "#print(softmax([1,2,3,100000]))\n",
    "#print(softmax_no_NS([1,2,3,1000]))\n",
    "#r = np.array(np.array([1,2,777,2]))\n",
    "#print(softmax(r))\n",
    "#r = np.array((np.array([1,2,777,2]),np.array([1,2,777,2]),np.array([1,2,777,2])))\n",
    "#print(softmax(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108f244",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b7942596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels[0:bs]\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d3b94",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb425b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(p,t):\n",
    "    # p stands for prediction and t stands for true label\n",
    "    # p = [0,0,1] and t = [1,0,0]\n",
    "    p = p+(1/100000) # for numerical stability\n",
    "    return -np.dot(t,np.log(p).T)\n",
    "\n",
    "#c = [1,1000000000000000,1,1]\n",
    "#c = softmax(c)\n",
    "#print(c)\n",
    "#c = crossEntropy(c,[0,1,0,0])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fd4f5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad189b98",
   "metadata": {},
   "source": [
    "In this section the three implementations will be compared in terms of time. Recall that all the predictions should be the same since the weights are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 100/100 [07:22<00:00,  4.43s/it, average_times=t: 0.0018 s, s: 4.4195 s, f: 0.0023 s, correct_predictions=100.0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward execution time in seconds: \n",
      "PyTorch: 0.0018 s, \n",
      "Slow: 4.4195 s, \n",
      "Fast: 0.0023 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "# np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "# np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "# np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "# np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "# np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "# np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "# np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "# np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "# np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "# dict_times={}\n",
    "# dict_times[\"ctorch\"]=[]\n",
    "# dict_times[\"cslow\"]=[]\n",
    "# dict_times[\"cfast\"]=[]\n",
    "\n",
    "# dict_pred={}\n",
    "# dict_pred[\"ctorch\"]=[]\n",
    "# dict_pred[\"cslow\"]=[]\n",
    "# dict_pred[\"cfast\"]=[]\n",
    "\n",
    "# #length = test_labels.shape[0]\n",
    "# length = 100\n",
    "# correct = 0\n",
    "# loop = tqdm(range(length),desc=\" Inferring...\")\n",
    "# for i in loop:\n",
    "#     c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "#     torch_c0 = torch.from_numpy(c0).float()\n",
    "#     ############### CNN PyTorch Implementation ##################\n",
    "#     start_time = time.time()\n",
    "#     outputs = model(torch_c0)\n",
    "#     end_time = time.time()\n",
    "#     _, predicted1 = torch.max(outputs.data, 1)\n",
    "#     dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "#     dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "#     ############### CNN Slow Implementation #####################\n",
    "#     start_time = time.time()\n",
    "#     c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=1,stride=2)\n",
    "#     c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "#     c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),np_k3,np_b_conv3,pad=1,stride=2)\n",
    "#     imlps = c3s.reshape(1,-1)\n",
    "#     _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "#     predicted2 = np.argmax(res,1)\n",
    "#     end_time = time.time()\n",
    "#     dict_times[\"cslow\"].append(end_time-start_time)\n",
    "#     dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "#     ############### CNN Fast Implementation #####################\n",
    "#     start_time = time.time()\n",
    "#     c1f,mask1f = Fast_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=1,stride=2)\n",
    "#     c2f,mask2f = Fast_ReLU_Conv(c1f.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "#     c3f,mask3f = Fast_ReLU_Conv(c2f.astype(np.float32),np_k3,np_b_conv3,pad=1,stride=2)\n",
    "#     imlpf = c3f.reshape(1,-1)\n",
    "#     _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "#     predicted3 = np.argmax(res,1)\n",
    "#     end_time = time.time()\n",
    "#     dict_times[\"cfast\"].append(end_time-start_time)\n",
    "#     dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "#     #####################################################################################\n",
    "#     #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "#     t = int(predicted1[0])\n",
    "#     s = int(predicted2[0])\n",
    "#     f = int(predicted3[0])\n",
    "#     if t == s and t == f:\n",
    "#         correct+=1\n",
    "#     #####################################################################################\n",
    "#     ### Keep track of the times #########################################################\n",
    "#     loop.set_postfix(average_times =f\"t: {round(sum(dict_times['ctorch'])/(i+1),4)} s, s: {round(sum(dict_times['cslow'])/(i+1),4)} s, f: {round(sum(dict_times['cfast'])/(i+1),4)} s\" , correct_predictions=f\"{100*correct/(i+1)}%\")\n",
    "# print(f\"Average forward execution time in seconds: \\nPyTorch: {round(sum(dict_times['ctorch'])/(i+1),4)} s, \\nSlow: {round(sum(dict_times['cslow'])/(i+1),4)} s, \\nFast: {round(sum(dict_times['cfast'])/(i+1),4)} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae1ea1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97018854",
   "metadata": {},
   "source": [
    "### Test for Slow approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b559791",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf57c6",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3b00564",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c8760193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88167606",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8fe14aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:36<00:00, 10.81s/it, avgBackward=5.8127 s, avgForward=4.9967 s, pendence=[-3.8954755e-05]] \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=1,stride=2)\n",
    "    c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=1,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "    gi3,gk3,gb3 = Slow_ReLU_FullConv(c2s,dL_i_mlp,k3,mask3s,pad=1,stride=2)\n",
    "    gi2,gk2,gb2 = Slow_ReLU_FullConv(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "    gi1,gk1,gb1 = Slow_ReLU_FullConv(c0,gi2,k1,mask1s,pad=1,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d3720",
   "metadata": {},
   "source": [
    "#### First 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f628e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
