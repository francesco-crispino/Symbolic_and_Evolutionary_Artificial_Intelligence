{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106183a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05471163",
   "metadata": {},
   "source": [
    "### Dataset Loading and Preprocessing\n",
    "\n",
    "This section handles the loading and initial preparation of the MNIST dataset. MNIST contains 28x28 pixel grayscale images of handwritten digits (0-9).\n",
    "\n",
    "**Key Operations:**\n",
    "\n",
    "1.  **Data Loading (`load_mnist_images`, `load_mnist_labels`):**\n",
    "    *   These functions read the MNIST dataset from its specific binary file format. `struct.unpack` is used to parse metadata (like image dimensions and count) from the file headers.\n",
    "    *   Image data is read as a flat byte array and then reshaped to `(num_images, rows, cols)`.\n",
    "\n",
    "2.  **One-Hot Encoding Labels:**\n",
    "    *   For multi-class classification with a softmax output and categorical cross-entropy loss, integer labels (e.g., digit `5`) are converted into a one-hot vector format (e.g., `[0,0,0,0,0,1,0,0,0,0]` for 10 classes).\n",
    "    *   This represents the true label as a probability distribution where the correct class has a probability of 1.\n",
    "    *   **Formula:** For a label $y_i$ and $K$ classes, the one-hot vector $Y_i$ has elements $Y_{i,j}$:\n",
    "        $$ Y_{i,j} = \\begin{cases} 1 & \\text{if } j = y_i \\\\ 0 & \\text{otherwise} \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9edcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Leggi intestazione: magic number, numero immagini, righe, colonne\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Leggi tutti i pixel e convertili in array numpy\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Ridimensiona l'array in (num_images, rows, cols)\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "#-------------- Data Extraction ---------------------------\n",
    "train_images = load_mnist_images('MNIST/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "test_images = load_mnist_images('MNIST/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('MNIST/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#--------------- Train data manipulation ------------------\n",
    "print(train_images.shape)  # (60000, 28, 28)\n",
    "print(train_labels.shape)  # (60000,)\n",
    "\n",
    "one_hot_labels = np.zeros(train_labels.shape[0]*10).reshape((train_labels.shape[0]),10)\n",
    "for i in range(len(train_labels)):\n",
    "    one_hot_labels[i][train_labels[i]]=1\n",
    "train_labels = one_hot_labels\n",
    "\n",
    "print(train_labels.shape) # (60000,10)\n",
    "\n",
    "#--------------- Test data manipulation -------------------\n",
    "print(test_images.shape)  # (10000, 28, 28)\n",
    "print(test_labels.shape)  # (10000,)\n",
    "\n",
    "one_hot_labels = np.zeros(test_labels.shape[0]*10).reshape((test_labels.shape[0]),10)\n",
    "for i in range(len(test_labels)):\n",
    "    one_hot_labels[i][test_labels[i]]=1\n",
    "test_labels = one_hot_labels\n",
    "\n",
    "print(test_labels.shape) # (10000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097cf66",
   "metadata": {},
   "source": [
    "## PyTorch CNN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8be5b",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is defined using PyTorch's `nn.Module` to serve as a reference and source of pre-trained weights.\n",
    "\n",
    "**Architecture (defined as `SimpleCNN` class):**\n",
    "\n",
    "1.  **Conv1 + ReLU1:** `nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 1, 28, 28)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(I - K + 2P)}{S} \\rfloor + 1 = \\lfloor \\frac{(28 - 2 + 0)}{2} \\rfloor + 1 = 14$\n",
    "    *   Output: `(B, 32, 14, 14)`\n",
    "\n",
    "2.  **Conv2 + ReLU2:** `nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)`\n",
    "    *   Input: `(B, 32, 14, 14)`\n",
    "    *   Padded input dim: $14 + 2*1 = 16$\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(16 - 2 + 0)}{2} \\rfloor + 1 = 8$\n",
    "    *   Output: `(B, 64, 8, 8)`\n",
    "\n",
    "3.  **Conv3 + ReLU3:** `nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)`\n",
    "    *   Input: `(B, 64, 8, 8)`\n",
    "    *   Output dimension: $O = \\lfloor \\frac{(8 - 2 + 0)}{2} \\rfloor + 1 = 4$\n",
    "    *   Output: `(B, 128, 4, 4)`\n",
    "\n",
    "4.  **Flatten:** `nn.Flatten()`\n",
    "    *   Input: `(B, 128, 4, 4)`\n",
    "    *   Output: `(B, 128 * 4 * 4)` which is `(B, 2048)`\n",
    "\n",
    "5.  **FC1 + ReLU4:** `nn.Linear(in_features=2048, out_features=250)`\n",
    "    *   Input: `(B, 2048)`\n",
    "    *   Operation: $Y = XW^T + b$\n",
    "    *   Output: `(B, 250)`\n",
    "\n",
    "6.  **FC2:** `nn.Linear(in_features=250, out_features=10)` (Output layer)\n",
    "    *   Input: `(B, 250)`\n",
    "    *   Output: `(B, 10)` (logits for 10 classes)\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"cnn.png\", style=\"border-radius:20px;\", height=300>\n",
    "    <figcaption>CNN Architecture (B: Batch size)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dabdea",
   "metadata": {},
   "source": [
    "### Model and Dataset Declaration with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff84faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1.------------------ CNN declaration -------------------\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # --------- Convolutional Layers ------------\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # ---------- Flatten to become MLP's input -----------\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = 128 * 4 * 4\n",
    "        # ---------- Multi Layer Perceptron ---------------\n",
    "        # Only one hidden layer for classification\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=250)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution: from 1x1x28x28 to 1x32x14x14\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        # Second Convolution: from 1x32x14x14 to 1x64x8x8\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        # Third Convolution: from 1x64x8x8 to 1x128x4x4\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# # 2.------------------ CNN's Dataset declaration ----------------------\n",
    "\n",
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(self, digits, labels, transform=None):\n",
    "#         assert len(digits) == len(labels), \"Number of digits and labels doesn't match\"\n",
    "#         self.digits = digits\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.digits)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         digit = self.digits[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         digit = digit.unsqueeze(0) # Needed operation to add the dimension of greyscale images (28,28) -> (1,28,28)\n",
    "#         return digit, label\n",
    "\n",
    "# tri = torch.from_numpy(train_images).float() / 255\n",
    "# trl = torch.from_numpy(train_labels).float()\n",
    "# tsi = torch.from_numpy(test_images).float() / 255\n",
    "# tsl = torch.from_numpy(test_labels).float()\n",
    "\n",
    "# train_dataset = CNNDataset(tri,trl)\n",
    "# test_dataset = CNNDataset(tsi,tsl)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 3.------ Training Setup ---------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(f\"device: {device}\")\n",
    "\n",
    "# model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# # Loss definition\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# # Optimisation definition\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5 \n",
    "\n",
    "# # 4.------- cycle training ------\n",
    "\n",
    "# print(\"\\nStarting Training...\")\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train() \n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "#     #tqdm is module used to have a progress bar\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for inputs, labels in progress_bar:\n",
    "\n",
    "#         # move data on the device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # make all gradients zero to avoid learning on gradients of previous steps\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         # loss computation\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass: compute the gradients\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Weights update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the loss\n",
    "#         running_loss += loss.item() * inputs.size(0) # multiply for batch size to obtain the correct mean\n",
    "\n",
    "#         # Update the progress bar\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     # Epochs' mean loss computation\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     epoch_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Tempo: {epoch_time:.2f}s - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- Test evaluation (after every epoch) ---\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient computation (we don't need gradients since we don't want to update the model in this phase)\n",
    "#         i=0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             if i >= 1:\n",
    "#                 continue\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1) # Obtain index with the maximum probability (it is our result)\n",
    "#             _,labels = torch.max(labels,1) # same for the test labels\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted==labels).sum().item()\n",
    "#             i+=1\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Test Loss: {avg_test_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# print(\"\\nTraining Complete.\")\n",
    "# #2m 9.4 secondi per avere un'epoca con cuda\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'simple_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8a955",
   "metadata": {},
   "source": [
    "### Weights extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee8995",
   "metadata": {},
   "source": [
    "### Extracting Pre-trained Weights from PyTorch Model\n",
    "\n",
    "This section loads weights from a pre-trained PyTorch model (`simple_cnn_mnist.pth`) and converts them into NumPy arrays. These NumPy weights will be used for our custom CNN implementations to ensure consistency for inference comparisons.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1.  **Load State Dictionary:** `model.load_state_dict(torch.load(...))` populates the instantiated `SimpleCNN` model with saved parameters. `map_location=torch.device('cpu')` ensures CPU loading.\n",
    "2.  **Evaluation Mode:** `model.eval()` sets the model to evaluation mode, which is good practice (disables layers like dropout if present).\n",
    "3.  **NumPy Conversion:** For each layer, weights (`.weight`) and biases (`.bias`) are extracted:\n",
    "    *   `.data.detach().numpy()`: Converts PyTorch tensors to NumPy arrays, detaching them from the computation graph.\n",
    "\n",
    "**Shape Conventions and Transpositions:**\n",
    "\n",
    "*   **Convolutional Kernels (`k1`, `k2`, `k3`):**\n",
    "    *   PyTorch: `(out_channels, in_channels, kernel_height, kernel_width)`.\n",
    "    *   Stored directly in `numpy_weights` with this shape, as our NumPy convolution functions expect this.\n",
    "*   **Convolutional Biases (`b_conv1`, etc.):**\n",
    "    *   PyTorch: `(out_channels,)`. Stored directly.\n",
    "*   **Fully Connected Weights (`w1`, `w2`):**\n",
    "    *   PyTorch: `(out_features, in_features)`.\n",
    "    *   For NumPy $XW+b$ where $X$ is `(batch, in_features)`, $W$ must be `(in_features, out_features)`. Thus, PyTorch weights are transposed (`.T`).\n",
    "*   **Fully Connected Biases (`b1`, `b2`):**\n",
    "    *   PyTorch: `(out_features,)`. Reshaped to `(1, out_features)` for NumPy broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e62c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛏️ Weights and Bias Extraction ⛏️\n",
      "\n",
      "k1: PyTorch Shape=(32, 1, 2, 2), NumPy Shape=(32, 1, 2, 2)\n",
      "b_conv1: NumPy Shape=(32,)\n",
      "k2: PyTorch Shape=(64, 32, 2, 2), NumPy Shape=(64, 32, 2, 2)\n",
      "b_conv2: NumPy Shape=(64,)\n",
      "k3: PyTorch Shape=(128, 64, 2, 2), NumPy Shape=(128, 64, 2, 2)\n",
      "b_conv3: NumPy Shape=(128,)\n",
      "w1: PyTorch Shape=(250, 2048), NumPy Shape=(2048, 250)\n",
      "b1: PyTorch Shape=(250,), NumPy Shape=(1, 250)\n",
      "w2: PyTorch Shape=(10, 250), NumPy Shape=(250, 10)\n",
      "b2: PyTorch Shape=(10,), NumPy Shape=(1, 10)\n",
      "\n",
      "Extraction complete. Numpy weights are in the dictionary 'numpy_weights'.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('simple_cnn_mnist.pth', map_location=torch.device('cpu'),weights_only=True)) # Carica su CPU\n",
    "\n",
    "model.eval() # good practice is to set model in evaluation when you want to extract weights\n",
    "\n",
    "# --- Parameters Extraction ⛏️ and Numpy Conversion ---\n",
    "\n",
    "# Weights container\n",
    "numpy_weights = {}\n",
    "\n",
    "# Move model on cpu\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"⛏️ Weights and Bias Extraction ⛏️\\n\")\n",
    "\n",
    "# Layer Conv1\n",
    "# PyTorch weight shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "# NumPy expected: (in_channels, out_channels, kernel_width, kernel_height) -> (1, 32, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_1 = model.conv1.weight.data.detach().numpy()\n",
    "# Transpose: (out, in, kH, kW) -> (in, out, kW, kH)\n",
    "numpy_weights['k1'] = pytorch_weights_of_kernels_in_layer_1\n",
    "\n",
    "# PyTorch bias shape: (out_channels,)\n",
    "numpy_weights['b_conv1'] = model.conv1.bias.data.detach().numpy() # Shape (32,)\n",
    "print(f\"k1: PyTorch Shape={pytorch_weights_of_kernels_in_layer_1.shape}, NumPy Shape={numpy_weights['k1'].shape}\")\n",
    "print(f\"b_conv1: NumPy Shape={numpy_weights['b_conv1'].shape}\")\n",
    "\n",
    "# Layer Conv2\n",
    "# PyTorch weight shape: (64, 32, 3, 3)\n",
    "# NumPy expected: (32, 64, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_2 = model.conv2.weight.data.detach().numpy()\n",
    "numpy_weights['k2'] = pytorch_weights_of_kernels_in_layer_2\n",
    "numpy_weights['b_conv2'] = model.conv2.bias.data.detach().numpy() # Shape (64,)\n",
    "print(f\"k2: PyTorch Shape={pytorch_weights_of_kernels_in_layer_2.shape}, NumPy Shape={numpy_weights['k2'].shape}\")\n",
    "print(f\"b_conv2: NumPy Shape={numpy_weights['b_conv2'].shape}\")\n",
    "\n",
    "# Layer Conv3\n",
    "# PyTorch weight shape: (128, 64, 3, 3)\n",
    "# NumPy expected: (64, 128, 3, 3)\n",
    "pytorch_weights_of_kernels_in_layer_3 = model.conv3.weight.data.detach().numpy()\n",
    "numpy_weights['k3'] = pytorch_weights_of_kernels_in_layer_3\n",
    "numpy_weights['b_conv3'] = model.conv3.bias.data.detach().numpy() # Shape (128,)\n",
    "print(f\"k3: PyTorch Shape={pytorch_weights_of_kernels_in_layer_3.shape}, NumPy Shape={numpy_weights['k3'].shape}\")\n",
    "print(f\"b_conv3: NumPy Shape={numpy_weights['b_conv3'].shape}\")\n",
    "\n",
    "# Layer FC1\n",
    "# PyTorch weight shape: (out_features, in_features) -> (250, 2048)\n",
    "# NumPy expected (per input @ W): (in_features, out_features) -> (2048, 250)\n",
    "pytorch_fc1_layer_weights = model.fc1.weight.data.detach().numpy()\n",
    "numpy_weights['w1'] = pytorch_fc1_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (out_features,) -> (250,)\n",
    "# NumPy expected (per aggiunta diretta): (1, out_features) -> (1, 250)\n",
    "pytorch_fc1_layer_biases = model.fc1.bias.data.detach().numpy()\n",
    "numpy_weights['b1'] = pytorch_fc1_layer_biases.reshape(1, -1) # Rendi (1, 250)\n",
    "print(f\"w1: PyTorch Shape={pytorch_fc1_layer_weights.shape}, NumPy Shape={numpy_weights['w1'].shape}\")\n",
    "print(f\"b1: PyTorch Shape={pytorch_fc1_layer_biases.shape}, NumPy Shape={numpy_weights['b1'].shape}\")\n",
    "\n",
    "# Layer FC2\n",
    "# PyTorch weight shape: (num_classes, 250) -> (10, 250)\n",
    "# NumPy expected: (250, num_classes) -> (250, 10)\n",
    "pytorch_fc2_layer_weights = model.fc2.weight.data.detach().numpy()\n",
    "numpy_weights['w2'] = pytorch_fc2_layer_weights.T # Trasponi\n",
    "# PyTorch bias shape: (num_classes,) -> (10,)\n",
    "# NumPy expected: (1, num_classes) -> (1, 10)\n",
    "pytorch_fc2_layer_biases = model.fc2.bias.data.detach().numpy()\n",
    "numpy_weights['b2'] = pytorch_fc2_layer_biases.reshape(1, -1) # Rendi (1, 10)\n",
    "print(f\"w2: PyTorch Shape={pytorch_fc2_layer_weights.shape}, NumPy Shape={numpy_weights['w2'].shape}\")\n",
    "print(f\"b2: PyTorch Shape={pytorch_fc2_layer_biases.shape}, NumPy Shape={numpy_weights['b2'].shape}\")\n",
    "\n",
    "print(\"\\nExtraction complete. Numpy weights are in the dictionary 'numpy_weights'.\")\n",
    "\n",
    "# Access Example:\n",
    "np_k1 = numpy_weights['k1']\n",
    "np_b_conv1 = numpy_weights['b_conv1']\n",
    "np_k2 = numpy_weights['k2']\n",
    "np_b_conv2 = numpy_weights['b_conv2']\n",
    "np_k3 = numpy_weights['k3']\n",
    "np_b_conv3 = numpy_weights['b_conv3']\n",
    "np_w1 = numpy_weights['w1']\n",
    "np_b1 = numpy_weights['b1']\n",
    "np_w2 = numpy_weights['w2']\n",
    "np_b2 = numpy_weights['b2']\n",
    "\n",
    "\n",
    "\n",
    "# [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]]\n",
    "\n",
    "\n",
    "#  [[[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]]\n",
    "\n",
    "\n",
    "\n",
    "#    [[[[-0.06239345  0.16331542  0.28573602]\n",
    "#    [ 0.299534    0.48019555  0.25194943]\n",
    "#    [-0.24432278  0.3191273  -0.06802213]]\n",
    "\n",
    "#   [[ 0.10294101 -0.14240074  0.01178457]\n",
    "#    [ 0.3072691  -0.06823204  0.30347323]\n",
    "#    [-0.06327374  0.3396498   0.07433306]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06822b",
   "metadata": {},
   "source": [
    "## CNN - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c17c8",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ca069",
   "metadata": {},
   "source": [
    "### Zero-Padding in Convolutions\n",
    "\n",
    "Zero-padding adds a border of zeros around an input image or feature map before convolution. For example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\quad \\xrightarrow{\\textcolor{lightgreen}{\\textnormal{zero padding}}} \\quad\n",
    "\\begin{bmatrix}\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 1 & 2 & 3 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 4 & 5 & 6 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & 7 & 8 & 9 & \\textcolor{lightgreen}{0} \\\\\n",
    "\\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0} & \\textcolor{lightgreen}{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It's crucial for:\n",
    "\n",
    "1.  **Controlling Output Spatial Dimensions:** Padding can be used to maintain or control the reduction in height/width of feature maps. The output dimension (e.g., height $O_H$) is given by:\n",
    "    $$ O_H = \\left\\lfloor \\frac{I_H - K_H + 2P_H}{S_H} \\right\\rfloor + 1 $$\n",
    "    where $I_H$ is input height, $K_H$ kernel height, $P_H$ padding on one side of height, and $S_H$ stride.\n",
    "2.  **Improving Feature Extraction at Borders:** Allows the kernel to process edge pixels more effectively.\n",
    "\n",
    "**`np.pad()` Usage:**\n",
    "For a 4D tensor (`BATCH, CHANNELS, HEIGHT, WIDTH`), `np.pad(array, ((0,0),(0,0),(p,p),(p,p)))` adds `p` zeros to the top/bottom of HEIGHT and left/right of WIDTH, leaving BATCH and CHANNELS unpadded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70273b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [ 4  5  6]\n",
      "   [ 7  8  9]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [13 14 15]\n",
      "   [16 17 18]]]\n",
      "\n",
      "\n",
      " [[[19 20 21]\n",
      "   [22 23 24]\n",
      "   [25 26 27]]\n",
      "\n",
      "  [[28 29 30]\n",
      "   [31 32 33]\n",
      "   [34 35 36]]]]\n",
      "[[[[ 0  0  0  0  0]\n",
      "   [ 0  1  2  3  0]\n",
      "   [ 0  4  5  6  0]\n",
      "   [ 0  7  8  9  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 10 11 12  0]\n",
      "   [ 0 13 14 15  0]\n",
      "   [ 0 16 17 18  0]\n",
      "   [ 0  0  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0  0  0]\n",
      "   [ 0 19 20 21  0]\n",
      "   [ 0 22 23 24  0]\n",
      "   [ 0 25 26 27  0]\n",
      "   [ 0  0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0  0]\n",
      "   [ 0 28 29 30  0]\n",
      "   [ 0 31 32 33  0]\n",
      "   [ 0 34 35 36  0]\n",
      "   [ 0  0  0  0  0]]]]\n"
     ]
    }
   ],
   "source": [
    "img9 = np.arange(1,37).reshape(2,2,3,3)\n",
    "pad_img9 = np.pad(img9,((0,0),(0,0),(1,1),(1,1)))\n",
    "print(img9)\n",
    "print(pad_img9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00035f0",
   "metadata": {},
   "source": [
    "### Delating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6aa4a",
   "metadata": {},
   "source": [
    "### Dilation for Backpropagation (Gradient w.r.t. Input)\n",
    "\n",
    "The `delateOne` function \"dilates\" an input matrix by inserting a single row and column of zeros between existing rows and columns along its last two spatial dimensions. This means it inserts $S-1$ zeros when the forward convolution stride $S$ was 2.\n",
    "\n",
    "**Relevance in Backpropagation for $\\frac{\\partial L}{\\partial X}$:**\n",
    "\n",
    "This dilation operation is a critical step when computing the gradient of the loss with respect to the input of a convolutional layer ($\\frac{\\partial L}{\\partial X}$), especially if the forward pass utilized a stride $S > 1$. Here is why:\n",
    "* When a forward convolution uses a stride $S > 1$, it effectively downsamples the input, resulting in an output feature map $Z$ with smaller spatial dimensions.\n",
    "* To calculate $\\frac{\\partial L}{\\partial X}$, we need to use the gradient flowing back from the subsequent layer, $\\frac{\\partial L}{\\partial Z}$ (where $Z$ is the output of the strided convolution). **Since the original input $X$ has larger spatial dimensions than $Z$, the gradient $\\frac{\\partial L}{\\partial Z}$ must be \"upsampled\" or \"spread out\" before it can be convolved with the kernel weights to produce a gradient of the correct shape for $X$.**\n",
    "\n",
    "**Dilation Step:** This upsampling is achieved by inserting $S-1$ rows and columns of zeros between the elements of $\\frac{\\partial L}{\\partial Z}$. The `delateOne` function in this notebook performs this specific operation for a stride $S=2$ (inserting one row/column of zeros).\n",
    "\n",
    "After $\\frac{\\partial L}{\\partial Z}$ is dilated to form $\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}$, it is then typically padded (with $K-1$ zeros where $K$ is the kernel dimension, adjusted for any original padding) and subsequently convolved with the 180-degree rotated (or flipped) kernel ($W_{rot180}$). This entire sequence of operations (padding the dilated output gradient and convolving it with the flipped kernel) is what yields $\\frac{\\partial L}{\\partial X}$ and is often referred to as a \"full convolution\" in this context (see \"A guide to convolution arithmetic for deep learning\" by Dumoulin and Visin, or the provided articles by Mayank Kaushik).\n",
    "\n",
    "The image below illustrates the concept of dilating an output gradient tensor. This dilation is a preparatory step before the gradient is used in further convolution operations during backpropagation for layers that had striding in their forward pass.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*luRORFyTmj9mJ7rVhzlbZA.png\" height=250, style=\"border-radius:20px;\">\n",
    "</figure>\n",
    "\n",
    "Illustrative example of dilating an output gradient tensor (like $\\frac{\\partial L}{\\partial Z}$) by inserting $S-1$ (stride minus one) zeros. For `delateOne`, $S=2$, so one zero is inserted between elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd18f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delateOne(matrix):\n",
    "    indix = np.arange(1,matrix.shape[3])\n",
    "    matrix = np.insert(matrix,indix,0,3)\n",
    "    indix = np.arange(-(matrix.shape[-2]-1),0)\n",
    "    matrix = np.insert(matrix,indix,0,-2)\n",
    "    return matrix\n",
    "\n",
    "def dilate(tensor, stride):\n",
    "    if stride == 1:\n",
    "        return tensor\n",
    "\n",
    "    batch_size, num_channels, height, width = tensor.shape\n",
    "    \n",
    "    dilated_height = height + (height - 1) * (stride - 1)\n",
    "    dilated_width = width + (width - 1) * (stride - 1)\n",
    "    \n",
    "    dilated_tensor = np.zeros((batch_size, num_channels, dilated_height, dilated_width))\n",
    "    dilated_tensor[:, :, ::stride, ::stride] = tensor\n",
    "    return dilated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4351c2f",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26d187",
   "metadata": {},
   "source": [
    "### \"Slow\" Convolution Forward Pass (Explicit Loops)\n",
    "\n",
    "`Slow_ReLU_Conv` implements a 2D convolution followed by ReLU activation using explicit nested Python loops. This is educationally valuable for clarity but computationally inefficient.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1.  **Inputs:**\n",
    "    *   `img`: Batch of images `(N, C_in, H_in, W_in)`.\n",
    "    *   `ker`: Filters `(C_out, C_in, K_H, K_W)`.\n",
    "    *   `bias`: Per-filter biases `(C_out,)`.\n",
    "2.  **Padding & Output Size:** Input `img` is padded. Output dimensions $(O_H, O_W)$ are calculated using the standard formula (see Padding section).\n",
    "3.  **Convolution:** For each output element $(n, f, y_{out}, x_{out})$:\n",
    "    $$ \\text{Output}(n, f, y_{out}, x_{out}) = \\left( \\sum_{c=0}^{C_{in}-1} \\sum_{k_y=0}^{K_H-1} \\sum_{k_x=0}^{K_W-1} \\text{Img}_{pad}(n, c, y_{out}S + k_y, x_{out}S + k_x) \\cdot \\text{Ker}(f, c, k_y, k_x) \\right) + \\text{Bias}(f) $$\n",
    "4.  **ReLU Activation:** If `applyReLU=True`: $\\text{ActivatedOutput} = \\max(0, \\text{Output})$. A binary `mask` (1 where Output > 0, else 0) is also returned for backpropagation.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/refs/heads/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif\" height=\"250\", style=\"border-radius:20px;\"/>\n",
    "    <figcaption>Convolution of a 4x4 image (blue) with a 2x2 kernel (green)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51071b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[ 1.  2.  3.  4.]\n",
      "   [ 5.  6.  7.  8.]\n",
      "   [ 9. 10. 11. 12.]]]]\n",
      "-------ker-------\n",
      "[[[[1 2]\n",
      "   [3 4]]]\n",
      "\n",
      "\n",
      " [[[5 6]\n",
      "   [7 8]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[  5.  19.  13.]\n",
      "   [ 47.  95.  45.]]\n",
      "\n",
      "  [[ 10.  40.  30.]\n",
      "   [104. 232. 126.]]]]\n",
      "-------Conv PyTorch-------\n",
      "tensor([[[[  5.,  19.,  13.],\n",
      "          [ 47.,  95.,  45.]],\n",
      "\n",
      "         [[ 10.,  40.,  30.],\n",
      "          [104., 232., 126.]]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This is a PyTorch Convolution example to be used to check if the convolution implemented in both slow and fast approaches are correct\n",
    "\n",
    "class CustomConv(nn.Module):\n",
    "    def __init__(self, kernel: torch.Tensor, bias: torch.Tensor = None, \n",
    "                 stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        out_ch, in_ch, k_h, k_w = kernel.shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
    "                              out_channels=out_ch,\n",
    "                              kernel_size=(k_h, k_w),\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              bias=(bias is not None))\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.copy_(kernel)\n",
    "            if bias is not None:\n",
    "                self.conv.bias.copy_(bias)\n",
    "\n",
    "        self.conv.weight.requires_grad_(False)\n",
    "        if bias is not None:\n",
    "            self.conv.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "def Slow_ReLU_Conv(img,ker,bias=np.array(0),pad=0,stride=1,applyReLU=True):\n",
    "    if applyReLU: # Forward case\n",
    "        out_ch, in_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = out_ch\n",
    "    else: # Backward case\n",
    "        in_ch, out_ch, k_width, k_height = ker.shape\n",
    "        nk_channel = in_ch\n",
    "\n",
    "    # bias has shape out_ch, 1, 1. It's a scalar value for each channel broadcasted to the kernel's width and height\n",
    "    # number of channels taken in input by the kernel 'in_ch' \n",
    "    # must be the same as the number of channels of the image 'channels'\n",
    "\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    n_images, channels, i_height, i_width  = img.shape\n",
    "    ni_height = int(((i_height - k_height) / stride) + 1) # new image height # Padding is already added\n",
    "    ni_width = int(((i_width - k_width) / stride) + 1) # new image width\n",
    "    ni = np.zeros((n_images, out_ch, ni_height, ni_width)).astype(np.float32) # new image\n",
    "\n",
    "    if in_ch != channels:\n",
    "        raise ValueError(f\"number of channels taken in input by the kernel ({in_ch}) must be the same as the number of channels of the image ({channels})\")\n",
    "\n",
    "    for one_img in range(n_images):\n",
    "        for one_k_channel in range(nk_channel):\n",
    "            for i_nih in range(ni_height): # which cycles row by row of the new image\n",
    "                for i_niw in range(ni_width): # which cycles column by column of the new image\n",
    "                    current_sum = 0.0 # convolution sum for the specific output cell\n",
    "                    # Convolution cycles\n",
    "                    for channel in range(channels): # channels == in_ch\n",
    "                        for i_kh in range(k_height):\n",
    "                            input_y = (i_nih * stride) + i_kh # get the y location, the height\n",
    "                            for i_kw in range(k_width):\n",
    "                                input_x = (i_niw * stride) + i_kw # get the x location, the width\n",
    "                                # check that everything stays in the measures\n",
    "                                if 0 <= input_y < i_height and 0 <= input_x < i_width:\n",
    "                                    input_val = img[one_img, channel, input_y, input_x]\n",
    "                                    kernel_val = ker[one_k_channel, channel, i_kh, i_kw]\n",
    "                                    current_sum += (input_val * kernel_val).astype(np.float32)\n",
    "                    ni[one_img, one_k_channel, i_nih, i_niw] = current_sum\n",
    "    if bias.all() != 0:\n",
    "        bias = bias.reshape(bias.shape[0],1,1)\n",
    "        if bias.shape[0] != out_ch:\n",
    "            raise ValueError(f\"bias dimension ({bias.shape[0]}) doesn't match kernel's number of channels ({out_ch})\")\n",
    "        ni = ni + bias\n",
    "    ni = ni.astype(np.float32)\n",
    "    if applyReLU:\n",
    "        ni = np.maximum(0, ni)\n",
    "        mask = ni.copy()\n",
    "        mask[mask > 0] = 1\n",
    "        return ni,mask\n",
    "    else:\n",
    "        return ni\n",
    "#-------------------------------------------- Examples --------------------------------------------------------\n",
    "img = np.arange(1,3*4+1).reshape(1,1,3,4).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,8+1).reshape(2,1,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=1,stride=2)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "# print(\"------mask-------\")\n",
    "# print(mask)\n",
    "\n",
    "\n",
    "my_kernel = torch.from_numpy(ker).float()\n",
    "\n",
    "my_bias = torch.from_numpy(np.array([1,2])).float()\n",
    "\n",
    "modelC = CustomConv(kernel=my_kernel,bias=my_bias, stride=2, padding=1)\n",
    "\n",
    "# input di prova (batch=1, canali=1, H=5, W=5)\n",
    "x = torch.from_numpy(img)\n",
    "y = modelC(x)\n",
    "print(\"-------Conv PyTorch-------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bad0a3",
   "metadata": {},
   "source": [
    "### Slow Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff4adf",
   "metadata": {},
   "source": [
    "**Actors:**\n",
    "1. W is the kernel\n",
    "2. $\\delta$ is the gradient\n",
    "3. x is the input to the convolution layer during forward\n",
    "4. b is the bias\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Derive delta**\n",
    "\n",
    "Deriving delta with respect to ReLU activation consists in the hadamard product (element-wise product) of the gradient ($\\delta$) and the mask obtained at the forward step, that is, all the elements in the convolved image greater than zero are put to one, the rest is zero.\n",
    "$$\n",
    "\\delta^{(i)} = \\delta_{\\text{flat reshaped}} \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "- **Gradient with respect to W**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(i)}} = \\text{Convolution}(x^{(i)}, \\delta)\n",
    "$$\n",
    "This convolution creates a matrix for every channel of input image $x^{i}$ and for every channel of output image $\\delta$, thus resulting in the correct number of channels\n",
    "\n",
    "- **Gradient w.r.t. the input \\( x \\)** (To go to the preceding layer):\n",
    "\n",
    "$$\n",
    "\\delta^{(i-1)} = \\text{Full\\_Convolution}(\\delta^{(i)}, W^{(i)})\n",
    "$$\n",
    "\n",
    "- **Gradient w.r.t the bias**\n",
    "\n",
    "Since the bias is added equally across the spatial dimensions of each output channel, the gradient is the sum of all elements in each output channel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{h,w} \\delta^{(i)}_{c,h,w}\n",
    "$$\n",
    "\n",
    "For batched inputs, sum also across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(i)}_c} = \\sum_{n,h,w} \\delta^{(i)}_{n,c,h,w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672622d3",
   "metadata": {},
   "source": [
    "### \"Slow\" Convolution Backward Pass (Explicit Loops)\n",
    "\n",
    "`Slow_ReLU_Gradient` computes gradients $\\frac{\\partial L}{\\partial X}$ (input gradient, `gi`), $\\frac{\\partial L}{\\partial W}$ (kernel gradient, `gk`), and $\\frac{\\partial L}{\\partial b}$ (bias gradient, `gb`), given $\\frac{\\partial L}{\\partial A}$ (output activation gradient, `d_img`).\n",
    "\n",
    "**Steps based on the formulas in the preceding markdown cell:**\n",
    "\n",
    "1.  **Backward ReLU:** $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\text{mask}$. (`d_img = np.multiply(d_img,mask)`)\n",
    "2.  **Gradient w.r.t. Bias (`gb`):** $\\frac{\\partial L}{\\partial b_f} = \\sum_{n,h,w} (\\frac{\\partial L}{\\partial Z})_{n,f,h,w}$. (`gb = d_img.sum((-1,-2))`, note: sum over batch is missing if batch_s > 1 and gradients are not accumulated externally).\n",
    "3.  **Gradient w.r.t. Kernel (`gk`):** $\\frac{\\partial L}{\\partial W} = \\text{Conv}(X_{padded}, \\frac{\\partial L}{\\partial Z})$.\n",
    "    *   The code iterates through each element of `gk` and computes its value by summing products of corresponding elements from `img` (original input, padded with forward pass `pad`) and `d_img` (which is $\\frac{\\partial L}{\\partial Z}$, after dilation if stride > 1 for this specific calculation, but the code uses the already dilated `d_img` from the `gi` section for the `dimg_height`, `dimg_width` loops).\n",
    "4.  **Gradient w.r.t. Input (`gi`):** $\\frac{\\partial L}{\\partial X} = \\text{FullConv}(\\left(\\frac{\\partial L}{\\partial Z}\\right)_{dilated}, W_{rot180})$.\n",
    "    *   $\\frac{\\partial L}{\\partial Z}$ is dilated if `stride==2` (`d_img = delateOne(d_img)`).\n",
    "    *   It's then padded: `d_imgPadded = np.pad(d_img, ..., (k_height-1-pad, ...))`.\n",
    "    *   Kernel `ker` is rotated 180 degrees (`ker180`).\n",
    "    *   The code then performs the convolution using nested loops to compute `gi`.\n",
    "\n",
    "This \"NEW APPROACH\" in comments refers to the direct loop-based implementation of these gradient convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1928d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imAge: (1, 1, 7, 7)\n",
      "kerNel: (2, 1, 2, 2)\n",
      "dimAge: (1, 2, 4, 4)\n",
      "ggi: (1, 1, 7, 7)\n",
      "ggk: (2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "def Slow_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "    ############################################# Gradient of Input Image ####################################\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image delated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel 180 degrees rotation (flipped vertically and then horizontally)\n",
    "    # FullConvolution(d_imgDelated, Rotated180Deg(kernel)) with stride 1\n",
    "    out_ch, in_ch, k_height, k_width = ker.shape\n",
    "    batch_s, in_ch, img_height, img_width = img.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "\n",
    "    # Delating the gradient of output\n",
    "    if stride == 2:\n",
    "        d_img = delateOne(d_img)\n",
    "    elif stride > 2:\n",
    "        raise ValueError(f\"Stride greater than 2 is not acceptable\")\n",
    "    d_imgPadded = np.pad(d_img,((0,0),(0,0),(k_height-1-pad,k_height-1-pad),(k_width-1-pad,k_width-1-pad)))\n",
    "    batch_s, out_ch, dimg_height, dimg_width = d_img.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    ker180 = np.rot90(ker,2,(-2,-1))\n",
    "\n",
    "    # Computation\n",
    "    gi = np.zeros_like(img)\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(img_height):\n",
    "            for i_giw in range(img_width):\n",
    "                for i_outch in range(out_ch):\n",
    "                    for i_inch in range(in_ch):\n",
    "                        for i_kh in range(k_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(k_width):\n",
    "                                x = i_gih + i_kw\n",
    "\n",
    "                                if 0 <= y < d_imgPadded.shape[-2] and 0 <= x < d_imgPadded.shape[-1]:\n",
    "                                    input_val = d_imgPadded[bs,i_outch,y,x]\n",
    "                                    ker_val = ker180[i_outch,i_inch,i_kh,i_kw] \n",
    "                                else:\n",
    "                                    break\n",
    "                                current_sum += input_val*ker_val\n",
    "                    gi[bs,i_inch,i_gih,i_giw] = current_sum\n",
    "                    current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the delated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    gk = np.zeros_like(ker)\n",
    "    img = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    current_sum = 0.0\n",
    "    for bs in range(batch_s):\n",
    "        for i_gih in range(k_height):\n",
    "            for i_giw in range(k_width):\n",
    "                for i_inch in range(in_ch):\n",
    "                    for i_outch in range(out_ch):\n",
    "                        for i_kh in range(dimg_height):\n",
    "                            y = i_gih + i_kh\n",
    "                            for i_kw in range(dimg_width):\n",
    "                                x = i_gih + i_kw\n",
    "                                if 0 <= y < img_height and 0 <= x < img_width:\n",
    "                                    input_val = img[bs,i_inch,y,x]\n",
    "                                    ker_val = d_img[bs,i_outch,i_kh,i_kw] \n",
    "                                    current_sum += input_val*ker_val\n",
    "                                else:\n",
    "                                    break\n",
    "                        gk[i_outch,i_inch,i_gih,i_giw] = current_sum\n",
    "                        current_sum = 0.0\n",
    "\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gb = d_img.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "in_ch = 1\n",
    "out_ch = 2\n",
    "idim = 7\n",
    "kdim = 2\n",
    "s = 2\n",
    "p = 1\n",
    "imAge = np.arange(1,1*in_ch*idim*idim+1).reshape(1,in_ch,idim,idim)\n",
    "kerNel = np.arange(1,out_ch*in_ch*(kdim**2)+1).reshape(out_ch,in_ch,kdim,kdim)\n",
    "dimAge,mask = Slow_ReLU_Conv(imAge,kerNel,stride=s,pad=p) \n",
    "dimAge = dimAge/np.mean(dimAge)\n",
    "ggi,ggk,ggb = Slow_ReLU_Gradient(imAge,dimAge,kerNel,mask,stride=s,pad=p)\n",
    "print(f\"imAge: {imAge.shape}\")\n",
    "print(f\"kerNel: {kerNel.shape}\")\n",
    "print(f\"dimAge: {dimAge.shape}\")\n",
    "print(f\"ggi: {ggi.shape}\")\n",
    "print(f\"ggk: {ggk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae10455",
   "metadata": {},
   "source": [
    "### \"Transposed\" Convolution Layer: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02937c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Conv Slow-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Slow-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[32231.]]\n",
      "\n",
      "  [[79176.]]]]\n"
     ]
    }
   ],
   "source": [
    "def Fast_ReLU_Conv(batch_of_images,kernels,biases=np.array(0),padding=0,stride=1,applyReLU=True):\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "\n",
    "    # im2col: Window creation\n",
    "    batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    sliding_windows = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,input_channels,kernel_height,kernel_width))[:,:,::stride,::stride]\n",
    "    sliding_windows = sliding_windows.reshape((-1,(kernel_height * kernel_width * input_channels)))\n",
    "\n",
    "    # Convolution\n",
    "    kernels = kernels.reshape((-1,(kernel_height*kernel_width*input_channels))).transpose(1,0)\n",
    "    image_dot_kernel = (sliding_windows @ kernels).astype(np.float32) # convolved image matrix\n",
    "\n",
    "    # ReLU activation\n",
    "    output_width = int(((image_width-kernel_width) / stride) + 1) # Padding was already added\n",
    "    output_height = int(((image_height-kernel_height) / stride) + 1)\n",
    "\n",
    "    # First operate a reshape keeping spatial ordering, which has channels at the end\n",
    "    output = image_dot_kernel.reshape(batch_size, output_width, output_height, kernels_number)\n",
    "\n",
    "    # Transpose to have input in shapes (batch, output_channel, height, width)\n",
    "    output = output.transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "    if biases.any() != 0:\n",
    "        output = (output + biases.reshape(1,-1,1,1))\n",
    "\n",
    "    if applyReLU:\n",
    "        output = np.maximum(0,output)\n",
    "\n",
    "    mask = np.copy(output)\n",
    "    mask[mask > 0] = 1\n",
    "\n",
    "    return output, mask\n",
    "\n",
    "\n",
    "\n",
    "img = np.arange(1,2*3*3+1).reshape(1,2,3,3).astype(np.float32)\n",
    "# print(\"-------img-------\")\n",
    "# print(img)\n",
    "ker = np.arange(1,16+1).reshape(2,2,2,2)\n",
    "# print(\"-------ker-------\")\n",
    "# print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "res,mask = Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(img,ker,bias,padding = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)\n",
    "res,mask = Slow_ReLU_Conv(res,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "X_c,mask = Fast_ReLU_Conv(X_c,ker,bias,padding = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c44f",
   "metadata": {},
   "source": [
    "### Transposed Convolution Forward Pass (Im2Col alternative approach)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56c9deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------img-------\n",
      "[[[[ 1.  2.  3.]\n",
      "   [ 4.  5.  6.]\n",
      "   [ 7.  8.  9.]]\n",
      "\n",
      "  [[10. 11. 12.]\n",
      "   [13. 14. 15.]\n",
      "   [16. 17. 18.]]]]\n",
      "-------ker-------\n",
      "[[[[ 1  2]\n",
      "   [ 3  4]]\n",
      "\n",
      "  [[ 5  6]\n",
      "   [ 7  8]]]\n",
      "\n",
      "\n",
      " [[[ 9 10]\n",
      "   [11 12]]\n",
      "\n",
      "  [[13 14]\n",
      "   [15 16]]]]\n",
      "-------bias-------\n",
      "[[[1]]\n",
      "\n",
      " [[2]]]\n",
      "-------Conv Slow-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Mat-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n",
      "-------Conv Fast-------\n",
      "[[[[ 357.  393.]\n",
      "   [ 465.  501.]]\n",
      "\n",
      "  [[ 838.  938.]\n",
      "   [1138. 1238.]]]]\n"
     ]
    }
   ],
   "source": [
    "def im2col_convolutional_matrix(batch_of_images, kernels, biases=None, padding=0, stride=1, applyReLU=True):\n",
    "    # kernels.shape = (kernels_number, kernel_channels, kernel_height, kernel_width)\n",
    "    # batch_of_images.shape = (batch_size, input_channels, image_height, image_width)\n",
    "\n",
    "    kernels_number, kernel_channels, kernel_height, kernel_width = kernels.shape\n",
    "    batch_size, input_channels, image_height, image_width = batch_of_images.shape\n",
    "\n",
    "    if kernel_channels != input_channels:\n",
    "        raise ValueError(f\"Numero di canali del kernel ({kernel_channels}) non corrisponde ai canali dell'input ({input_channels})\")\n",
    "\n",
    "    # Calcola dimensioni output\n",
    "    output_height = (image_height - kernel_height + 2 * padding) // stride + 1\n",
    "    output_width = (image_width - kernel_width + 2 * padding) // stride + 1\n",
    "\n",
    "    # Padding\n",
    "    if padding > 0:\n",
    "        batch_of_images_padded = np.pad(batch_of_images, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    else:\n",
    "        batch_of_images_padded = batch_of_images\n",
    "\n",
    "    # Estrarre le patch usando sliding_window_view (im2col)\n",
    "    # La finestra ha le dimensioni del kernel e si muove sull'immagine paddata\n",
    "    # La shape della finestra è (1 (per batch), input_channels, kernel_height, kernel_width)\n",
    "    # L'ultimo '1' è per la dimensione del batch delle patch, che è sempre 1 qui.\n",
    "    # Il '1' prima di input_channels è per il canale di output virtuale delle patch, che non ci serve qui.\n",
    "    patches = np.lib.stride_tricks.sliding_window_view(\n",
    "        batch_of_images_padded,\n",
    "        (1, input_channels, kernel_height, kernel_width)\n",
    "    )\n",
    "    # patches.shape ora è (batch_size, 1, H_out_stride1, W_out_stride1, 1, input_channels, kernel_height, kernel_width)\n",
    "    # H_out_stride1 e W_out_stride1 sono le dimensioni dell'output se stride fosse 1\n",
    "\n",
    "    # Applichiamo lo stride alle patch selezionate\n",
    "    patches_strided = patches[:, :, ::stride, ::stride, :, :, :, :]\n",
    "    # patches_strided.shape: (batch_size, 1, output_height, output_width, 1, input_channels, kernel_height, kernel_width)\n",
    "\n",
    "    # Reshape delle patch per la moltiplicazione matriciale (X_col)\n",
    "    # Ogni riga di X_col è una patch appiattita\n",
    "    # X_col shape: (batch_size * output_height * output_width, input_channels * kernel_height * kernel_width)\n",
    "    X_col = patches_strided.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * output_height * output_width,\n",
    "        input_channels * kernel_height * kernel_width\n",
    "    )\n",
    "\n",
    "    # Reshape dei kernel per la moltiplicazione matriciale (W_col)\n",
    "    # Ogni colonna di W_col è un kernel appiattito\n",
    "    # W_col shape: (input_channels * kernel_height * kernel_width, kernels_number)\n",
    "    W_col = kernels.reshape(kernels_number, -1).T\n",
    "    # kernels.shape (OC, IC, KH, KW) -> (OC, IC*KH*KW) -> .T -> (IC*KH*KW, OC)\n",
    "\n",
    "    # Moltiplicazione matriciale per ottenere l'output srotolato\n",
    "    # output_col shape: (batch_size * output_height * output_width, kernels_number)\n",
    "    output_col = X_col @ W_col\n",
    "\n",
    "    # Reshape dell'output nella forma 4D desiderata\n",
    "    # (batch_size, output_height, output_width, kernels_number)\n",
    "    output = output_col.reshape(batch_size, output_height, output_width, kernels_number)\n",
    "    # Trasponi per avere (batch_size, kernels_number, output_height, output_width)\n",
    "    output = output.transpose(0, 3, 1, 2)\n",
    "\n",
    "    # Aggiunta dei bias\n",
    "    if biases is not None and biases.any() != 0:\n",
    "        output = output + biases.reshape(1, -1, 1, 1) # biases.shape = (kernels_number,)\n",
    "\n",
    "    mask = None # Inizializza la maschera\n",
    "\n",
    "    if applyReLU:\n",
    "        output_activated = np.maximum(0, output)\n",
    "        mask = (output_activated > 0).astype(output.dtype) # Maschera binaria (0 o 1)\n",
    "        output = output_activated\n",
    "    else: # Se non applichiamo ReLU, la maschera può essere considerata tutta 1 (per il backward)\n",
    "        mask = np.ones_like(output)\n",
    "\n",
    "\n",
    "    return output.astype(np.float32), mask.astype(np.float32)\n",
    "\n",
    "img = np.arange(1,2*3*3+1).reshape(1,2,3,3).astype(np.float32)\n",
    "print(\"-------img-------\")\n",
    "print(img)\n",
    "ker = np.arange(1,16+1).reshape(2,2,2,2)\n",
    "print(\"-------ker-------\")\n",
    "print(ker)\n",
    "bias = np.array([1,2]).reshape(2,1,1)\n",
    "print(\"-------bias-------\")\n",
    "print(bias)\n",
    "res, mask = Slow_ReLU_Conv(img,ker,bias,pad=0,stride=1)\n",
    "print(\"-------Conv Slow-------\")\n",
    "print(res)\n",
    "print(\"-------Conv Mat-------\")\n",
    "res, mask = im2col_convolutional_matrix(img, ker, bias, padding=0, stride=1)\n",
    "print(res)\n",
    "# res,mask = Slow_ReLU_Conv(res,ker,bias,pad=0,stride=1)\n",
    "# print(\"-------Conv Slow-------\")\n",
    "# print(res)\n",
    "X_c, mask = Fast_ReLU_Conv(img,ker,bias,padding = 0,stride=1)\n",
    "print(\"-------Conv Fast-------\")\n",
    "print(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6912ed",
   "metadata": {},
   "source": [
    "Dimensional reshape test to see if everything goes into the right place (it does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c3fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1245. 1323.]\n",
      "   [1479. 1557.]]\n",
      "\n",
      "  [[2973. 3195.]\n",
      "   [3639. 3861.]]\n",
      "\n",
      "  [[4701. 5067.]\n",
      "   [5799. 6165.]]\n",
      "\n",
      "  [[6429. 6939.]\n",
      "   [7959. 8469.]]]]\n",
      "[[ 1  2  4  5]\n",
      " [ 2  3  5  6]\n",
      " [ 4  5  7  8]\n",
      " [ 5  6  8  9]\n",
      " [10 11 13 14]\n",
      " [11 12 14 15]\n",
      " [13 14 16 17]\n",
      " [14 15 17 18]\n",
      " [19 20 22 23]\n",
      " [20 21 23 24]\n",
      " [22 23 25 26]\n",
      " [23 24 26 27]]\n",
      "[[1245. 1323. 1479. 1557.]\n",
      " [2973. 3195. 3639. 3861.]\n",
      " [4701. 5067. 5799. 6165.]\n",
      " [6429. 6939. 7959. 8469.]]\n",
      "[[1245. 2973. 4701. 6429.]\n",
      " [1323. 3195. 5067. 6939.]\n",
      " [1479. 3639. 5799. 7959.]\n",
      " [1557. 3861. 6165. 8469.]]\n",
      "[[ 17592.  43224.  68856.  94488.]\n",
      " [ 23196.  56892.  90588. 124284.]\n",
      " [ 34404.  84228. 134052. 183876.]\n",
      " [ 40008.  97896. 155784. 213672.]\n",
      " [ 68028. 166236. 264444. 362652.]\n",
      " [ 73632. 179904. 286176. 392448.]\n",
      " [ 84840. 207240. 329640. 452040.]\n",
      " [ 90444. 220908. 351372. 481836.]\n",
      " [118464. 289248. 460032. 630816.]\n",
      " [124068. 302916. 481764. 660612.]\n",
      " [135276. 330252. 525228. 720204.]\n",
      " [140880. 343920. 546960. 750000.]]\n",
      "[[[[ 17592.  23196.]\n",
      "   [ 34404.  40008.]]\n",
      "\n",
      "  [[ 68028.  73632.]\n",
      "   [ 84840.  90444.]]\n",
      "\n",
      "  [[118464. 124068.]\n",
      "   [135276. 140880.]]]\n",
      "\n",
      "\n",
      " [[[ 43224.  56892.]\n",
      "   [ 84228.  97896.]]\n",
      "\n",
      "  [[166236. 179904.]\n",
      "   [207240. 220908.]]\n",
      "\n",
      "  [[289248. 302916.]\n",
      "   [330252. 343920.]]]\n",
      "\n",
      "\n",
      " [[[ 68856.  90588.]\n",
      "   [134052. 155784.]]\n",
      "\n",
      "  [[264444. 286176.]\n",
      "   [329640. 351372.]]\n",
      "\n",
      "  [[460032. 481764.]\n",
      "   [525228. 546960.]]]\n",
      "\n",
      "\n",
      " [[[ 94488. 124284.]\n",
      "   [183876. 213672.]]\n",
      "\n",
      "  [[362652. 392448.]\n",
      "   [452040. 481836.]]\n",
      "\n",
      "  [[630816. 660612.]\n",
      "   [720204. 750000.]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "s = 1\n",
    "p = 0\n",
    "in_ch = 3\n",
    "out_ch = 4\n",
    "i_dim = 3\n",
    "k_dim = 2\n",
    "img = np.arange(1,i_dim*in_ch*i_dim+1).reshape(1,in_ch,i_dim,i_dim)\n",
    "ker = np.arange(1,out_ch*k_dim*in_ch*k_dim+1).reshape(out_ch,in_ch,k_dim,k_dim)\n",
    "d_img, mask = im2col_convolutional_matrix(img,ker,stride=s,padding=p)\n",
    "print(d_img)\n",
    "_,_,dimg_height,dimg_width = d_img.shape\n",
    "windom_pimg = np.lib.stride_tricks.sliding_window_view(img,(1,1,dimg_height,dimg_width)).reshape(-1,dimg_height*dimg_width)\n",
    "print(windom_pimg)\n",
    "print(d_img.reshape(-1,dimg_height*dimg_width))\n",
    "d_img = d_img.reshape(-1,dimg_height*dimg_width).transpose(1,0)\n",
    "print(d_img)\n",
    "iop = windom_pimg @ d_img\n",
    "print(iop)\n",
    "print(iop.transpose(1,0).reshape(out_ch,in_ch,k_dim,kdim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c414872",
   "metadata": {},
   "source": [
    "### Fast Convolution Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e1403",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c5583e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fast_ReLU_Gradient(img,d_img,ker,mask,pad=0,stride=1):\n",
    "    \"\"\"\n",
    "    NEW APPROACH !\n",
    "    Performs the backward pass of the convolution layer. It takes the original image, \n",
    "    the gradient image, and then the kernel, padding and stride used in the convolution. Also the mask is needed to perform the ReLU operation.\n",
    "    It returns the gradient w.r.t. the Original Image to back propagate and the gradient of the kernel\n",
    "    \"\"\" \n",
    "    ############################################# Gradient of Input Image ####################################\n",
    "    # The computation consists in a convolution where the image is the gradient of the output image delated (zeros between matrix elements) of stride-1\n",
    "    # and padded of kernel-1 dimensions \n",
    "    # and the kernel 180 degrees rotation (flipped vertically and then horizontally)\n",
    "    # FullConvolution(d_imgDelated, Rotated180Deg(kernel)) with stride 1\n",
    "    out_ch, in_ch, k_height, k_width = ker.shape\n",
    "    batch_s, in_ch, img_height, img_width = img.shape\n",
    "\n",
    "    # backward ReLU\n",
    "    d_img = np.multiply(d_img,mask)\n",
    "\n",
    "    # Delating the gradient of output\n",
    "    if stride == 2:\n",
    "        d_img = delateOne(d_img)\n",
    "    elif stride > 2:\n",
    "        raise ValueError(f\"Stride greater than 2 is not acceptable\")\n",
    "    d_imgPadded = np.pad(d_img,((0,0),(0,0),(k_height-1-pad,k_height-1-pad),(k_width-1-pad,k_width-1-pad)))\n",
    "\n",
    "    batch_s, out_ch, dimg_height, dimg_width = d_img.shape\n",
    "    \n",
    "    # flipping the kernel\n",
    "    ker180 = np.rot90(ker,2,(-2,-1))\n",
    "\n",
    "    window_dPad = np.lib.stride_tricks.sliding_window_view(d_imgPadded,(1,out_ch,k_width,k_height)).reshape(-1,(k_width*k_height*out_ch)) # window matrix\n",
    "    # Convolution\n",
    "    ker = ker.reshape((-1,(k_width*k_height*out_ch))).transpose(1,0)\n",
    "\n",
    "    gi = (window_dPad @ ker).astype(np.float32).transpose(1,0).reshape(img.shape)\n",
    "    \n",
    "    ############################################# Gradient of Kernel ####################################\n",
    "    # The computation consists in a convolution between the original image and the delated gradient of the output image in order to\n",
    "    # find the kernel\n",
    "    imgPad = np.pad(img,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    windom_pimg = np.lib.stride_tricks.sliding_window_view(imgPad,(1,1,dimg_height,dimg_width)).reshape(-1,dimg_height*dimg_width)\n",
    "    d_img = d_img.reshape(-1,dimg_height*dimg_width).transpose(1,0)\n",
    "    gk = (windom_pimg @ d_img).astype(np.float32).transpose(1,0).reshape(out_ch,in_ch,k_height,k_width)\n",
    "    ############################################# Gradient of Bias ####################################\n",
    "    # The computation consists in summing the gradient of the output image together to find the bias for every channel\n",
    "    gb = d_img.sum((-1,-2)) # sum over height and width\n",
    "    \n",
    "    ################################################### Return Results ###############################################\n",
    "    return gi,gk,gb\n",
    "\n",
    "def im2col_gradient_convolutional_matrix(\n",
    "    original_forward_input, # X\n",
    "    d_image, # dL/dA (gradiente rispetto all'output attivato del layer corrente)\n",
    "    kernels, # W\n",
    "    mask,    # Maschera della ReLU\n",
    "    padding, # Padding usato nella forward pass\n",
    "    stride   # Stride usato nella forward pass\n",
    "    ):\n",
    "\n",
    "    # 0. Backward ReLU: dL/dZ = dL/dA * mask\n",
    "    # (dL/dZ è il gradiente rispetto all'output del layer PRIMA della ReLU)\n",
    "    gradient_pre_activation = np.multiply(d_image, mask) # dL/dZ\n",
    "\n",
    "    # --- Parametri dimensionali ---\n",
    "    kernels_number, input_channels_kernel, kernel_height, kernel_width = kernels.shape # OC, IC, KH, KW\n",
    "    batch_size, input_channels_orig, input_height, input_width = original_forward_input.shape # BS, IC, IH, IW\n",
    "    # _, _, output_height_grad, output_width_grad = gradient_pre_activation.shape # BS, OC, OH, OW\n",
    "\n",
    "    # --- 1. Gradiente rispetto ai Bias (gradient_wrt_biases = dL/db) ---\n",
    "    # Somma `gradient_pre_activation` (dL/dZ) lungo le dimensioni batch, altezza, larghezza.\n",
    "    # gradient_pre_activation ha shape (batch_size, kernels_number, oh_grad, ow_grad)\n",
    "    # gradient_wrt_biases deve avere shape (kernels_number,)\n",
    "    gradient_wrt_biases = np.sum(gradient_pre_activation, axis=(0, 2, 3))\n",
    "\n",
    "    # --- 2. Gradiente rispetto ai Pesi (gradient_wrt_kernels = dL/dW) ---\n",
    "    # dL/dW = conv(X_padded, dL/dZ_dilated_se_necessario) o più precisamente X_col.T @ dL/dZ_col\n",
    "    # X_col sono le patch dell'input originale che hanno generato l'output.\n",
    "    # dL/dZ_col è il gradiente pre-attivazione appiattito.\n",
    "\n",
    "    # Padding dell'input originale (come nella forward)\n",
    "    if padding > 0:\n",
    "        X_padded_for_dW = np.pad(original_forward_input, ((0,0), (0,0), (padding,padding), (padding,padding)), mode='constant')\n",
    "    else:\n",
    "        X_padded_for_dW = original_forward_input\n",
    "\n",
    "    # Estrarre X_col dall'input originale paddato\n",
    "    # La finestra ha le dimensioni del kernel originale\n",
    "    patches_X_for_dW = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_padded_for_dW,\n",
    "        (1, input_channels_orig, kernel_height, kernel_width)\n",
    "    )[:, :, ::stride, ::stride, :, :, :, :] # Applica stride originale\n",
    "    # Shape: (bs, 1, oh, ow, 1, ic, kh, kw)\n",
    "\n",
    "    X_col_for_dW = patches_X_for_dW.transpose(0, 2, 3, 5, 6, 7, 1, 4).reshape(\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3], # bs * oh * ow\n",
    "        input_channels_orig * kernel_height * kernel_width # ic * kh * kw\n",
    "    )\n",
    "\n",
    "    # Reshape di dL/dZ (gradient_pre_activation)\n",
    "    # Shape: (bs, oc, oh, ow) -> (bs * oh * ow, oc)\n",
    "    dLdZ_col = gradient_pre_activation.transpose(0, 2, 3, 1).reshape(\n",
    "        batch_size * gradient_pre_activation.shape[2] * gradient_pre_activation.shape[3],\n",
    "        kernels_number # oc\n",
    "    )\n",
    "\n",
    "    # Calcolo di dL/dW\n",
    "    # (ic*kh*kw, bs*oh*ow) @ (bs*oh*ow, oc) -> (ic*kh*kw, oc)\n",
    "    gradient_wrt_kernels_flat = X_col_for_dW.T @ dLdZ_col\n",
    "\n",
    "    # Reshape nella forma dei kernel originali (oc, ic, kh, kw)\n",
    "    gradient_wrt_kernels = gradient_wrt_kernels_flat.reshape(\n",
    "        input_channels_orig, kernel_height, kernel_width, kernels_number\n",
    "    ).transpose(3, 0, 1, 2)\n",
    "\n",
    "\n",
    "    # --- 3. Gradiente rispetto all'Input (gradient_wrt_input = dL/dX) ---\n",
    "    # Questo è una convoluzione trasposta (o \"full convolution\")\n",
    "    # dL/dX = FullConv(dL/dZ_dilated, W_rot180)\n",
    "\n",
    "    # a. Dilatazione di gradient_pre_activation (dL/dZ) se lo stride della forward era > 1\n",
    "    dilated_grad_pre_activation = dilate(gradient_pre_activation, stride)\n",
    "\n",
    "    # b. Kernel per dL/dX: ruotati di 180° e canali scambiati\n",
    "    # kernels.shape = (OC, IC, KH, KW)\n",
    "    # Per la convoluzione trasposta, i canali di input/output del kernel si invertono\n",
    "    # flipped_kernels_for_dX.shape deve essere (IC_new, OC_new, KH, KW)\n",
    "    # dove IC_new = OC (canali di dL/dZ), OC_new = IC (canali di dL/dX)\n",
    "    # Quindi, shape finale (IC, OC, KH, KW)\n",
    "    flipped_kernels_for_dX = np.rot90(kernels, 2, axes=(2, 3)).transpose(1, 0, 2, 3)\n",
    "\n",
    "    # c. Padding per la convoluzione interna di dL/dX (convoluzione trasposta)\n",
    "    # Padding_effettivo_trasposta = KernelDim - 1 - Padding_Forward_Originale\n",
    "    # Questo padding si applica a dilated_grad_pre_activation\n",
    "    padding_for_dX_conv = kernel_height - 1 - padding # padding è il padding della forward originale\n",
    "\n",
    "    # d. Esegui la convoluzione (stride interno SEMPRE 1)\n",
    "    # Stiamo usando la nostra funzione di convoluzione efficiente\n",
    "    gradient_wrt_input_raw, _ = im2col_convolutional_matrix(\n",
    "        dilated_grad_pre_activation, # Immagine di input per questa convoluzione\n",
    "        flipped_kernels_for_dX,      # Kernels per questa convoluzione\n",
    "        biases=None,                 # Nessun bias nel calcolo del gradiente dell'input\n",
    "        padding=padding_for_dX_conv,\n",
    "        stride=1,                    # Lo stride è sempre 1 qui\n",
    "        applyReLU=False              # Nessuna attivazione qui\n",
    "    )\n",
    "\n",
    "    # e. Crop/slice finale per assicurare le dimensioni corrette di dL/dX\n",
    "    # L'output della convoluzione trasposta potrebbe essere leggermente più grande dell'input originale\n",
    "    # a seconda delle formule di padding e dimensioni.\n",
    "    # Vogliamo che abbia le dimensioni di original_forward_input: (batch_size, input_channels_orig, input_height, input_width)\n",
    "    gradient_wrt_input = gradient_wrt_input_raw[:, :, :input_height, :input_width]\n",
    "\n",
    "    return gradient_wrt_input, gradient_wrt_kernels, gradient_wrt_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daa1dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Fast_ReLU_Gradient(batch_of_images,d_image,kernel,mask,pad=0,stride=1):\n",
    "#     out_ch, in_ch, kh, kw = kernel.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "#     bs, nc, i_height,i_width = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "\n",
    "#     batchSize, out_ch, dh, dw = d_image.shape # number of kernels, number of input channels, kernel width and kernel height\n",
    "#     ni_height = int(((i_height-1)*stride)+kh) # new image height\n",
    "#     ni_width =  int(((i_width-1)*stride)+kw) # new image width\n",
    "#     height_to_pad = (ni_height-dh)\n",
    "#     width_to_pad = (ni_width-dw)\n",
    "\n",
    "#     half_htp = height_to_pad//2\n",
    "#     half_wtp = width_to_pad//2\n",
    "\n",
    "#     d_image = np.multiply(d_image,mask)\n",
    "#     d_imgP = np.pad(d_image,((0,0),(0,0),(half_htp,half_htp),(half_wtp,half_wtp)))\n",
    "\n",
    "#     batch_of_images = np.pad(batch_of_images,((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "#     bs, nc, iw, ih = batch_of_images.shape # batch of images' number of images, number of channels, single image's width, single images's height\n",
    "    \n",
    "#     ############################## Computing the gradient of the bias ##############################################\n",
    "#     gb = d_image.sum((0,-1,-2)) # sum over batch, height and width\n",
    "\n",
    "#     ########################################## Gradient of Kernel ###################################################\n",
    "#     window_boi = np.lib.stride_tricks.sliding_window_view(batch_of_images,(1,1,dh,dw))[:,:,::stride,::stride].reshape((-1,(dw*dh*1))) # window matrix\n",
    "#     d_image = d_image.reshape((-1,(dw*dh*1))).transpose(1,0)\n",
    "#     gk = (window_boi @ d_image).transpose(1,0).reshape(out_ch, in_ch, kh, kw,).astype(np.float32) # convolved image matrix\n",
    "\n",
    "#     ########################################## Gradient of Image ###################################################\n",
    "#     gi,_ = Fast_ReLU_Conv(d_imgP,kernel.transpose(1,0,2,3),stride = stride,pad=pad,applyReLU=False)\n",
    "#     # window_dboi = np.lib.stride_tricks.sliding_window_view(d_imgP,(1,out_ch,kh,kw))[:,:,::stride,::stride].reshape((-1,(kw*kh*out_ch))) # window matrix\n",
    "#     # kernel = kernel.reshape((-1,(kw*kh*out_ch))).transpose(1,0)\n",
    "#     # gi = (window_dboi @ kernel).reshape(bs, i_height, i_width, nc).transpose(0,3,1,2).astype(np.float32)\n",
    "\n",
    "#     ################################################### Return Results ###############################################\n",
    "#     return gi,gk,gb\n",
    "\n",
    "# s = 2\n",
    "# p = 1\n",
    "# in_ch = 3\n",
    "# out_ch = 32\n",
    "# i_dim = 28\n",
    "# k_dim = 3\n",
    "# img = np.arange(1,i_dim*in_ch*i_dim+1).reshape(1,in_ch,i_dim,i_dim)\n",
    "# ker = np.arange(1,out_ch*k_dim*in_ch*k_dim+1).reshape(out_ch,in_ch,k_dim,k_dim)\n",
    "# bias = np.ones(out_ch)\n",
    "# d_img,mask = Fast_ReLU_Conv(img,ker,bias,stride=s,pad=p)\n",
    "\n",
    "# print(\"-------------img--------------\")\n",
    "# #print(img)\n",
    "# print(img.shape)\n",
    "# print(\"-------------ker--------------\")\n",
    "# #print(ker)\n",
    "# print(ker.shape)\n",
    "# print(\"################################\")\n",
    "# print(\"-------------d_img--------------\")\n",
    "# #print(d_img-2)\n",
    "# print(d_img.shape)\n",
    "\n",
    "# print(\"************************************\")\n",
    "# # a,b,c = Fast_ReLU_Gradient(img,d_img-2,ker,mask,stride=s,pad=p)\n",
    "# # print(\"-------------gi-----------------\")\n",
    "# # print(a.shape)\n",
    "# # print(\"--------------gk-----------------\")\n",
    "# # print(b.shape)\n",
    "# # print(\"--------------gb-----------------\")\n",
    "# # print(c.shape)\n",
    "\n",
    "# ####################### Expected result ##########################\n",
    "# # [[[[ 1  2  3  4]\n",
    "# #    [ 5  6  7  8]\n",
    "# #    [ 9 10 11 12]\n",
    "# #    [13 14 15 16]]]]\n",
    "# # -------------d_img--------------\n",
    "# # [[[[ 43.  53.  63.]\n",
    "# #    [ 83.  93. 103.]\n",
    "# #    [123. 133. 143.]]\n",
    "\n",
    "# #   [[ 99. 125. 151.]\n",
    "# #    [203. 229. 255.]\n",
    "# #    [307. 333. 359.]]]]\n",
    "# # (1, 2, 3, 3)\n",
    "# # --------------------------------\n",
    "# # [[[[ 964. 2034. 2494. 1246.]\n",
    "# #    [2636. 5268. 6044. 2912.]\n",
    "# #    [4332. 8372. 9148. 4320.]\n",
    "# #    [2088. 3922. 4238. 1938.]]]]\n",
    "# # [[[[ 6042.  6879.]\n",
    "# #    [ 9390. 10227.]]]\n",
    "\n",
    "\n",
    "# #  [[[15018. 17079.]\n",
    "# #    [23262. 25323.]]]]\n",
    "# # [ 837. 2061.]\n",
    "# ##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249b2ef",
   "metadata": {},
   "source": [
    "### MLP Layer: Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3f21f",
   "metadata": {},
   "source": [
    "### MLP Forward Pass\n",
    "\n",
    "`ReLU_SoftMax_FullyConnected` executes the forward pass of a two-layer Multi-Layer Perceptron (one hidden layer, one output layer), typically used for classification after feature extraction by convolutional layers.\n",
    "\n",
    "**Operations:**\n",
    "\n",
    "1.  **Input:** `input_array` ($X_{mlp}$), the flattened output from conv layers.\n",
    "2.  **Hidden Layer (fc1):**\n",
    "    *   Linear: $Z_1 = X_{mlp} W_1 + b_1$ (output `fl`)\n",
    "    *   ReLU Activation: $A_1 = \\max(0, Z_1)$ (output `fa`)\n",
    "3.  **Output Layer (fc2):**\n",
    "    *   Linear: $Z_2 = A_1 W_2 + b_2$ (output `sl`, logits)\n",
    "    *   Softmax Activation: $P = \\text{Softmax}(Z_2)$ (output `sa`, probabilities)\n",
    "        $$ \\text{Softmax}(z)_j = \\frac{e^{z_j - \\max(z)}}{\\sum_{k} e^{z_k - \\max(z)}} $$\n",
    "        (The subtraction of $\\max(z)$ aids numerical stability).\n",
    "\n",
    "Returns `fl, fa, sl, sa` (pre-activations, activations, logits, probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68206d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x,axis=-1,keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x,axis=-1,keepdims=True)\n",
    "\n",
    "def ReLU_SoftMax_FullyConnected(input_array,w1,b1,w2,b2):\n",
    "    fl = (input_array @ w1)+b1 # first layer\n",
    "    fa = np.maximum(0,fl) # first activation: ReLU\n",
    "    sl = (fa @ w2)+b2 # second layer\n",
    "    sa = softmax(sl) # second activation: SoftMax\n",
    "    return fl,fa,sl,sa\n",
    "\n",
    "#print(softmax([1,2,3,100000]))\n",
    "#print(softmax_no_NS([1,2,3,1000]))\n",
    "#r = np.array(np.array([1,2,777,2]))\n",
    "#print(softmax(r))\n",
    "#r = np.array((np.array([1,2,777,2]),np.array([1,2,777,2]),np.array([1,2,777,2])))\n",
    "#print(softmax(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3eaeb8",
   "metadata": {},
   "source": [
    "### MLP Layer: Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7b0a1",
   "metadata": {},
   "source": [
    "### MLP Backward Pass\n",
    "\n",
    "`ReLU_SoftMax_FC_Backward` computes gradients for the MLP. Inputs: batch size `bs`, predictions `pred` ($P$), true `labels` ($Y$), weights $W_1, W_2$, hidden activation `fa` ($A_1$), hidden pre-activation `fl` ($Z_1$), MLP input `i_mlp` ($X_{mlp}$).\n",
    "\n",
    "**Gradients (from output layer backwards):**\n",
    "\n",
    "1.  $\\frac{\\partial L}{\\partial Z_2} = P - Y$ (`dL_dz2`)\n",
    "2.  $\\frac{\\partial L}{\\partial W_2} = A_1^T \\frac{\\partial L}{\\partial Z_2}$ (`dL_dw2`)\n",
    "3.  $\\frac{\\partial L}{\\partial b_2} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_2}$ (`dL_db2`)\n",
    "4.  $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} W_2^T$ (`dL_dfa`)\n",
    "5.  $\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\text{ReLU}'(Z_1)$ (`dL_dfl`, where $\\text{ReLU}'(Z_1)$ is 1 if $Z_1 > 0$, else 0)\n",
    "6.  $\\frac{\\partial L}{\\partial W_1} = X_{mlp}^T \\frac{\\partial L}{\\partial Z_1}$ (`dL_dw1`)\n",
    "7.  $\\frac{\\partial L}{\\partial b_1} = \\sum_{bs} \\frac{\\partial L}{\\partial Z_1}$ (`dL_db1`)\n",
    "8.  $\\frac{\\partial L}{\\partial X_{mlp}} = \\frac{\\partial L}{\\partial Z_1} W_1^T$ (`dL_i_mlp`) (gradient to pass to conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10736bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_SoftMax_FC_Backward(bs,pred,labels,w1,w2,fa,fl,i_mlp):\n",
    "    dL_dz2 = pred-labels[0:bs]\n",
    "    dL_dw2 = fa.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "    dL_dfa = dL_dz2 @ w2.T\n",
    "    dReLU = (fl > 0).astype(float)\n",
    "    dL_dfl = dL_dfa * dReLU\n",
    "    dL_dw1 = i_mlp.reshape(bs, -1).T @ dL_dfl\n",
    "    dL_db1 = np.sum(dL_dfl, axis=0)\n",
    "    dL_i_mlp = dL_dfl @ w1.T\n",
    "    return dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd7105",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf96a8c",
   "metadata": {},
   "source": [
    "### Loss Function: Categorical Cross-Entropy\n",
    "\n",
    "`crossEntropy` calculates the Categorical Cross-Entropy loss for a single sample.\n",
    "\n",
    "**Formula:** Given predicted probabilities $P=(p_1, ..., p_K)$ and one-hot true label $Y=(y_1, ..., y_K)$:\n",
    "$$ L(P, Y) = - \\sum_{k=1}^{K} y_k \\log(p_k) $$\n",
    "If class $c$ is the true class ($y_c=1$), $L = - \\log(p_c)$.\n",
    "A small epsilon (`1/100000`) is added to $p$ to prevent $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1291609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(p,t):\n",
    "    # p stands for prediction and t stands for true label\n",
    "    # p = [0,0,1] and t = [1,0,0]\n",
    "    p = p+(1/100000) # for numerical stability\n",
    "    return -np.dot(t,np.log(p).T)\n",
    "\n",
    "#c = [1,1000000000000000,1,1]\n",
    "#c = softmax(c)\n",
    "#print(c)\n",
    "#c = crossEntropy(c,[0,1,0,0])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49079",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e3c9f",
   "metadata": {},
   "source": [
    "## Inference: Comparing Implementations\n",
    "\n",
    "This section compares the inference (prediction) performance and correctness of three CNN implementations: PyTorch, \"Slow\" NumPy (loop-based), and \"Fast\" NumPy (Im2Col-based). All use identical pre-trained weights.\n",
    "\n",
    "**Objectives:**\n",
    "1.  **Correctness:** Verify that all three models yield the same predictions.\n",
    "2.  **Speed:** Compare average inference time per image.\n",
    "\n",
    "The loop iterates through test images, runs each model, records predictions and times. This demonstrates the efficiency gains from optimized libraries (PyTorch) and vectorized NumPy (Fast) over naive loops (Slow). The `pad` and `stride` parameters in the NumPy calls (`Slow_ReLU_Conv`, `Fast_ReLU_Conv`) are set to match the PyTorch model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab6c9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inferring...: 100%|██████████| 100/100 [01:39<00:00,  1.01it/s, average_times=t: 0.0032586622 s, s: 0.9865179253 s, f: 0.0012524176 s, c: 0.0005700207 s, correct_predictions=100.0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward execution time in seconds: \n",
      "PyTorch: 0.0032586622 s, \n",
      "Slow: 0.9865179253 s, \n",
      "Fast: 0.0012524176 s, \n",
      "Convolutional Matrix: 0.0005700207 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np_k1 = numpy_weights['k1'].astype(np.float32)\n",
    "np_b_conv1 = numpy_weights['b_conv1'].astype(np.float32)\n",
    "np_k2 = numpy_weights['k2'].astype(np.float32)\n",
    "np_b_conv2 = numpy_weights['b_conv2'].astype(np.float32)\n",
    "np_k3 = numpy_weights['k3'].astype(np.float32)\n",
    "np_b_conv3 = numpy_weights['b_conv3'].astype(np.float32)\n",
    "np_w1 = numpy_weights['w1'].astype(np.float32)\n",
    "np_b1 = numpy_weights['b1'].astype(np.float32)\n",
    "np_w2 = numpy_weights['w2'].astype(np.float32)\n",
    "np_b2 = numpy_weights['b2'].astype(np.float32)\n",
    "\n",
    "dict_times={}\n",
    "dict_times[\"ctorch\"]=[]\n",
    "dict_times[\"cslow\"]=[]\n",
    "dict_times[\"cfast\"]=[]\n",
    "dict_times[\"ccm\"]=[]\n",
    "\n",
    "dict_pred={}\n",
    "dict_pred[\"ctorch\"]=[]\n",
    "dict_pred[\"cslow\"]=[]\n",
    "dict_pred[\"cfast\"]=[]\n",
    "dict_pred[\"ccm\"]=[]\n",
    "\n",
    "#length = test_labels.shape[0]\n",
    "length = 100\n",
    "correct = 0\n",
    "skip = True\n",
    "loop = tqdm(range(length),desc=\" Inferring...\")\n",
    "for i in loop:\n",
    "    c0 = test_images[i].reshape(1,1,28,28).astype(np.float32)\n",
    "    torch_c0 = torch.from_numpy(c0).float()\n",
    "    ############### CNN PyTorch Implementation ##################\n",
    "    start_time = time.time()\n",
    "    outputs = model(torch_c0)\n",
    "    end_time = time.time()\n",
    "    _, predicted1 = torch.max(outputs.data, 1)\n",
    "    dict_times[\"ctorch\"].append(end_time-start_time)\n",
    "    dict_pred[\"ctorch\"].append(np.array(predicted1))\n",
    "    ############### CNN Slow Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,pad=0,stride=2)\n",
    "    c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),np_k2,np_b_conv2,pad=1,stride=2)\n",
    "    c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),np_k3,np_b_conv3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlps,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted2 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cslow\"].append(end_time-start_time)\n",
    "    dict_pred[\"cslow\"].append(np.array(predicted2))\n",
    "    ############### CNN Fast Implementation #####################\n",
    "    start_time = time.time()\n",
    "    c1f,mask1f = Fast_ReLU_Conv(c0.astype(np.float32),np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2f,mask2f = Fast_ReLU_Conv(c1f.astype(np.float32),np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3f,mask3f = Fast_ReLU_Conv(c2f.astype(np.float32),np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpf = c3f.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpf,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted3 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"cfast\"].append(end_time-start_time)\n",
    "    dict_pred[\"cfast\"].append(np.array(predicted3))\n",
    "    ############## CNN Convolutional Matrix Implementation ###########\n",
    "    start_time = time.time()\n",
    "    c1c,mask1c = im2col_convolutional_matrix(c0.astype(np.float32),np_k1,np_b_conv1,padding=0,stride=2)\n",
    "    c2c,mask2c = im2col_convolutional_matrix(c1c.astype(np.float32),np_k2,np_b_conv2,padding=1,stride=2)\n",
    "    c3c,mask3c = im2col_convolutional_matrix(c2c.astype(np.float32),np_k3,np_b_conv3,padding=0,stride=2)\n",
    "    imlpc = c3c.reshape(1,-1)\n",
    "    _,_,_,res = ReLU_SoftMax_FullyConnected(imlpc,np_w1,np_b1,np_w2,np_b2)\n",
    "    predicted4 = np.argmax(res,1)\n",
    "    end_time = time.time()\n",
    "    dict_times[\"ccm\"].append(end_time-start_time)\n",
    "    dict_pred[\"ccm\"].append(np.array(predicted4))\n",
    "    #####################################################################################\n",
    "    #### Check that outputs of Slow Approach and Fast Approach have the same results ###\n",
    "    t = int(predicted1[0])\n",
    "    s = int(predicted2[0])\n",
    "    f = int(predicted3[0])\n",
    "    c = int(predicted4[0])\n",
    "    if t == s and t == f:\n",
    "        correct+=1\n",
    "    #####################################################################################\n",
    "    ### Keep track of the times #########################################################\n",
    "    tat = round(sum(dict_times['ctorch'])/(i+1),10)\n",
    "    sat = round(sum(dict_times['cslow'])/(i+1),10)\n",
    "    fat = round(sum(dict_times['cfast'])/(i+1),10)\n",
    "    cat = round(sum(dict_times['ccm'])/(i+1),10)\n",
    "    loop.set_postfix(average_times =f\"t: {tat} s, s: {sat} s, f: {fat} s, c: {cat} s\" , correct_predictions=f\"{100*correct/(i+1)}%\")\n",
    "tat = round(sum(dict_times['ctorch'])/length,10)\n",
    "sat = round(sum(dict_times['cslow'])/length,10)\n",
    "fat = round(sum(dict_times['cfast'])/length,10)\n",
    "cat = round(sum(dict_times['ccm'])/length,10)\n",
    "print(f\"Average forward execution time in seconds: \\nPyTorch: {tat} s, \\nSlow: {sat} s, \\nFast: {fat} s, \\nConvolutional Matrix: {cat} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abe784",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3b44b",
   "metadata": {},
   "source": [
    "### Test for Slow approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f8f80",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d1c78",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a476a",
   "metadata": {},
   "source": [
    "### NumPy Model Training: Weights Initialization\n",
    "\n",
    "For training our NumPy CNNs from scratch, weights and biases are initialized randomly.\n",
    "The shapes are taken from `numpy_weights` (derived from the PyTorch model) to maintain architectural consistency. `np.random.rand()` provides initial values (uniform in [0,1)). While more advanced initializers exist, this suffices for observing basic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382d816",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b10d0",
   "metadata": {},
   "source": [
    "### Training the \"Slow\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This tests training the loop-based `Slow_ReLU_Conv` and `Slow_ReLU_Gradient` implementations on a single image.\n",
    "\n",
    "**Per-Epoch Steps:**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> Slow_ReLU_Conv (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> Slow_ReLU_Conv (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> Slow_ReLU_Conv (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa` (probabilities)\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** Gradients are computed using `ReLU_SoftMax_FC_Backward` for MLP, then `Slow_ReLU_Gradient` is called sequentially for conv layers, propagating gradients backward.\n",
    "4.  **Weight Update:** Parameters updated via $W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W_{old}}$.\n",
    "\n",
    "The loss is plotted to observe learning. The padding and stride parameters in `Slow_ReLU_Conv` calls are set to match the PyTorch model architecture, ensuring the flattened features `imlps` have the correct dimension (2048) for the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToBeTrained = True\n",
    "if ToBeTrained:\n",
    "    avg_loss = []\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    numEpochs = 20\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    loop = tqdm(range(numEpochs))\n",
    "    for i in loop:\n",
    "        c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "        \n",
    "        # Forward\n",
    "        sfts = time.time() # slow forward time start\n",
    "        c1s,mask1s = Slow_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "        c2s,mask2s = Slow_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "        c3s,mask3s = Slow_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "\n",
    "        imlps = c3s.reshape(1,-1)\n",
    "        fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "        sfte = time.time() # slow forward time end\n",
    "        sft = sfte - sfts\n",
    "        forward_time.append(sft)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossEntropy(sa,train_labels[0])\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        sbts = time.time() # slow backward time start\n",
    "        dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "        dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "        gi3,gk3,gb3 = Slow_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "\n",
    "        gi2,gk2,gb2 = Slow_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "        gi1,gk1,gb1 = Slow_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "        sbte = time.time() # slow backward time end\n",
    "        sbt = sbte - sbts\n",
    "        backward_time.append(sbt)\n",
    "\n",
    "        # Weights update\n",
    "        w1 -= lr*dL_dw1\n",
    "        b1 -= lr*dL_db1\n",
    "        w2 -= lr*dL_dw2\n",
    "        b2 -= lr*dL_db2\n",
    "        k3 -= lr*gk3\n",
    "        k2 -= lr*gk2\n",
    "        k1 -= lr*gk1\n",
    "        bc3 -= lr*gb3.reshape(-1)\n",
    "        bc2 -= lr*gb2.reshape(-1)\n",
    "        bc1 -= lr*gb1.reshape(-1)\n",
    "        \n",
    "        if len(avg_loss) >= 2:\n",
    "            loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "    plt.plot(avg_loss)\n",
    "    plt.show()\n",
    "# 2.64135 <-> 2.64095\n",
    "# 2.64055 <-> 2.64020\n",
    "# 2.64015 <-> 2.63980\n",
    "# 2.63910 <-> 2.63840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef545289",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 3.6265 s\n",
    "- average backward time : 9.8262 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Slow Approach.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea8a5",
   "metadata": {},
   "source": [
    "### Test for Fast approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1f5ac",
   "metadata": {},
   "source": [
    "In this panel the approach is tested to see if it learns or not. the test uses first just one image, then the first 100 for each eopch, in order to see if the loss descends during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a336a9",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = np.random.rand(int(numpy_weights['k1'].flatten().shape[0])).reshape(numpy_weights['k1'].shape)\n",
    "bc1 = np.random.rand(int(numpy_weights['b_conv1'].flatten().shape[0])).reshape(numpy_weights['b_conv1'].shape)\n",
    "k2 = np.random.rand(int(numpy_weights['k2'].flatten().shape[0])).reshape(numpy_weights['k2'].shape)\n",
    "bc2 = np.random.rand(int(numpy_weights['b_conv2'].flatten().shape[0])).reshape(numpy_weights['b_conv2'].shape)\n",
    "k3 = np.random.rand(int(numpy_weights['k3'].flatten().shape[0])).reshape(numpy_weights['k3'].shape)\n",
    "bc3 = np.random.rand(int(numpy_weights['b_conv3'].flatten().shape[0])).reshape(numpy_weights['b_conv3'].shape)\n",
    "w1 = np.random.rand(int(numpy_weights['w1'].flatten().shape[0])).reshape(numpy_weights['w1'].shape)\n",
    "b1 = np.random.rand(int(numpy_weights['b1'].flatten().shape[0])).reshape(numpy_weights['b1'].shape)\n",
    "w2 = np.random.rand(int(numpy_weights['w2'].flatten().shape[0])).reshape(numpy_weights['w2'].shape)\n",
    "b2 = np.random.rand(int(numpy_weights['b2'].flatten().shape[0])).reshape(numpy_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgList(listA):\n",
    "    sum_li = sum(listA)\n",
    "    length_li = len(listA)\n",
    "    return round(sum_li/length_li,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d6af1",
   "metadata": {},
   "source": [
    "#### Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fff279",
   "metadata": {},
   "source": [
    "### Training the \"Fast\" NumPy CNN (Single Image Test)\n",
    "\n",
    "This tests training using the Im2Col-based `Fast_ReLU_Conv` and the revised `Fast_ReLU_Gradient` (from cell `c808bdb6`) on a single image.\n",
    "\n",
    "**Per-Epoch Steps (differences from \"Slow\" are conv/grad functions):**\n",
    "1.  **Forward Pass:**\n",
    "    *   `c0 -> Fast_ReLU_Conv (k1,bc1,p=0,s=2) -> c1s`\n",
    "    *   `c1s -> Fast_ReLU_Conv (k2,bc2,p=1,s=2) -> c2s`\n",
    "    *   `c2s -> Fast_ReLU_Conv (k3,bc3,p=0,s=2) -> c3s`\n",
    "    *   `c3s -> flatten -> imlps -> ReLU_SoftMax_FullyConnected -> sa`\n",
    "2.  **Loss:** `loss = crossEntropy(sa, true_label)`\n",
    "3.  **Backward Pass:** `ReLU_SoftMax_FC_Backward` for MLP, then `Fast_ReLU_Gradient` (using `sliding_window_view` for `gi` and `gk`) for conv layers.\n",
    "4.  **Weight Update:** Standard gradient descent.\n",
    "\n",
    "The loss is plotted. Consistent padding/stride ensures correct feature dimensions for the MLP. This setup tests the learning capability and performance of the more optimized NumPy convolution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_loss = []\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "numEpochs = 20\n",
    "bs = 1\n",
    "lr = 0.001\n",
    "loop = tqdm(range(numEpochs))\n",
    "for i in loop:\n",
    "    c0 = train_images[0].reshape(1,1,28,28).astype(np.float32)\n",
    "    \n",
    "    # Forward\n",
    "    sfts = time.time() # slow forward time start\n",
    "    c1s,mask1s = Fast_ReLU_Conv(c0.astype(np.float32),k1,bc1,pad=0,stride=2)\n",
    "    c2s,mask2s = Fast_ReLU_Conv(c1s.astype(np.float32),k2,bc2,pad=1,stride=2)\n",
    "    c3s,mask3s = Fast_ReLU_Conv(c2s.astype(np.float32),k3,bc3,pad=0,stride=2)\n",
    "    imlps = c3s.reshape(1,-1)\n",
    "    fl,fa,sl,sa = ReLU_SoftMax_FullyConnected(imlps,w1,b1,w2,b2)\n",
    "    sfte = time.time() # slow forward time end\n",
    "    sft = sfte - sfts\n",
    "    forward_time.append(sft)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossEntropy(sa,train_labels[0])\n",
    "    avg_loss.append(loss)\n",
    "\n",
    "    # Backward\n",
    "    sbts = time.time() # slow backward time start\n",
    "    dL_i_mlp,dL_dw1,dL_db1,dL_dw2,dL_db2 = ReLU_SoftMax_FC_Backward(bs,sa,train_labels[0],w1,w2,fa,fl,imlps)\n",
    "    dL_i_mlp = dL_i_mlp.reshape(c3s.shape)\n",
    "\n",
    "    gi3,gk3,gb3 = Fast_ReLU_Gradient(c2s,dL_i_mlp,k3,mask3s,pad=0,stride=2)\n",
    "    gi2,gk2,gb2 = Fast_ReLU_Gradient(c1s,gi3,k2,mask2s,pad=1,stride=2)\n",
    "    gi1,gk1,gb1 = Fast_ReLU_Gradient(c0,gi2,k1,mask1s,pad=0,stride=2)\n",
    "    sbte = time.time() # slow backward time end\n",
    "    sbt = sbte - sbts\n",
    "    backward_time.append(sbt)\n",
    "\n",
    "    # Weights update\n",
    "    w1 -= lr*dL_dw1\n",
    "    b1 -= lr*dL_db1\n",
    "    w2 -= lr*dL_dw2\n",
    "    b2 -= lr*dL_db2\n",
    "    k3 -= lr*gk3\n",
    "    k2 -= lr*gk2\n",
    "    k1 -= lr*gk1\n",
    "    bc3 -= lr*gb3\n",
    "    bc2 -= lr*gb2\n",
    "    bc1 -= lr*gb1\n",
    "    \n",
    "    if len(avg_loss) > 2:\n",
    "        loop.set_postfix(pendence=f\" {avg_loss[i]-avg_loss[i-1]}\",avgForward=f\"{avgList(forward_time)} s\", avgBackward=f\"{avgList(backward_time)} s\" )\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd84d2",
   "metadata": {},
   "source": [
    "These are the results for 20 epochs of one image:\n",
    "- average forward time : 0.0022 s\n",
    "- average backward time : 0.0097 s\n",
    "\n",
    "Plot of the loss:\n",
    "\n",
    "<img src=\"IMAGES\\Fast Approach.png\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
